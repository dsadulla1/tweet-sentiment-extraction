{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do you need psuedo labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You might need a token for space itself? start and stop tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not implemented\n",
    "\n",
    "1. https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention\n",
    "2. https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification model with test as well in train? Will increase the vocab size as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * https://www.tensorflow.org/tutorials/text/transformer\n",
    "# * https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://rubikscode.net/2019/08/05/transformer-with-python-and-tensorflow-2-0-attention-layers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V99\"\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "TRAIN_SPLIT_RATIO = 0.8\n",
    "\n",
    "DROPOUT = 0.3\n",
    "MIN_LR = 1e-6\n",
    "MAX_LR = 1e-3\n",
    "BATCH_SIZE = 1024\n",
    "PREDICT_BATCH_SIZE = 2048\n",
    "STEP_SIZE = 10\n",
    "CLR_METHOD = \"triangular\" # exp_range, triangular, triangular2\n",
    "NUM_EPOCHS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "import pickle, os, sys, re\n",
    "from time import time, ctime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, MaxPooling1D, Layer\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tokenizers\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import *\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "np.random.seed(54321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May  5 03:21:15 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  1\n",
      "selected_text  object  1\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27480, 'selected_text': 22463, 'sentiment': 3}\n",
      "            textID                                    text selected_text  \\\n",
      "count   27481       27480                                   27480          \n",
      "unique  27481       27480                                   22463          \n",
      "top     42a4413514  knows its all going to sort itself out  good           \n",
      "freq    1           1                                       199            \n",
      "\n",
      "       sentiment  \n",
      "count   27481     \n",
      "unique  3         \n",
      "top     neutral   \n",
      "freq    11118     \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1   I`d have responded, if I were going             \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going  neutral   \n",
       "1  Sooo SAD                             negative  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\",\n",
    "                 encoding=\"utf8\")\n",
    "\n",
    "print(pd.concat((df.dtypes, df.isna().sum()), axis=1))\n",
    "print(df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df[i].nunique() for i in df.columns})\n",
    "print(df.describe()) #.astype(int)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID                  text sentiment\n",
      "count   3534        3534                  3534    \n",
      "unique  3534        3534                  3       \n",
      "top     755bcfd6a2  Captain Jack is back  neutral \n",
      "freq    1           1                     1430    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0  Last session of the day  http://twitpic.com/67ezh                                                         \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0  neutral   \n",
       "1  positive  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "print(pd.concat((test_df.dtypes, test_df.isna().sum()), axis=1))\n",
    "print(test_df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df[i].nunique() for i in test_df.columns})\n",
    "print(test_df.describe())\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>c77717b103</td>\n",
       "      <td>I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½</td>\n",
       "      <td>I love to!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>28dbada620</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "44   c77717b103   \n",
       "192  28dbada620   \n",
       "\n",
       "                                                                                                       text  \\\n",
       "44    I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½   \n",
       "192   *phew*  Will make a note in case anyone else runs into the same issueï¿½                                \n",
       "\n",
       "                                                                selected_text  \\\n",
       "44   I love to!                                                                 \n",
       "192  *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "    sentiment  \n",
       "44   positive  \n",
       "192  neutral   "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>7223fdccc2</td>\n",
       "      <td>tikcets are only ï¿½91...each...BUT I SO WANT TO GO</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>43ad351369</td>\n",
       "      <td>AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "145  7223fdccc2   \n",
       "618  43ad351369   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "145  tikcets are only ï¿½91...each...BUT I SO WANT TO GO                                                                                              \n",
       "618  AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s   \n",
       "\n",
       "    sentiment  \n",
       "145  positive  \n",
       "618  positive  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[test_df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment count in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>7781</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>11117</td>\n",
       "      <td>1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>8582</td>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text  text\n",
       "sentiment             \n",
       "negative   7781   1001\n",
       "neutral    11117  1430\n",
       "positive   8582   1103"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df.groupby(\"sentiment\")[[\"text\"]].count(), test_df.groupby(\"sentiment\")[[\"text\"]].count()], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For traceability\n",
    "df[\"original_index\"] = df.index\n",
    "test_df[\"original_index\"] = test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_idxs = [18, 27, 32, 39, 48, 49, 64, 66, 84, 92, 102, 116, 129, 132, 134, 149, 160, 166, 168, 189, 196, 197, 210, 223, 247, 251, 260, 285, 295, 296, 297, 309, 349, 362, 366, 382, 396, 398, 406, 410, 425, 458, 460, 482, 492, 500, 504, 517, 533, 537, 543, 569, 573, 581, 604, 605, 608, 634, 636, 637, 639, 642, 665, 670, 674, 678, 685, 702, 707, 709, 710, 727, 737, 746, 751, 757, 768, 778, 787, 794, 804, 809, 853, 854, 856, 863, 871, 872, 912, 931, 937, 942, 950, 956, 957, 963, 992, 993, 999, 1001, 1011, 1012, 1036, 1039, 1057, 1062, 1076, 1077, 1083, 1096, 1105, 1116, 1117, 1122, 1134, 1137, 1150, 1159, 1199, 1200, 1209, 1217, 1264, 1271, 1280, 1283, 1298, 1300, 1303, 1319, 1327, 1342, 1360, 1363, 1365, 1372, 1374, 1376, 1382, 1393, 1416, 1417, 1420, 1447, 1454, 1468, 1472, 1513, 1515, 1518, 1531, 1548, 1558, 1567, 1580, 1588, 1605, 1623, 1648, 1649, 1657, 1659, 1661, 1665, 1672, 1674, 1678, 1683, 1693, 1696, 1713, 1723, 1728, 1739, 1747, 1754, 1798, 1806, 1808, 1814, 1822, 1828, 1830, 1835, 1850, 1857, 1860, 1866, 1888, 1901, 1902, 1929, 1938, 1940, 1987, 1994, 2003, 2004, 2008, 2011, 2023, 2054, 2056, 2065, 2068, 2083, 2086, 2094, 2106, 2113, 2124, 2125, 2136, 2139, 2145, 2168, 2180, 2181, 2187, 2213, 2225, 2227, 2230, 2239, 2273, 2279, 2286, 2288, 2298, 2323, 2324, 2335, 2343, 2359, 2364, 2371, 2385, 2388, 2392, 2398, 2401, 2404, 2410, 2415, 2421, 2430, 2438, 2439, 2440, 2444, 2445, 2448, 2470, 2478, 2489, 2499, 2505, 2515, 2520, 2559, 2574, 2576, 2589, 2598, 2600, 2640, 2643, 2650, 2671, 2679, 2696, 2699, 2706, 2716, 2718, 2731, 2743, 2749, 2750, 2785, 2787, 2789, 2805, 2809, 2825, 2838, 2856, 2888, 2898, 2903, 2923, 2925, 2931, 2933, 2942, 2957, 2961, 2986, 2995, 3034, 3068, 3075, 3077, 3080, 3088, 3089, 3096, 3097, 3101, 3121, 3147, 3160, 3161, 3176, 3182, 3187, 3211, 3225, 3234, 3257, 3263, 3267, 3281, 3290, 3295, 3317, 3319, 3325, 3333, 3359, 3364, 3369, 3374, 3390, 3399, 3400, 3411, 3424, 3473, 3475, 3476, 3479, 3492, 3499, 3504, 3509, 3517, 3519, 3532, 3553, 3578, 3588, 3590, 3593, 3598, 3605, 3606, 3607, 3622, 3634, 3669, 3672, 3678, 3682, 3685, 3694, 3708, 3720, 3729, 3750, 3755, 3784, 3794, 3798, 3801, 3806, 3807, 3813, 3826, 3834, 3872, 3886, 3895, 3903, 3925, 3937, 3946, 3962, 3969, 3998, 4031, 4047, 4053, 4074, 4091, 4093, 4100, 4108, 4113, 4119, 4143, 4148, 4163, 4164, 4183, 4196, 4254, 4260, 4265, 4307, 4308, 4309, 4312, 4318, 4325, 4327, 4328, 4329, 4344, 4347, 4369, 4371, 4377, 4384, 4402, 4404, 4405, 4407, 4412, 4421, 4422, 4437, 4455, 4457, 4471, 4473, 4479, 4483, 4486, 4514, 4532, 4537, 4542, 4543, 4545, 4550, 4554, 4563, 4575, 4576, 4591, 4595, 4631, 4636, 4637, 4639, 4654, 4655, 4704, 4706, 4714, 4718, 4724, 4729, 4735, 4747, 4749, 4750, 4764, 4800, 4814, 4841, 4842, 4844, 4864, 4866, 4869, 4875, 4891, 4908, 4918, 4924, 4928, 4933, 4943, 4946, 4947, 4956, 4968, 4980, 4994, 5009, 5011, 5038, 5056, 5079, 5083, 5101, 5133, 5167, 5173, 5174, 5188, 5189, 5196, 5197, 5213, 5218, 5229, 5241, 5245, 5258, 5279, 5290, 5294, 5297, 5308, 5317, 5331, 5342, 5343, 5353, 5355, 5356, 5358, 5379, 5385, 5386, 5399, 5418, 5433, 5436, 5450, 5456, 5466, 5489, 5504, 5509, 5510, 5530, 5542, 5560, 5583, 5603, 5607, 5618, 5620, 5624, 5626, 5678, 5697, 5712, 5725, 5751, 5757, 5775, 5785, 5799, 5822, 5832, 5836, 5844, 5895, 5902, 5904, 5913, 5918, 5938, 5939, 5961, 5984, 5990, 6025, 6031, 6033, 6060, 6077, 6084, 6090, 6113, 6131, 6134, 6151, 6152, 6176, 6203, 6229, 6230, 6248, 6261, 6277, 6289, 6296, 6304, 6308, 6317, 6319, 6321, 6335, 6338, 6343, 6353, 6356, 6360, 6377, 6382, 6395, 6404, 6425, 6433, 6468, 6474, 6475, 6476, 6477, 6510, 6528, 6540, 6574, 6580, 6582, 6593, 6609, 6627, 6628, 6633, 6635, 6650, 6656, 6657, 6670, 6673, 6676, 6677, 6679, 6686, 6702, 6724, 6740, 6745, 6759, 6801, 6804, 6830, 6833, 6834, 6842, 6859, 6871, 6875, 6878, 6880, 6885, 6896, 6934, 6939, 6948, 6950, 6958, 6970, 6979, 6987, 7010, 7017, 7024, 7040, 7099, 7107, 7112, 7136, 7147, 7149, 7158, 7184, 7225, 7227, 7251, 7274, 7288, 7305, 7306, 7308, 7331, 7344, 7355, 7373, 7407, 7408, 7409, 7410, 7423, 7436, 7438, 7442, 7444, 7450, 7469, 7473, 7479, 7487, 7492, 7506, 7513, 7521, 7528, 7530, 7546, 7567, 7581, 7582, 7597, 7599, 7602, 7632, 7635, 7636, 7642, 7662, 7663, 7701, 7752, 7758, 7765, 7769, 7778, 7780, 7788, 7801, 7816, 7818, 7831, 7836, 7837, 7849, 7893, 7895, 7896, 7922, 7927, 7945, 7948, 7968, 7997, 8003, 8005, 8010, 8030, 8049, 8051, 8052, 8059, 8060, 8085, 8094, 8142, 8143, 8151, 8153, 8161, 8169, 8170, 8196, 8208, 8212, 8215, 8224, 8235, 8241, 8249, 8250, 8276, 8282, 8285, 8286, 8288, 8289, 8299, 8314, 8338, 8343, 8345, 8349, 8373, 8379, 8390, 8397, 8399, 8407, 8427, 8428, 8432, 8439, 8442, 8444, 8484, 8493, 8497, 8500, 8507, 8510, 8534, 8546, 8559, 8569, 8579, 8588, 8594, 8604, 8617, 8620, 8624, 8631, 8638, 8661, 8671, 8674, 8687, 8691, 8696, 8706, 8720, 8729, 8749, 8756, 8758, 8769, 8774, 8780, 8800, 8803, 8809, 8815, 8827, 8830, 8875, 8902, 8906, 8910, 8917, 8919, 8922, 8951, 8954, 8968, 8985, 8986, 8991, 8999, 9014, 9034, 9050, 9057, 9063, 9070, 9088, 9098, 9112, 9113, 9122, 9127, 9140, 9155, 9157, 9169, 9179, 9190, 9216, 9223, 9226, 9264, 9272, 9281, 9297, 9300, 9303, 9322, 9354, 9361, 9374, 9377, 9381, 9385, 9400, 9403, 9419, 9422, 9425, 9428, 9436, 9442, 9449, 9454, 9456, 9458, 9470, 9482, 9496, 9500, 9535, 9539, 9541, 9554, 9556, 9560, 9565, 9574, 9594, 9605, 9610, 9631, 9632, 9660, 9689, 9691, 9696, 9708, 9710, 9712, 9725, 9726, 9734, 9742, 9753, 9770, 9789, 9799, 9801, 9812, 9817, 9836, 9839, 9850, 9852, 9881, 9882, 9891, 9906, 9912, 9920, 9922, 9928, 9976, 9980, 9984, 10002, 10004, 10007, 10029, 10039, 10042, 10050, 10052, 10057, 10067, 10070, 10092, 10093, 10123, 10131, 10138, 10145, 10151, 10164, 10167, 10170, 10179, 10184, 10185, 10190, 10197, 10200, 10211, 10218, 10219, 10222, 10228, 10245, 10247, 10255, 10259, 10276, 10293, 10298, 10299, 10306, 10319, 10327, 10342, 10348, 10377, 10379, 10449, 10465, 10469, 10492, 10504, 10505, 10508, 10521, 10528, 10529, 10530, 10570, 10573, 10577, 10578, 10597, 10611, 10639, 10651, 10666, 10672, 10675, 10690, 10706, 10711, 10738, 10742, 10747, 10749, 10779, 10782, 10783, 10795, 10813, 10849, 10852, 10853, 10884, 10885, 10888, 10892, 10901, 10914, 10918, 10919, 10933, 10935, 10956, 10966, 10968, 10970, 10981, 10986, 10996, 10999, 11005, 11032, 11038, 11051, 11055, 11060, 11067, 11097, 11102, 11155, 11157, 11159, 11162, 11167, 11176, 11187, 11215, 11222, 11228, 11230, 11257, 11263, 11265, 11274, 11286, 11289, 11297, 11300, 11308, 11323, 11337, 11339, 11341, 11347, 11349, 11353, 11371, 11381, 11386, 11426, 11431, 11448, 11449, 11474, 11480, 11500, 11507, 11553, 11562, 11582, 11588, 11594, 11612, 11616, 11618, 11636, 11643, 11650, 11658, 11666, 11679, 11683, 11693, 11695, 11698, 11699, 11706, 11716, 11723, 11730, 11732, 11741, 11745, 11780, 11794, 11798, 11808, 11812, 11828, 11829, 11837, 11840, 11841, 11844, 11849, 11853, 11854, 11855, 11861, 11862, 11872, 11875, 11878, 11886, 11890, 11899, 11905, 11912, 11938, 11963, 11967, 11973, 11980, 11985, 11999, 12026, 12027, 12029, 12039, 12054, 12067, 12115, 12124, 12134, 12136, 12138, 12150, 12187, 12205, 12206, 12224, 12242, 12258, 12269, 12277, 12283, 12295, 12305, 12314, 12322, 12331, 12333, 12334, 12350, 12355, 12356, 12360, 12370, 12372, 12389, 12395, 12396, 12397, 12405, 12410, 12416, 12429, 12439, 12440, 12442, 12446, 12474, 12482, 12486, 12507, 12516, 12521, 12522, 12526, 12527, 12537, 12550, 12563, 12576, 12585, 12586, 12587, 12598, 12606, 12630, 12631, 12635, 12639, 12650, 12657, 12662, 12669, 12687, 12703, 12718, 12724, 12732, 12736, 12745, 12767, 12787, 12803, 12804, 12808, 12818, 12830, 12843, 12855, 12856, 12870, 12884, 12903, 12914, 12920, 12923, 12924, 12927, 12936, 12972, 12977, 12998, 13003, 13004, 13009, 13044, 13068, 13069, 13074, 13080, 13095, 13098, 13124, 13130, 13153, 13154, 13176, 13219, 13222, 13227, 13237, 13267, 13274, 13293, 13301, 13304, 13307, 13330, 13333, 13354, 13365, 13373, 13379, 13381, 13389, 13397, 13422, 13430, 13433, 13445, 13466, 13473, 13476, 13482, 13490, 13493, 13528, 13529, 13535, 13543, 13558, 13559, 13586, 13637, 13643, 13646, 13665, 13674, 13678, 13679, 13704, 13707, 13710, 13718, 13727, 13733, 13747, 13762, 13772, 13785, 13796, 13803, 13812, 13814, 13817, 13827, 13844, 13846, 13847, 13848, 13854, 13855, 13861, 13864, 13873, 13899, 13907, 13936, 13939, 13946, 13949, 13954, 13964, 13965, 13974, 13975, 13978, 13980, 14002, 14006, 14007, 14030, 14043, 14044, 14057, 14058, 14060, 14073, 14091, 14109, 14110, 14112, 14117, 14131, 14132, 14159, 14172, 14173, 14176, 14184, 14194, 14197, 14200, 14204, 14207, 14213, 14230, 14257, 14273, 14275, 14287, 14299, 14301, 14307, 14308, 14347, 14362, 14365, 14381, 14386, 14400, 14416, 14428, 14436, 14446, 14457, 14458, 14462, 14487, 14499, 14509, 14532, 14548, 14551, 14571, 14596, 14598, 14611, 14621, 14639, 14640, 14648, 14662, 14669, 14671, 14676, 14680, 14687, 14689, 14691, 14718, 14731, 14747, 14779, 14787, 14825, 14839, 14844, 14847, 14855, 14870, 14880, 14891, 14908, 14946, 14959, 14960, 14967, 14971, 14986, 15007, 15010, 15016, 15044, 15049, 15055, 15056, 15112, 15131, 15158, 15165, 15174, 15177, 15196, 15203, 15206, 15207, 15213, 15217, 15233, 15236, 15253, 15259, 15286, 15296, 15307, 15308, 15321, 15323, 15327, 15339, 15363, 15372, 15374, 15379, 15380, 15383, 15385, 15392, 15410, 15449, 15452, 15462, 15464, 15465, 15477, 15483, 15503, 15505, 15528, 15532, 15548, 15562, 15566, 15571, 15605, 15610, 15622, 15651, 15660, 15666, 15674, 15709, 15721, 15731, 15735, 15736, 15741, 15767, 15774, 15785, 15792, 15793, 15811, 15851, 15879, 15883, 15884, 15912, 15926, 15927, 15931, 15940, 15959, 15965, 15985, 15998, 16020, 16027, 16031, 16032, 16044, 16070, 16100, 16117, 16129, 16136, 16137, 16140, 16149, 16152, 16175, 16186, 16201, 16241, 16261, 16269, 16272, 16279, 16290, 16308, 16319, 16324, 16325, 16327, 16338, 16341, 16345, 16352, 16354, 16371, 16372, 16385, 16391, 16416, 16423, 16448, 16455, 16468, 16470, 16477, 16481, 16484, 16491, 16493, 16500, 16509, 16510, 16516, 16541, 16559, 16577, 16589, 16632, 16638, 16642, 16643, 16649, 16650, 16654, 16665, 16677, 16684, 16704, 16720, 16726, 16734, 16765, 16774, 16775, 16777, 16780, 16797, 16798, 16799, 16803, 16807, 16809, 16830, 16832, 16861, 16863, 16868, 16876, 16889, 16890, 16902, 16915, 16927, 16939, 16949, 16962, 16968, 17002, 17010, 17013, 17021, 17023, 17029, 17062, 17066, 17084, 17093, 17094, 17102, 17152, 17160, 17167, 17173, 17183, 17221, 17239, 17250, 17267, 17277, 17278, 17282, 17296, 17307, 17315, 17333, 17336, 17344, 17346, 17348, 17354, 17365, 17374, 17385, 17387, 17397, 17401, 17404, 17412, 17417, 17432, 17436, 17444, 17452, 17467, 17469, 17501, 17502, 17513, 17530, 17531, 17547, 17550, 17555, 17573, 17577, 17578, 17586, 17600, 17602, 17611, 17627, 17640, 17645, 17648, 17649, 17652, 17667, 17676, 17682, 17721, 17724, 17729, 17731, 17751, 17754, 17762, 17770, 17772, 17781, 17802, 17820, 17823, 17833, 17848, 17857, 17858, 17865, 17867, 17887, 17893, 17909, 17924, 17926, 17945, 17971, 17977, 17986, 17990, 18003, 18011, 18042, 18046, 18052, 18077, 18086, 18091, 18099, 18103, 18123, 18128, 18131, 18142, 18153, 18165, 18167, 18173, 18181, 18206, 18226, 18252, 18263, 18276, 18277, 18284, 18314, 18319, 18327, 18330, 18342, 18352, 18355, 18362, 18364, 18375, 18383, 18398, 18403, 18432, 18447, 18453, 18465, 18467, 18485, 18486, 18492, 18505, 18508, 18510, 18515, 18536, 18540, 18548, 18550, 18557, 18563, 18570, 18574, 18576, 18616, 18622, 18627, 18639, 18649, 18655, 18673, 18681, 18712, 18745, 18746, 18758, 18773, 18778, 18782, 18826, 18832, 18834, 18842, 18851, 18853, 18858, 18862, 18867, 18872, 18877, 18908, 18910, 18912, 18913, 18919, 18925, 18929, 18930, 18933, 18936, 18944, 18974, 18975, 18997, 19009, 19011, 19014, 19016, 19028, 19057, 19098, 19106, 19114, 19130, 19133, 19147, 19157, 19162, 19166, 19173, 19179, 19198, 19213, 19225, 19230, 19239, 19241, 19266, 19274, 19279, 19283, 19285, 19316, 19327, 19335, 19352, 19358, 19362, 19366, 19377, 19379, 19405, 19423, 19428, 19431, 19437, 19444, 19456, 19469, 19475, 19476, 19481, 19486, 19492, 19506, 19507, 19536, 19537, 19562, 19586, 19589, 19622, 19634, 19651, 19663, 19679, 19685, 19690, 19697, 19707, 19712, 19714, 19715, 19717, 19724, 19734, 19744, 19754, 19769, 19779, 19792, 19807, 19830, 19850, 19863, 19875, 19903, 19904, 19908, 19963, 19985, 19988, 20003, 20012, 20014, 20018, 20040, 20042, 20054, 20078, 20079, 20083, 20087, 20089, 20114, 20121, 20122, 20138, 20155, 20162, 20186, 20199, 20216, 20222, 20234, 20238, 20254, 20285, 20299, 20302, 20305, 20337, 20345, 20364, 20365, 20374, 20382, 20394, 20403, 20440, 20450, 20451, 20476, 20506, 20508, 20516, 20527, 20536, 20540, 20541, 20542, 20557, 20566, 20584, 20608, 20611, 20613, 20618, 20627, 20644, 20656, 20663, 20674, 20682, 20690, 20699, 20707, 20711, 20737, 20751, 20765, 20771, 20774, 20778, 20779, 20789, 20795, 20798, 20834, 20851, 20865, 20867, 20869, 20872, 20883, 20892, 20894, 20895, 20896, 20901, 20919, 20924, 20933, 20937, 20938, 20946, 20947, 20955, 20957, 20958, 20965, 20966, 20970, 20974, 20991, 21001, 21013, 21018, 21020, 21025, 21114, 21142, 21205, 21210, 21227, 21230, 21236, 21253, 21262, 21294, 21300, 21316, 21331, 21339, 21349, 21356, 21365, 21369, 21376, 21399, 21410, 21413, 21418, 21420, 21435, 21447, 21452, 21455, 21468, 21470, 21474, 21486, 21495, 21501, 21517, 21530, 21539, 21546, 21547, 21553, 21556, 21564, 21569, 21579, 21593, 21595, 21599, 21603, 21632, 21643, 21690, 21713, 21726, 21732, 21737, 21738, 21744, 21755, 21773, 21782, 21800, 21807, 21829, 21839, 21848, 21876, 21893, 21923, 21926, 21931, 21958, 21967, 21970, 21973, 21982, 22002, 22029, 22031, 22035, 22044, 22051, 22068, 22073, 22099, 22117, 22121, 22126, 22145, 22146, 22149, 22161, 22166, 22170, 22177, 22205, 22210, 22231, 22233, 22234, 22244, 22246, 22251, 22272, 22280, 22299, 22317, 22321, 22327, 22350, 22355, 22363, 22365, 22378, 22383, 22387, 22391, 22393, 22395, 22403, 22406, 22418, 22423, 22428, 22437, 22444, 22451, 22453, 22460, 22480, 22491, 22510, 22530, 22536, 22554, 22557, 22561, 22562, 22564, 22571, 22588, 22593, 22606, 22607, 22610, 22611, 22642, 22666, 22717, 22729, 22738, 22744, 22745, 22763, 22768, 22769, 22774, 22782, 22789, 22796, 22800, 22836, 22841, 22843, 22858, 22864, 22886, 22889, 22897, 22925, 22928, 22938, 22944, 22945, 22948, 22950, 22956, 22983, 22986, 22988, 22993, 22994, 22997, 23028, 23030, 23039, 23047, 23081, 23091, 23097, 23105, 23108, 23111, 23113, 23133, 23145, 23150, 23161, 23172, 23176, 23199, 23200, 23202, 23204, 23205, 23211, 23212, 23226, 23227, 23231, 23236, 23239, 23263, 23265, 23266, 23269, 23290, 23311, 23315, 23320, 23323, 23340, 23352, 23361, 23372, 23375, 23376, 23381, 23386, 23390, 23401, 23415, 23418, 23428, 23429, 23453, 23467, 23479, 23505, 23508, 23527, 23528, 23535, 23537, 23541, 23553, 23555, 23557, 23559, 23563, 23566, 23581, 23585, 23596, 23598, 23604, 23605, 23617, 23630, 23643, 23658, 23669, 23673, 23680, 23683, 23690, 23691, 23701, 23707, 23714, 23720, 23725, 23733, 23746, 23750, 23751, 23769, 23774, 23784, 23809, 23818, 23831, 23839, 23842, 23857, 23924, 23930, 23944, 23945, 23947, 23959, 23971, 23975, 23987, 23989, 24002, 24003, 24009, 24019, 24026, 24039, 24041, 24046, 24062, 24071, 24077, 24079, 24087, 24096, 24119, 24121, 24137, 24150, 24169, 24181, 24194, 24195, 24199, 24200, 24203, 24210, 24223, 24258, 24272, 24274, 24291, 24293, 24300, 24305, 24318, 24339, 24378, 24380, 24383, 24392, 24402, 24442, 24468, 24476, 24480, 24488, 24490, 24495, 24502, 24504, 24505, 24516, 24562, 24572, 24594, 24595, 24597, 24600, 24606, 24609, 24622, 24623, 24629, 24631, 24632, 24633, 24657, 24662, 24664, 24666, 24682, 24697, 24709, 24753, 24758, 24766, 24833, 24840, 24851, 24858, 24869, 24886, 24909, 24915, 24916, 24929, 24945, 24951, 24966, 24972, 24980, 24996, 25012, 25017, 25043, 25059, 25067, 25104, 25121, 25125, 25127, 25133, 25150, 25191, 25210, 25224, 25228, 25240, 25243, 25255, 25264, 25267, 25272, 25282, 25284, 25290, 25292, 25293, 25334, 25338, 25350, 25365, 25368, 25380, 25389, 25391, 25400, 25413, 25422, 25440, 25444, 25446, 25456, 25460, 25472, 25486, 25495, 25499, 25509, 25512, 25528, 25530, 25532, 25547, 25559, 25591, 25593, 25597, 25601, 25614, 25616, 25638, 25672, 25691, 25698, 25712, 25713, 25721, 25722, 25725, 25732, 25753, 25760, 25775, 25794, 25813, 25819, 25828, 25833, 25901, 25908, 25947, 25983, 25987, 25989, 25996, 26008, 26017, 26025, 26030, 26039, 26047, 26054, 26058, 26077, 26088, 26092, 26112, 26183, 26185, 26190, 26201, 26204, 26237, 26239, 26241, 26248, 26251, 26252, 26256, 26263, 26268, 26282, 26314, 26331, 26346, 26358, 26360, 26364, 26377, 26428, 26434, 26460, 26481, 26485, 26497, 26532, 26533, 26538, 26561, 26567, 26585, 26588, 26595, 26596, 26618, 26625, 26633, 26643, 26651, 26677, 26687, 26715, 26721, 26732, 26733, 26746, 26760, 26762, 26769, 26779, 26781, 26822, 26828, 26830, 26870, 26882, 26883, 26889, 26892, 26894, 26914, 26927, 26932, 26944, 26953, 26957, 26963, 26967, 26973, 26976, 26978, 26985, 27030, 27038, 27044, 27048, 27052, 27067, 27111, 27115, 27121, 27142, 27144, 27153, 27167, 27205, 27209, 27217, 27218, 27229, 27233, 27240, 27250, 27280, 27302, 27332, 27339, 27349, 27362, 27376, 27386, 27396, 27401, 27413, 27426, 27429, 27456, 27470, 27474, 27476, 27477, 27480]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27481, 5)\n",
      "(24864, 5)\n",
      "(24863, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df = df[(~df.index.isin(anomalous_idxs))]\n",
    "print(df.shape)\n",
    "df = df[(~df.text.isna())]\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.copy()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment_code\"] = df[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments = df[\"sentiment_code\"].cat.codes.values\n",
    "\n",
    "test_df[\"sentiment_code\"] = test_df[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments_test = test_df[\"sentiment_code\"].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"selected_text\"] = df[\"selected_text\"].astype(str)\n",
    "test_df[\"text\"] = test_df[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_mod\"] = \"xxxSTART \" + df.text + \" xxxSENTIMENT \" + df.sentiment + \" xxxEND\"\n",
    "test_df[\"text_mod\"] = \"xxxSTART \" + test_df.text + \" xxxSENTIMENT \" + test_df.sentiment + \" xxxEND\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Shuffle and Train Val Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24863 19890 4973 24863\n"
     ]
    }
   ],
   "source": [
    "n_rows = df.shape[0]\n",
    "idx = [i for i in np.arange(n_rows)]\n",
    "np.random.shuffle(idx)\n",
    "train_idx, val_idx = idx[:round(TRAIN_SPLIT_RATIO * n_rows)], idx[round(TRAIN_SPLIT_RATIO * n_rows):]\n",
    "\n",
    "print(len(idx), len(train_idx), len(val_idx), len(train_idx) + len(val_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"set_name\"] = np.where(df.index.isin(val_idx), \"val\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set_name\n",
       "train    19890\n",
       "val      4973 \n",
       "Name: textID, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"set_name\")[\"textID\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../results/texts\"):\n",
    "    for i in os.listdir(\"../results/texts/\"):\n",
    "        os.remove(\"../results/texts/\"+i)\n",
    "else:\n",
    "    os.mkdir(\"../results/texts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip(df.textID.loc[df.set_name==\"train\"], df.text_mod.loc[df.set_name==\"train\"]):\n",
    "    with open(\"../results/texts/\"+i+\".txt\", \"w+\") as f:\n",
    "        f.write(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "paths = [str(x) for x in Path(\"../results/texts/\").glob(\"**/*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(add_prefix_space=True, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=paths, min_frequency=2, show_progress=True, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"xxxSTART\",\n",
    "    \"xxxSENTIMENT\",\n",
    "    \"xxxEND\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"../results/tokenizers/\", MODEL_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens = tokenizer.encode_batch(df.text_mod.tolist())\n",
    "Y_tokens = tokenizer.encode_batch(df.selected_text.tolist())\n",
    "X_tokens_test = tokenizer.encode_batch(test_df.text_mod.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i.ids for i in X_tokens]\n",
    "Y = [i.ids for i in Y_tokens]\n",
    "X_test = [i.ids for i in X_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_att = [i.attention_mask for i in X_tokens]\n",
    "Y_att = [i.attention_mask for i in Y_tokens]\n",
    "X_att_test = [i.attention_mask for i in X_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(i) for i in X])\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "print(VOCAB_SIZE, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremities(l_string, s_string, print_it=False):\n",
    "    len_l = len(l_string)\n",
    "    len_s = len(s_string)\n",
    "    \n",
    "    for i in range(len_l - len_s + 1):\n",
    "        if (i + len_s) <= len_l:\n",
    "            substring = l_string[i:i+len_s]\n",
    "            if substring == s_string:\n",
    "                if print_it:\n",
    "                    print(l_string)\n",
    "                    print(substring)\n",
    "                    print(i, i+len_s, substring)\n",
    "                \n",
    "                start_vector, end_vector = [0] * len_l, [0] * len_l\n",
    "                start_vector[i], end_vector[i+len_s-1] = 1, 1\n",
    "                \n",
    "                return (start_vector, end_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_starts, Y_stops = [], []\n",
    "anomaly_idx, counter = [], 0\n",
    "for num, (i,j) in enumerate(zip(X_tokens, Y_tokens)):\n",
    "    x,y = i.ids, j.ids\n",
    "    #s,e = get_extremities(x, y)\n",
    "    try:\n",
    "        s,e = get_extremities(x, y)\n",
    "    except TypeError as t:\n",
    "        counter += 1\n",
    "        anomaly_idx.append(num)\n",
    "    Y_starts.append(s)\n",
    "    Y_stops.append(e)\n",
    "print(num + 1, \"\\t: #Processed\")\n",
    "\n",
    "print(counter,\"\\t: # of Anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anomaly_idx, sep=\",\")\n",
    "df.iloc[anomaly_idx].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_idx = 201\n",
    "print(df.text[check_idx])\n",
    "print(df.selected_text[check_idx])\n",
    "print([[i,j,k,l] for i,j,k,l in zip(X_tokens[check_idx].tokens,\n",
    "                                    X_tokens[check_idx].ids,\n",
    "                                    Y_starts[check_idx],\n",
    "                                    Y_stops[check_idx])])\n",
    "print([[i,j] for i,j in zip(Y_tokens[check_idx].ids,\n",
    "                            Y_tokens[check_idx].tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Validation  split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = [X[i] for i in train_idx], [X[i] for i in val_idx]\n",
    "X_att_train, X_att_val = [X_att[i] for i in train_idx], [X_att[i] for i in val_idx]\n",
    "\n",
    "Y_train, Y_val = [Y[i] for i in train_idx], [Y[i] for i in val_idx]\n",
    "Y_starts_train, Y_starts_val = [Y_starts[i] for i in train_idx], [Y_starts[i] for i in val_idx]\n",
    "Y_stops_train, Y_stops_val = [Y_stops[i] for i in train_idx], [Y_stops[i] for i in val_idx]\n",
    "\n",
    "print(\"\\n\",\n",
    "    len(X_train),\"\\t\",\": X_train\",\"\\n\",\n",
    "    len(X_att_train),\"\\t\",\": X_att_train\",\"\\n\",\n",
    "    len(Y_train),\"\\t\",\": Y_train\",\"\\n\",\n",
    "    len(Y_starts_train),\"\\t\",\": Y_starts_train\",\"\\n\",\n",
    "    len(Y_stops_train),\"\\t\",\": Y_stops_train\",\"\\n\",\n",
    "    len(X_val),\"\\t\",\": X_val\",\"\\n\",\n",
    "    len(X_att_val),\"\\t\",\": X_att_val\",\"\\n\",\n",
    "    len(Y_val),\"\\t\",\": Y_val\",\"\\n\",\n",
    "    len(Y_starts_val),\"\\t\",\": Y_starts_val\",\"\\n\",\n",
    "    len(Y_stops_val),\"\\t\",\": Y_stops_val\",\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=max_len, padding=\"post\")\n",
    "X_att_train = pad_sequences(X_att_train, maxlen=max_len, padding=\"post\")\n",
    "Y_train = pad_sequences(Y_train, maxlen=max_len, padding=\"post\")\n",
    "Y_starts_train = pad_sequences(Y_starts_train, maxlen=max_len, padding=\"post\")\n",
    "Y_stops_train = pad_sequences(Y_stops_train, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "X_val = pad_sequences(X_val, maxlen=max_len, padding=\"post\")\n",
    "X_att_val = pad_sequences(X_att_val, maxlen=max_len, padding=\"post\")\n",
    "Y_val = pad_sequences(Y_val, maxlen=max_len, padding=\"post\")\n",
    "Y_starts_val = pad_sequences(Y_starts_val, maxlen=max_len, padding=\"post\")\n",
    "Y_stops_val = pad_sequences(Y_stops_val, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding=\"post\")\n",
    "X_att_test = pad_sequences(X_att_test, maxlen=max_len, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\",\n",
    "     X_train.shape, \"\\t: X_train \", \"\\n\",\n",
    "     X_att_train.shape, \"\\t: X_att_train \", \"\\n\",\n",
    "     Y_train.shape, \"\\t: Y_train \", \"\\n\",\n",
    "     Y_starts_train.shape, \"\\t: Y_starts_train \", \"\\n\",\n",
    "     Y_stops_train.shape, \"\\t: Y_stops_train \", \"\\n\",\n",
    "\n",
    "     X_val.shape, \"\\t: X_val \", \"\\n\",\n",
    "     X_att_val.shape, \"\\t: X_att_val \", \"\\n\",\n",
    "     Y_val.shape, \"\\t: Y_val \", \"\\n\",\n",
    "     Y_starts_val.shape, \"\\t: Y_starts_val \", \"\\n\",\n",
    "     Y_stops_val.shape, \"\\t: Y_stops_val \", \"\\n\",\n",
    "\n",
    "     X_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for zero input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax([X_train.sum(axis=1)==0]), np.min([X_train.sum(axis=1)]))\n",
    "print(np.argmax([X_val.sum(axis=1)==0]), np.min([X_val.sum(axis=1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train\n",
    "num = 100\n",
    "[[i,j,k,l,m] for i,j,k,l,m in zip(X_train[num],\n",
    "                                  X_att_train[num],\n",
    "                                  Y_train[num],\n",
    "                                  Y_starts_train[num],\n",
    "                                  Y_stops_train[num])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Val\n",
    "num = 100\n",
    "[[i,j,k,l,m] for i,j,k,l,m in zip(X_val[num],\n",
    "                                  X_att_val[num],\n",
    "                                  Y_val[num],\n",
    "                                  Y_starts_val[num],\n",
    "                                  Y_stops_val[num])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "num = 100\n",
    "[[i,j,k] for i,j,k in zip(X_tokens_test[num].ids,\n",
    "                          X_test[num],\n",
    "                          X_tokens_test[num].attention_mask)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_att_flags = Input((max_len), name=\"att_flags\")\n",
    "input_sequences = Input((max_len), name=\"words\")\n",
    "\n",
    "emb_sequences = Embedding(input_dim=VOCAB_SIZE, input_length=max_len, output_dim=64, mask_zero=True)(input_sequences)\n",
    "\n",
    "seq = Bidirectional(LSTM(32, activation=None, return_sequences=True))(emb_sequences)\n",
    "seq = BatchNormalization()(seq)\n",
    "seq = Activation(\"relu\")(seq)\n",
    "seq = Dropout(DROPOUT)(seq)\n",
    "\n",
    "seq = Bidirectional(LSTM(32, activation=None, return_sequences=True))(seq)\n",
    "seq = BatchNormalization()(seq)\n",
    "seq = Activation(\"relu\")(seq)\n",
    "seq = Dropout(DROPOUT)(seq)\n",
    "\n",
    "seq = Bidirectional(LSTM(32, activation=None, return_sequences=False))(seq)\n",
    "#seq = Attention(max_len)(seq)\n",
    "seq = BatchNormalization()(seq)\n",
    "seq = Activation(\"relu\")(seq)\n",
    "seq = Dropout(DROPOUT)(seq)\n",
    "\n",
    "seq = Dense(max_len, activation=\"relu\")(seq)\n",
    "seq = BatchNormalization()(seq)\n",
    "seq = Dropout(DROPOUT)(seq)\n",
    "\n",
    "att = Dense(max_len, activation=\"relu\")(input_att_flags)\n",
    "att = BatchNormalization()(att)\n",
    "att = Dropout(DROPOUT)(att)\n",
    "\n",
    "convs = Conv1D(filters=32, kernel_size=8, padding=\"same\", activation=None)(emb_sequences)\n",
    "convs = BatchNormalization()(convs)\n",
    "convs = Activation(\"relu\")(convs)\n",
    "convs = MaxPooling1D(pool_size=2, strides=1)(convs)\n",
    "convs = Dropout(DROPOUT)(convs)\n",
    "\n",
    "convs = Conv1D(filters=32, kernel_size=8, padding=\"same\", activation=None)(convs)\n",
    "convs = BatchNormalization()(convs)\n",
    "convs = Activation(\"relu\")(convs)\n",
    "convs = MaxPooling1D(pool_size=2, strides=1)(convs)\n",
    "convs = Dropout(DROPOUT)(convs)\n",
    "\n",
    "convs = Conv1D(filters=32, kernel_size=8, padding=\"same\", activation=None)(convs)\n",
    "convs = BatchNormalization()(convs)\n",
    "convs = Activation(\"relu\")(convs)\n",
    "convs = MaxPooling1D(pool_size=2, strides=1)(convs)\n",
    "convs = Dropout(DROPOUT)(convs)\n",
    "\n",
    "convs = Flatten()(convs)\n",
    "convs = Dense(max_len, activation=None)(convs)\n",
    "convs = BatchNormalization()(convs)\n",
    "convs = Activation(\"relu\")(convs)\n",
    "convs = Dropout(DROPOUT)(convs)\n",
    "\n",
    "conv = Multiply()([att, convs])\n",
    "seq = Multiply()([att, seq])\n",
    "concat_layer = concatenate([conv, seq])\n",
    "\n",
    "output_starts = Dense(max_len, activation=None)(concat_layer)\n",
    "output_starts = BatchNormalization()(output_starts)\n",
    "output_starts = Activation(\"relu\")(output_starts)\n",
    "output_starts = Dropout(DROPOUT)(output_starts)\n",
    "\n",
    "output_starts = Dense(max_len, activation=None)(output_starts)\n",
    "output_starts = Activation(\"relu\")(output_starts)\n",
    "\n",
    "output_stops = Dense(max_len, activation=None)(concat_layer)\n",
    "output_stops = BatchNormalization()(output_stops)\n",
    "output_stops = Activation(\"relu\")(output_stops)\n",
    "output_stops = Dropout(DROPOUT)(output_stops)\n",
    "\n",
    "output_stops = Dense(max_len, activation=None)(output_stops)\n",
    "output_stops = Activation(\"relu\")(output_stops)\n",
    "\n",
    "output_starts = Dense(max_len, activation='softmax', name=\"starts\")(output_starts)\n",
    "output_stops = Dense(max_len, activation='softmax', name=\"stops\")(output_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model([input_att_flags, input_sequences], [output_starts, output_stops])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=MIN_LR)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=adam , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks\n",
    "mcp = ModelCheckpoint(filepath=\"../results/\"+MODEL_PREFIX+\"BestCheckpoint.h5\",\n",
    "                      monitor='val_loss',\n",
    "                      mode=\"auto\",\n",
    "                      save_weights_only=False,\n",
    "                      save_best_only=True)\n",
    "\n",
    "clr = CyclicLR(mode=CLR_METHOD,\n",
    "               base_lr=MIN_LR,\n",
    "               max_lr=MAX_LR,\n",
    "               step_size= STEP_SIZE * (X_train.shape[0] // BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x={\"att_flags\":X_att_train,\n",
    "                       \"words\":X_train},\n",
    "                    y={\"starts\":Y_starts_train.argmax(axis=1),\n",
    "                       \"stops\":Y_stops_train.argmax(axis=1)},\n",
    "                    shuffle=True,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=({\"att_flags\":X_att_val, \"words\":X_val},\n",
    "                                     {\"starts\":Y_starts_val.argmax(axis=1), \"stops\":Y_stops_val.argmax(axis=1)}),\n",
    "                    verbose=1,\n",
    "                    callbacks=[clr, mcp]) #es, rlrop, tb, mcp,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric_names = ['loss' ,'accuracy']\n",
    "metric_names = [\"loss\", \"starts_loss\", \"stops_loss\", \"starts_accuracy\", \"stops_accuracy\"]\n",
    "\n",
    "for i, j in zip(metric_names, ['val_'+i for i in metric_names]):\n",
    "    plt.plot(history.history[i])\n",
    "    plt.plot(history.history[j])\n",
    "    plt.title('Model '+i)\n",
    "    plt.ylabel(i.upper())\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../results/\"+MODEL_PREFIX+\"EndCheckpoint.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = tf.keras.models.load_model(\n",
    "    filepath=\"../results/\"+MODEL_PREFIX+\"BestCheckpoint.h5\",\n",
    "    compile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4973/4973 [==============================] - 2s 371us/sample - loss: 3.0774 - starts_loss: 1.5199 - stops_loss: 1.5401 - starts_accuracy: 0.6075 - stops_accuracy: 0.6105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.0773680700086956, 1.5198622, 1.5400939, 0.6074804, 0.6104967]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x = {\"att_flags\":X_att_val, \"words\":X_val},\n",
    "               y={\"starts\":Y_starts_val.argmax(axis=1), \"stops\":Y_stops_val.argmax(axis=1)},\n",
    "               batch_size=PREDICT_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(x = {\"att_flags\":X_att_train, \"words\":X_train},\n",
    "                           batch_size=PREDICT_BATCH_SIZE)\n",
    "pred_val = model.predict(x = {\"att_flags\":X_att_val, \"words\":X_val},\n",
    "                         batch_size=PREDICT_BATCH_SIZE)\n",
    "pred_test = model.predict(x = {\"att_flags\":X_att_test, \"words\":X_test},\n",
    "                          batch_size=PREDICT_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_starts_train, pred_stops_train = pred_train[0], pred_train[1]\n",
    "pred_starts_val, pred_stops_val = pred_val[0], pred_val[1]\n",
    "pred_starts_test, pred_stops_test = pred_test[0], pred_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19890, 108) (19890, 108)\n",
      "(4973, 108) (4973, 108)\n",
      "(3534, 108) (3534, 108)\n"
     ]
    }
   ],
   "source": [
    "print(pred_starts_train.shape, pred_stops_train.shape)\n",
    "print(pred_starts_val.shape, pred_stops_val.shape)\n",
    "print(pred_starts_test.shape, pred_stops_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {\n",
    "    \"train\":{\n",
    "        \"starts\":{\n",
    "            \"y_true\":Y_starts_train.argmax(axis=1),\n",
    "            \"y_pred\":pred_starts_train.argmax(axis=1)\n",
    "        },\n",
    "        \"stops\":{\n",
    "            \"y_true\":Y_stops_train.argmax(axis=1),\n",
    "            \"y_pred\":pred_stops_train.argmax(axis=1)\n",
    "        }\n",
    "    },\n",
    "    \"valid\":{\n",
    "        \"starts\":{\n",
    "            \"y_true\":Y_starts_val.argmax(axis=1),\n",
    "            \"y_pred\":pred_starts_val.argmax(axis=1)\n",
    "        },\n",
    "        \"stops\":{\n",
    "            \"y_true\":Y_stops_train.argmax(axis=1),\n",
    "            \"y_pred\":pred_stops_train.argmax(axis=1)\n",
    "        }        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.85 \t|| train \t|| starts \t|| accuracy_score\n",
      "64.14 \t|| train \t|| stops \t|| accuracy_score\n",
      "60.75 \t|| valid \t|| starts \t|| accuracy_score\n",
      "64.14 \t|| valid \t|| stops \t|| accuracy_score\n",
      "================================================================\n",
      "6.14 \t|| train \t|| starts \t|| f1_score\n",
      "54.61 \t|| train \t|| stops \t|| f1_score\n",
      "5.46 \t|| valid \t|| starts \t|| f1_score\n",
      "54.61 \t|| valid \t|| stops \t|| f1_score\n",
      "================================================================\n",
      "7.83 \t|| train \t|| starts \t|| precision_score\n",
      "54.92 \t|| train \t|| stops \t|| precision_score\n",
      "5.65 \t|| valid \t|| starts \t|| precision_score\n",
      "54.92 \t|| valid \t|| stops \t|| precision_score\n",
      "================================================================\n",
      "6.60 \t|| train \t|| starts \t|| recall_score\n",
      "58.50 \t|| train \t|| stops \t|| recall_score\n",
      "6.11 \t|| valid \t|| starts \t|| recall_score\n",
      "58.50 \t|| valid \t|| stops \t|| recall_score\n",
      "================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V15_train_starts.csv\n",
      "\t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V15_train_stops.csv\n",
      "\t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V15_valid_starts.csv\n",
      "\t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V15_valid_stops.csv\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "def print_metrics():\n",
    "    funcs = [accuracy_score, f1_score, precision_score, recall_score, confusion_matrix]\n",
    "    for f in funcs:\n",
    "        for data_set in [\"train\",\"valid\"]:\n",
    "            for var in [\"starts\", \"stops\"]:\n",
    "                if f in [accuracy_score]:\n",
    "                    res = f(**preds[data_set][var])\n",
    "                    print(\"{:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "                elif f in [confusion_matrix]:\n",
    "                    res = f(**preds[data_set][var], labels = np.arange(max_len))\n",
    "                    np.savetxt(X=res, fmt='%i', delimiter=\",\",\n",
    "                               fname=\"../results/ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                    print(\"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__, \"\\t||\", \n",
    "                          \"../results/ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                else:\n",
    "                    res = f(**preds[data_set][var], average=\"macro\")\n",
    "                    print(\"{:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "        print(\"================================================================\")\n",
    "\n",
    "print_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_words_train = [tokenizer.decode(i) for i in Y_train]\n",
    "Y_words_val = [tokenizer.decode(i) for i in Y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19890 19780\n",
      "4973 4956\n",
      "3534 3520\n"
     ]
    }
   ],
   "source": [
    "print(pred_starts_train.shape[0],\n",
    "      sum([s<e for s,e in zip(pred_starts_train.argmax(axis=1),\n",
    "                              pred_stops_train.argmax(axis=1))]))\n",
    "print(pred_starts_val.shape[0],\n",
    "      sum([s<e for s,e in zip(pred_starts_val.argmax(axis=1),\n",
    "                              pred_stops_val.argmax(axis=1))]))\n",
    "print(pred_starts_test.shape[0],\n",
    "      sum([s<e for s,e in zip(pred_starts_test.argmax(axis=1),\n",
    "                              pred_stops_test.argmax(axis=1))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(string):\n",
    "    string = re.sub(pattern=\" (negative|positive|neutral)[ ]+$\", repl=\"\", string=string)\n",
    "    string = re.sub(pattern=\" +\", repl=\" \", string=string.strip())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words_train = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_train, pred_starts_train.argmax(axis=1), pred_stops_train.argmax(axis=1))]\n",
    "pred_words_val = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_val, pred_starts_val.argmax(axis=1), pred_stops_val.argmax(axis=1))]\n",
    "pred_words_test = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_test, pred_starts_test.argmax(axis=1), pred_stops_test.argmax(axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    a = set(str1)\n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Jaccard Score 0.703761448202351\n",
      "Validation Jaccard Score 0.6879099603285186\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Jaccard Score\", np.mean([jaccard(str1=i, str2=j) for i,j in zip(Y_words_train, pred_words_train)]))\n",
    "print(\"Validation Jaccard Score\", np.mean([jaccard(str1=i, str2=j) for i,j in zip(Y_words_val, pred_words_val)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_labels = {i:j for i,j in enumerate(df.sentiment_code.cat.categories)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['selected_text'] = pred_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>last session of the day http://twitpic.com/67ez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "      <td>shanghai is also really exciting (precisely -- skyscrapers galore). good tweeps in china: (sh) (bj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recession hit Veronique Branquinho, she has to quit her company, such a shame!</td>\n",
       "      <td>negative</td>\n",
       "      <td>recession hit veronique branquinho, she has to quit her company, such a shame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy bday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>i like it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>that`s great!! weee!! visitors!</td>\n",
       "      <td>positive</td>\n",
       "      <td>that`s great!! weee!! visitors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I THINK EVERYONE HATES ME ON HERE   lol</td>\n",
       "      <td>negative</td>\n",
       "      <td>i think everyone hates me on here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>soooooo wish i could, but im in school and myspace is completely blocked</td>\n",
       "      <td>negative</td>\n",
       "      <td>soooooo wish i could, but im in school and myspace is completely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and within a short time of the last clue all of them</td>\n",
       "      <td>neutral</td>\n",
       "      <td>and within a short time of the last clue all of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What did you get?  My day is alright.. haven`t done anything yet. leaving soon to my stepsister though!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>what did you get? my day is alright.. haven`t done anything yet. leaving soon to my stepsister though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>My bike was put on hold...should have known that.... argh total bummer</td>\n",
       "      <td>negative</td>\n",
       "      <td>.... argh total</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I checked.  We didn`t win</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i checked. we didn`t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>.. and you`re on twitter! Did the tavern bore you that much?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>.. and you`re on twitter! did the tavern bore you that much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I`m in VA for the weekend, my youngest son turns 2 tomorrow......it makes me kinda sad, he is getting so big, check out my twipics</td>\n",
       "      <td>negative</td>\n",
       "      <td>i`m in va for the weekend, my youngest son turns 2 tomorrow......it makes me kinda sad, he is getting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Its coming out the socket  I feel like my phones hole is not a virgin. That`s how loose it is... :`(</td>\n",
       "      <td>negative</td>\n",
       "      <td>its coming out the socket i feel like my phones hole is not a virgin. that`s how loose it is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                  text  \\\n",
       "0   Last session of the day  http://twitpic.com/67ezh                                                                                    \n",
       "1    Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).                              \n",
       "2   Recession hit Veronique Branquinho, she has to quit her company, such a shame!                                                       \n",
       "3    happy bday!                                                                                                                         \n",
       "4    http://twitpic.com/4w75p - I like it!!                                                                                              \n",
       "5    that`s great!! weee!! visitors!                                                                                                     \n",
       "6   I THINK EVERYONE HATES ME ON HERE   lol                                                                                              \n",
       "7    soooooo wish i could, but im in school and myspace is completely blocked                                                            \n",
       "8    and within a short time of the last clue all of them                                                                                \n",
       "9    What did you get?  My day is alright.. haven`t done anything yet. leaving soon to my stepsister though!                             \n",
       "10  My bike was put on hold...should have known that.... argh total bummer                                                               \n",
       "11   I checked.  We didn`t win                                                                                                           \n",
       "12   .. and you`re on twitter! Did the tavern bore you that much?                                                                        \n",
       "13  I`m in VA for the weekend, my youngest son turns 2 tomorrow......it makes me kinda sad, he is getting so big, check out my twipics   \n",
       "14  Its coming out the socket  I feel like my phones hole is not a virgin. That`s how loose it is... :`(                                 \n",
       "\n",
       "   sentiment  \\\n",
       "0   neutral    \n",
       "1   positive   \n",
       "2   negative   \n",
       "3   positive   \n",
       "4   positive   \n",
       "5   positive   \n",
       "6   negative   \n",
       "7   negative   \n",
       "8   neutral    \n",
       "9   neutral    \n",
       "10  negative   \n",
       "11  neutral    \n",
       "12  neutral    \n",
       "13  negative   \n",
       "14  negative   \n",
       "\n",
       "                                                                                            selected_text  \n",
       "0   last session of the day http://twitpic.com/67ez                                                        \n",
       "1   shanghai is also really exciting (precisely -- skyscrapers galore). good tweeps in china: (sh) (bj     \n",
       "2   recession hit veronique branquinho, she has to quit her company, such a shame                          \n",
       "3   happy bday                                                                                             \n",
       "4   i like it                                                                                              \n",
       "5   that`s great!! weee!! visitors                                                                         \n",
       "6   i think everyone hates me on here                                                                      \n",
       "7   soooooo wish i could, but im in school and myspace is completely                                       \n",
       "8   and within a short time of the last clue all of                                                        \n",
       "9   what did you get? my day is alright.. haven`t done anything yet. leaving soon to my stepsister though  \n",
       "10  .... argh total                                                                                        \n",
       "11  i checked. we didn`t                                                                                   \n",
       "12  .. and you`re on twitter! did the tavern bore you that much                                            \n",
       "13  i`m in va for the weekend, my youngest son turns 2 tomorrow......it makes me kinda sad, he is getting  \n",
       "14  its coming out the socket i feel like my phones hole is not a virgin. that`s how loose it is...        "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[[\"text\", \"sentiment\",\"selected_text\"]].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[[\"textID\", \"selected_text\"]].to_csv(\"../results/submission_\"+MODEL_PREFIX+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 28 12:41:59 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spot Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spot_check(index_to_check, mode):\n",
    "    print(\"================== Sample #\", index_to_check, \"==================\")\n",
    "    if mode == \"train\":\n",
    "        print(\"Selected_text:\")\n",
    "        print('\\t', Y_words[train_idx[index_to_check]])\n",
    "        print(\"Predicted_text:\")\n",
    "        print('\\t', pred_words_train[index_to_check])\n",
    "        print(\"Predictions:\")\n",
    "        print('\\t',*[[\"%.2f\"%j, \n",
    "                      \"%.2f\"%k,\n",
    "                      i,\n",
    "                      X_words[train_idx[index_to_check]][num]] for num, (i,j,k) in enumerate(zip(X_train[index_to_check].tolist(),\n",
    "                                                                                               pred_starts_train[index_to_check].tolist(),\n",
    "                                                                                               pred_stops_train[index_to_check].tolist())) if i!=0], sep=\"\\n\\t\")\n",
    "        \n",
    "    elif mode == \"validation\":\n",
    "        print(\"Selected_text:\")\n",
    "        print('\\t', Y_words[val_idx[index_to_check]])\n",
    "        print(\"Predicted_text:\")\n",
    "        print('\\t', pred_words_val[index_to_check])\n",
    "        print(\"Predictions:\")\n",
    "        print('\\t',*[[\"%.2f\"%j, \n",
    "                      \"%.2f\"%k,\n",
    "                      i,\n",
    "                      X_words[val_idx[index_to_check]][num]] for num, (i,j,k) in enumerate(zip(X_val[index_to_check].tolist(),\n",
    "                                                                                               pred_starts_val[index_to_check].tolist(),\n",
    "                                                                                               pred_stops_val[index_to_check].tolist())) if i!=0], sep=\"\\n\\t\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Selected_text:\")\n",
    "        print('\\t', \"Not Available\")\n",
    "        print(\"Predicted_text:\")\n",
    "        print('\\t', pred_words_test[index_to_check])\n",
    "        print(\"Predictions:\")\n",
    "        print('\\t',*[[\"%.2f\"%j, \n",
    "                      \"%.2f\"%k,\n",
    "                      i,\n",
    "                      X_words_test[index_to_check][num]] for num, (i,j,k) in enumerate(zip(X_test[index_to_check].tolist(),\n",
    "                                                                                               pred_starts_test[index_to_check].tolist(),\n",
    "                                                                                               pred_stops_test[index_to_check].tolist())) if i!=0], sep=\"\\n\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10,20):\n",
    "    spot_check(i, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200,210):\n",
    "    spot_check(i, mode=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "    spot_check(i, mode=\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
