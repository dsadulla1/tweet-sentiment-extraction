{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V21\"\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.8\n",
    "BATCH_SIZE = 16\n",
    "PREDICT_BATCH_SIZE = 512\n",
    "DROPOUT = 0.3\n",
    "\n",
    "RUN_ON_SAMPLE = False\n",
    "EXCLUDE_NEUTRAL_CLASS = True\n",
    "MAX_LR = 5e-3\n",
    "MID_LR = 1e-4\n",
    "MIN_LR = 1e-6\n",
    "NUM_EPOCHS = [10, 5, 2]\n",
    "NUM_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, MaxPooling1D, Layer, AdditiveAttention, Attention\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    a = set(str1)\n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(98765)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 17 03:49:39 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0', '2.8.0', '0.5.2']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\"  target=\"_blank\"><h2 id=\"limiting_gpu_memory_growth\" data-text=\"Limiting GPU memory growth\" tabindex=\"0\">Limiting GPU memory growth</h2></a>\n",
    "<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\"><code translate=\"no\" dir=\"ltr\">CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the <code translate=\"no\" dir=\"ltr\">tf.config.experimental.set_visible_devices</code> method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it okay to exclude neutral text from models and force it later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVERAGE: 0.5334592552617378\n",
      "MEAN JACCARD: 0.9808316180492903\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7094</th>\n",
       "      <td>588e531efe</td>\n",
       "      <td>Moooorning! Fancy a coffee?</td>\n",
       "      <td>Moooorning! Fancy a coffee?</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>dd01108dba</td>\n",
       "      <td>....... i`m wondering if you`re as awake as i am. ?</td>\n",
       "      <td>i`m wondering if you`re as awake as i am. ?</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>e1c569d4cf</td>\n",
       "      <td>Will certainly do that.</td>\n",
       "      <td>Will certainly do that.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>208cb66bdd</td>\n",
       "      <td>Crab boat? I have my mobile set to go off every night, to remind me to head to bed</td>\n",
       "      <td>Crab boat? I have my mobile set to go off every night, to remind me to head to bed</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10481</th>\n",
       "      <td>9a4f40c0b4</td>\n",
       "      <td>yes, yes it was</td>\n",
       "      <td>yes, yes it was</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID  \\\n",
       "7094   588e531efe   \n",
       "4148   dd01108dba   \n",
       "3430   e1c569d4cf   \n",
       "4714   208cb66bdd   \n",
       "10481  9a4f40c0b4   \n",
       "\n",
       "                                                                                      text  \\\n",
       "7094                                                           Moooorning! Fancy a coffee?   \n",
       "4148                                   ....... i`m wondering if you`re as awake as i am. ?   \n",
       "3430                                                               Will certainly do that.   \n",
       "4714    Crab boat? I have my mobile set to go off every night, to remind me to head to bed   \n",
       "10481                                                                      yes, yes it was   \n",
       "\n",
       "                                                                            selected_text  \\\n",
       "7094                                                          Moooorning! Fancy a coffee?   \n",
       "4148                                          i`m wondering if you`re as awake as i am. ?   \n",
       "3430                                                              Will certainly do that.   \n",
       "4714   Crab boat? I have my mobile set to go off every night, to remind me to head to bed   \n",
       "10481                                                                     yes, yes it was   \n",
       "\n",
       "      sentiment  \n",
       "7094    neutral  \n",
       "4148    neutral  \n",
       "3430    neutral  \n",
       "4714    neutral  \n",
       "10481   neutral  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"../data/train.csv\", encoding=\"utf8\")\n",
    "# cases where neutral's text and selected text columns are the same\n",
    "neutrals = df2.loc[df2.sentiment==\"neutral\"].copy()\n",
    "neutrals = neutrals.reset_index(drop=True)\n",
    "neutrals[\"text\"] = neutrals[\"text\"].astype(str)\n",
    "neutrals[\"selected_text\"] = neutrals[\"selected_text\"].astype(str)\n",
    "print(\"COVERAGE:\", np.sum(np.where((neutrals.text == neutrals.selected_text), 1, 0))/neutrals.shape[0])\n",
    "print(\"MEAN JACCARD:\", np.mean([jaccard(str1=i, str2=j) for i,j in zip(neutrals.selected_text,neutrals.text)]))\n",
    "\n",
    "neutrals.loc[neutrals.text != neutrals.selected_text].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  1\n",
      "selected_text  object  1\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27480, 'selected_text': 22463, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count        27481   \n",
      "unique       27481   \n",
      "top     b0a00379a7   \n",
      "freq             1   \n",
      "\n",
      "                                                                                                                           text  \\\n",
      "count                                                                                                                     27480   \n",
      "unique                                                                                                                    27480   \n",
      "top     Hmm...what to make for dinner tonight???  No clue.  Don`t feel like cooking anything    Hubby wants to go away tonight.   \n",
      "freq                                                                                                                          1   \n",
      "\n",
      "       selected_text sentiment  \n",
      "count          27480     27481  \n",
      "unique         22463         3  \n",
      "top             good   neutral  \n",
      "freq             199     11118  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\", encoding=\"utf8\")\n",
    "\n",
    "print(pd.concat((df.dtypes, df.isna().sum()), axis=1))\n",
    "print(df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df[i].nunique() for i in df.columns})\n",
    "print(df.describe())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count         3534   \n",
      "unique        3534   \n",
      "top     664d148574   \n",
      "freq             1   \n",
      "\n",
      "                                                                  text  \\\n",
      "count                                                             3534   \n",
      "unique                                                            3534   \n",
      "top     http://twitpic.com/4wi9p - playing with ethan. i love you baby   \n",
      "freq                                                                 1   \n",
      "\n",
      "       sentiment  \n",
      "count       3534  \n",
      "unique         3  \n",
      "top      neutral  \n",
      "freq        1430  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "print(pd.concat((test_df.dtypes, test_df.isna().sum()), axis=1))\n",
    "print(test_df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df[i].nunique() for i in test_df.columns})\n",
    "print(test_df.describe())\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>c77717b103</td>\n",
       "      <td>I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½</td>\n",
       "      <td>I love to!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>28dbada620</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "44   c77717b103   \n",
       "192  28dbada620   \n",
       "\n",
       "                                                                                                       text  \\\n",
       "44    I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½   \n",
       "192                                *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "                                                                selected_text  \\\n",
       "44                                                                 I love to!   \n",
       "192  *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "    sentiment  \n",
       "44   positive  \n",
       "192   neutral  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>7223fdccc2</td>\n",
       "      <td>tikcets are only ï¿½91...each...BUT I SO WANT TO GO</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>43ad351369</td>\n",
       "      <td>AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "145  7223fdccc2   \n",
       "618  43ad351369   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "145                                                                                             tikcets are only ï¿½91...each...BUT I SO WANT TO GO   \n",
       "618  AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s   \n",
       "\n",
       "    sentiment  \n",
       "145  positive  \n",
       "618  positive  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[test_df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment count in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>7781</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>11117</td>\n",
       "      <td>1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>8582</td>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text  text\n",
       "sentiment             \n",
       "negative    7781  1001\n",
       "neutral    11117  1430\n",
       "positive    8582  1103"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df.groupby(\"sentiment\")[[\"text\"]].count(), test_df.groupby(\"sentiment\")[[\"text\"]].count()], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For traceability\n",
    "df[\"original_index\"] = df.index\n",
    "test_df[\"original_index\"] = test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_idxs = [513, 1060, 5543, 8410, 13605, 24778, 25258] + [18, 27, 32, 39, 48, 49, 64, 66, 84, 92, 102, 116, 129, 132, 134, 149, 160, 166, 168, 189, 196, 197, 210, 223, 247, 251, 260, 285, 295, 296, 297, 309, 349, 362, 366, 382, 396, 398, 406, 410, 425, 458, 460, 482, 492, 500, 504, 517, 533, 537, 543, 569, 573, 581, 604, 605, 608, 634, 636, 637, 639, 642, 665, 670, 674, 678, 685, 702, 707, 709, 710, 727, 737, 746, 751, 757, 768, 778, 787, 794, 804, 809, 853, 854, 856, 863, 871, 872, 912, 931, 937, 942, 950, 956, 957, 963, 992, 993, 999, 1001, 1011, 1012, 1036, 1039, 1057, 1062, 1076, 1077, 1083, 1096, 1105, 1116, 1117, 1122, 1134, 1137, 1150, 1159, 1199, 1200, 1209, 1217, 1264, 1271, 1280, 1283, 1298, 1300, 1303, 1319, 1327, 1342, 1360, 1363, 1365, 1372, 1374, 1376, 1382, 1393, 1416, 1417, 1420, 1447, 1454, 1468, 1472, 1513, 1515, 1518, 1531, 1548, 1558, 1567, 1580, 1588, 1605, 1623, 1648, 1649, 1657, 1659, 1661, 1665, 1672, 1674, 1678, 1683, 1693, 1696, 1713, 1723, 1728, 1739, 1747, 1754, 1798, 1806, 1808, 1814, 1822, 1828, 1830, 1835, 1850, 1857, 1860, 1866, 1888, 1901, 1902, 1929, 1938, 1940, 1987, 1994, 2003, 2004, 2008, 2011, 2023, 2054, 2056, 2065, 2068, 2083, 2086, 2094, 2106, 2113, 2124, 2125, 2136, 2139, 2145, 2168, 2180, 2181, 2187, 2213, 2225, 2227, 2230, 2239, 2273, 2279, 2286, 2288, 2298, 2323, 2324, 2335, 2343, 2359, 2364, 2371, 2385, 2388, 2392, 2398, 2401, 2404, 2410, 2415, 2421, 2430, 2438, 2439, 2440, 2444, 2445, 2448, 2470, 2478, 2489, 2499, 2505, 2515, 2520, 2559, 2574, 2576, 2589, 2598, 2600, 2640, 2643, 2650, 2671, 2679, 2696, 2699, 2706, 2716, 2718, 2731, 2743, 2749, 2750, 2785, 2787, 2789, 2805, 2809, 2825, 2838, 2856, 2888, 2898, 2903, 2923, 2925, 2931, 2933, 2942, 2957, 2961, 2986, 2995, 3034, 3068, 3075, 3077, 3080, 3088, 3089, 3096, 3097, 3101, 3121, 3147, 3160, 3161, 3176, 3182, 3187, 3211, 3225, 3234, 3257, 3263, 3267, 3281, 3290, 3295, 3317, 3319, 3325, 3333, 3359, 3364, 3369, 3374, 3390, 3399, 3400, 3411, 3424, 3473, 3475, 3476, 3479, 3492, 3499, 3504, 3509, 3517, 3519, 3532, 3553, 3578, 3588, 3590, 3593, 3598, 3605, 3606, 3607, 3622, 3634, 3669, 3672, 3678, 3682, 3685, 3694, 3708, 3720, 3729, 3750, 3755, 3784, 3794, 3798, 3801, 3806, 3807, 3813, 3826, 3834, 3872, 3886, 3895, 3903, 3925, 3937, 3946, 3962, 3969, 3998, 4031, 4047, 4053, 4074, 4091, 4093, 4100, 4108, 4113, 4119, 4143, 4148, 4163, 4164, 4183, 4196, 4254, 4260, 4265, 4307, 4308, 4309, 4312, 4318, 4325, 4327, 4328, 4329, 4344, 4347, 4369, 4371, 4377, 4384, 4402, 4404, 4405, 4407, 4412, 4421, 4422, 4437, 4455, 4457, 4471, 4473, 4479, 4483, 4486, 4514, 4532, 4537, 4542, 4543, 4545, 4550, 4554, 4563, 4575, 4576, 4591, 4595, 4631, 4636, 4637, 4639, 4654, 4655, 4704, 4706, 4714, 4718, 4724, 4729, 4735, 4747, 4749, 4750, 4764, 4800, 4814, 4841, 4842, 4844, 4864, 4866, 4869, 4875, 4891, 4908, 4918, 4924, 4928, 4933, 4943, 4946, 4947, 4956, 4968, 4980, 4994, 5009, 5011, 5038, 5056, 5079, 5083, 5101, 5133, 5167, 5173, 5174, 5188, 5189, 5196, 5197, 5213, 5218, 5229, 5241, 5245, 5258, 5279, 5290, 5294, 5297, 5308, 5317, 5331, 5342, 5343, 5353, 5355, 5356, 5358, 5379, 5385, 5386, 5399, 5418, 5433, 5436, 5450, 5456, 5466, 5489, 5504, 5509, 5510, 5530, 5542, 5560, 5583, 5603, 5607, 5618, 5620, 5624, 5626, 5678, 5697, 5712, 5725, 5751, 5757, 5775, 5785, 5799, 5822, 5832, 5836, 5844, 5895, 5902, 5904, 5913, 5918, 5938, 5939, 5961, 5984, 5990, 6025, 6031, 6033, 6060, 6077, 6084, 6090, 6113, 6131, 6134, 6151, 6152, 6176, 6203, 6229, 6230, 6248, 6261, 6277, 6289, 6296, 6304, 6308, 6317, 6319, 6321, 6335, 6338, 6343, 6353, 6356, 6360, 6377, 6382, 6395, 6404, 6425, 6433, 6468, 6474, 6475, 6476, 6477, 6510, 6528, 6540, 6574, 6580, 6582, 6593, 6609, 6627, 6628, 6633, 6635, 6650, 6656, 6657, 6670, 6673, 6676, 6677, 6679, 6686, 6702, 6724, 6740, 6745, 6759, 6801, 6804, 6830, 6833, 6834, 6842, 6859, 6871, 6875, 6878, 6880, 6885, 6896, 6934, 6939, 6948, 6950, 6958, 6970, 6979, 6987, 7010, 7017, 7024, 7040, 7099, 7107, 7112, 7136, 7147, 7149, 7158, 7184, 7225, 7227, 7251, 7274, 7288, 7305, 7306, 7308, 7331, 7344, 7355, 7373, 7407, 7408, 7409, 7410, 7423, 7436, 7438, 7442, 7444, 7450, 7469, 7473, 7479, 7487, 7492, 7506, 7513, 7521, 7528, 7530, 7546, 7567, 7581, 7582, 7597, 7599, 7602, 7632, 7635, 7636, 7642, 7662, 7663, 7701, 7752, 7758, 7765, 7769, 7778, 7780, 7788, 7801, 7816, 7818, 7831, 7836, 7837, 7849, 7893, 7895, 7896, 7922, 7927, 7945, 7948, 7968, 7997, 8003, 8005, 8010, 8030, 8049, 8051, 8052, 8059, 8060, 8085, 8094, 8142, 8143, 8151, 8153, 8161, 8169, 8170, 8196, 8208, 8212, 8215, 8224, 8235, 8241, 8249, 8250, 8276, 8282, 8285, 8286, 8288, 8289, 8299, 8314, 8338, 8343, 8345, 8349, 8373, 8379, 8390, 8397, 8399, 8407, 8427, 8428, 8432, 8439, 8442, 8444, 8484, 8493, 8497, 8500, 8507, 8510, 8534, 8546, 8559, 8569, 8579, 8588, 8594, 8604, 8617, 8620, 8624, 8631, 8638, 8661, 8671, 8674, 8687, 8691, 8696, 8706, 8720, 8729, 8749, 8756, 8758, 8769, 8774, 8780, 8800, 8803, 8809, 8815, 8827, 8830, 8875, 8902, 8906, 8910, 8917, 8919, 8922, 8951, 8954, 8968, 8985, 8986, 8991, 8999, 9014, 9034, 9050, 9057, 9063, 9070, 9088, 9098, 9112, 9113, 9122, 9127, 9140, 9155, 9157, 9169, 9179, 9190, 9216, 9223, 9226, 9264, 9272, 9281, 9297, 9300, 9303, 9322, 9354, 9361, 9374, 9377, 9381, 9385, 9400, 9403, 9419, 9422, 9425, 9428, 9436, 9442, 9449, 9454, 9456, 9458, 9470, 9482, 9496, 9500, 9535, 9539, 9541, 9554, 9556, 9560, 9565, 9574, 9594, 9605, 9610, 9631, 9632, 9660, 9689, 9691, 9696, 9708, 9710, 9712, 9725, 9726, 9734, 9742, 9753, 9770, 9789, 9799, 9801, 9812, 9817, 9836, 9839, 9850, 9852, 9881, 9882, 9891, 9906, 9912, 9920, 9922, 9928, 9976, 9980, 9984, 10002, 10004, 10007, 10029, 10039, 10042, 10050, 10052, 10057, 10067, 10070, 10092, 10093, 10123, 10131, 10138, 10145, 10151, 10164, 10167, 10170, 10179, 10184, 10185, 10190, 10197, 10200, 10211, 10218, 10219, 10222, 10228, 10245, 10247, 10255, 10259, 10276, 10293, 10298, 10299, 10306, 10319, 10327, 10342, 10348, 10377, 10379, 10449, 10465, 10469, 10492, 10504, 10505, 10508, 10521, 10528, 10529, 10530, 10570, 10573, 10577, 10578, 10597, 10611, 10639, 10651, 10666, 10672, 10675, 10690, 10706, 10711, 10738, 10742, 10747, 10749, 10779, 10782, 10783, 10795, 10813, 10849, 10852, 10853, 10884, 10885, 10888, 10892, 10901, 10914, 10918, 10919, 10933, 10935, 10956, 10966, 10968, 10970, 10981, 10986, 10996, 10999, 11005, 11032, 11038, 11051, 11055, 11060, 11067, 11097, 11102, 11155, 11157, 11159, 11162, 11167, 11176, 11187, 11215, 11222, 11228, 11230, 11257, 11263, 11265, 11274, 11286, 11289, 11297, 11300, 11308, 11323, 11337, 11339, 11341, 11347, 11349, 11353, 11371, 11381, 11386, 11426, 11431, 11448, 11449, 11474, 11480, 11500, 11507, 11553, 11562, 11582, 11588, 11594, 11612, 11616, 11618, 11636, 11643, 11650, 11658, 11666, 11679, 11683, 11693, 11695, 11698, 11699, 11706, 11716, 11723, 11730, 11732, 11741, 11745, 11780, 11794, 11798, 11808, 11812, 11828, 11829, 11837, 11840, 11841, 11844, 11849, 11853, 11854, 11855, 11861, 11862, 11872, 11875, 11878, 11886, 11890, 11899, 11905, 11912, 11938, 11963, 11967, 11973, 11980, 11985, 11999, 12026, 12027, 12029, 12039, 12054, 12067, 12115, 12124, 12134, 12136, 12138, 12150, 12187, 12205, 12206, 12224, 12242, 12258, 12269, 12277, 12283, 12295, 12305, 12314, 12322, 12331, 12333, 12334, 12350, 12355, 12356, 12360, 12370, 12372, 12389, 12395, 12396, 12397, 12405, 12410, 12416, 12429, 12439, 12440, 12442, 12446, 12474, 12482, 12486, 12507, 12516, 12521, 12522, 12526, 12527, 12537, 12550, 12563, 12576, 12585, 12586, 12587, 12598, 12606, 12630, 12631, 12635, 12639, 12650, 12657, 12662, 12669, 12687, 12703, 12718, 12724, 12732, 12736, 12745, 12767, 12787, 12803, 12804, 12808, 12818, 12830, 12843, 12855, 12856, 12870, 12884, 12903, 12914, 12920, 12923, 12924, 12927, 12936, 12972, 12977, 12998, 13003, 13004, 13009, 13044, 13068, 13069, 13074, 13080, 13095, 13098, 13124, 13130, 13153, 13154, 13176, 13219, 13222, 13227, 13237, 13267, 13274, 13293, 13301, 13304, 13307, 13330, 13333, 13354, 13365, 13373, 13379, 13381, 13389, 13397, 13422, 13430, 13433, 13445, 13466, 13473, 13476, 13482, 13490, 13493, 13528, 13529, 13535, 13543, 13558, 13559, 13586, 13637, 13643, 13646, 13665, 13674, 13678, 13679, 13704, 13707, 13710, 13718, 13727, 13733, 13747, 13762, 13772, 13785, 13796, 13803, 13812, 13814, 13817, 13827, 13844, 13846, 13847, 13848, 13854, 13855, 13861, 13864, 13873, 13899, 13907, 13936, 13939, 13946, 13949, 13954, 13964, 13965, 13974, 13975, 13978, 13980, 14002, 14006, 14007, 14030, 14043, 14044, 14057, 14058, 14060, 14073, 14091, 14109, 14110, 14112, 14117, 14131, 14132, 14159, 14172, 14173, 14176, 14184, 14194, 14197, 14200, 14204, 14207, 14213, 14230, 14257, 14273, 14275, 14287, 14299, 14301, 14307, 14308, 14347, 14362, 14365, 14381, 14386, 14400, 14416, 14428, 14436, 14446, 14457, 14458, 14462, 14487, 14499, 14509, 14532, 14548, 14551, 14571, 14596, 14598, 14611, 14621, 14639, 14640, 14648, 14662, 14669, 14671, 14676, 14680, 14687, 14689, 14691, 14718, 14731, 14747, 14779, 14787, 14825, 14839, 14844, 14847, 14855, 14870, 14880, 14891, 14908, 14946, 14959, 14960, 14967, 14971, 14986, 15007, 15010, 15016, 15044, 15049, 15055, 15056, 15112, 15131, 15158, 15165, 15174, 15177, 15196, 15203, 15206, 15207, 15213, 15217, 15233, 15236, 15253, 15259, 15286, 15296, 15307, 15308, 15321, 15323, 15327, 15339, 15363, 15372, 15374, 15379, 15380, 15383, 15385, 15392, 15410, 15449, 15452, 15462, 15464, 15465, 15477, 15483, 15503, 15505, 15528, 15532, 15548, 15562, 15566, 15571, 15605, 15610, 15622, 15651, 15660, 15666, 15674, 15709, 15721, 15731, 15735, 15736, 15741, 15767, 15774, 15785, 15792, 15793, 15811, 15851, 15879, 15883, 15884, 15912, 15926, 15927, 15931, 15940, 15959, 15965, 15985, 15998, 16020, 16027, 16031, 16032, 16044, 16070, 16100, 16117, 16129, 16136, 16137, 16140, 16149, 16152, 16175, 16186, 16201, 16241, 16261, 16269, 16272, 16279, 16290, 16308, 16319, 16324, 16325, 16327, 16338, 16341, 16345, 16352, 16354, 16371, 16372, 16385, 16391, 16416, 16423, 16448, 16455, 16468, 16470, 16477, 16481, 16484, 16491, 16493, 16500, 16509, 16510, 16516, 16541, 16559, 16577, 16589, 16632, 16638, 16642, 16643, 16649, 16650, 16654, 16665, 16677, 16684, 16704, 16720, 16726, 16734, 16765, 16774, 16775, 16777, 16780, 16797, 16798, 16799, 16803, 16807, 16809, 16830, 16832, 16861, 16863, 16868, 16876, 16889, 16890, 16902, 16915, 16927, 16939, 16949, 16962, 16968, 17002, 17010, 17013, 17021, 17023, 17029, 17062, 17066, 17084, 17093, 17094, 17102, 17152, 17160, 17167, 17173, 17183, 17221, 17239, 17250, 17267, 17277, 17278, 17282, 17296, 17307, 17315, 17333, 17336, 17344, 17346, 17348, 17354, 17365, 17374, 17385, 17387, 17397, 17401, 17404, 17412, 17417, 17432, 17436, 17444, 17452, 17467, 17469, 17501, 17502, 17513, 17530, 17531, 17547, 17550, 17555, 17573, 17577, 17578, 17586, 17600, 17602, 17611, 17627, 17640, 17645, 17648, 17649, 17652, 17667, 17676, 17682, 17721, 17724, 17729, 17731, 17751, 17754, 17762, 17770, 17772, 17781, 17802, 17820, 17823, 17833, 17848, 17857, 17858, 17865, 17867, 17887, 17893, 17909, 17924, 17926, 17945, 17971, 17977, 17986, 17990, 18003, 18011, 18042, 18046, 18052, 18077, 18086, 18091, 18099, 18103, 18123, 18128, 18131, 18142, 18153, 18165, 18167, 18173, 18181, 18206, 18226, 18252, 18263, 18276, 18277, 18284, 18314, 18319, 18327, 18330, 18342, 18352, 18355, 18362, 18364, 18375, 18383, 18398, 18403, 18432, 18447, 18453, 18465, 18467, 18485, 18486, 18492, 18505, 18508, 18510, 18515, 18536, 18540, 18548, 18550, 18557, 18563, 18570, 18574, 18576, 18616, 18622, 18627, 18639, 18649, 18655, 18673, 18681, 18712, 18745, 18746, 18758, 18773, 18778, 18782, 18826, 18832, 18834, 18842, 18851, 18853, 18858, 18862, 18867, 18872, 18877, 18908, 18910, 18912, 18913, 18919, 18925, 18929, 18930, 18933, 18936, 18944, 18974, 18975, 18997, 19009, 19011, 19014, 19016, 19028, 19057, 19098, 19106, 19114, 19130, 19133, 19147, 19157, 19162, 19166, 19173, 19179, 19198, 19213, 19225, 19230, 19239, 19241, 19266, 19274, 19279, 19283, 19285, 19316, 19327, 19335, 19352, 19358, 19362, 19366, 19377, 19379, 19405, 19423, 19428, 19431, 19437, 19444, 19456, 19469, 19475, 19476, 19481, 19486, 19492, 19506, 19507, 19536, 19537, 19562, 19586, 19589, 19622, 19634, 19651, 19663, 19679, 19685, 19690, 19697, 19707, 19712, 19714, 19715, 19717, 19724, 19734, 19744, 19754, 19769, 19779, 19792, 19807, 19830, 19850, 19863, 19875, 19903, 19904, 19908, 19963, 19985, 19988, 20003, 20012, 20014, 20018, 20040, 20042, 20054, 20078, 20079, 20083, 20087, 20089, 20114, 20121, 20122, 20138, 20155, 20162, 20186, 20199, 20216, 20222, 20234, 20238, 20254, 20285, 20299, 20302, 20305, 20337, 20345, 20364, 20365, 20374, 20382, 20394, 20403, 20440, 20450, 20451, 20476, 20506, 20508, 20516, 20527, 20536, 20540, 20541, 20542, 20557, 20566, 20584, 20608, 20611, 20613, 20618, 20627, 20644, 20656, 20663, 20674, 20682, 20690, 20699, 20707, 20711, 20737, 20751, 20765, 20771, 20774, 20778, 20779, 20789, 20795, 20798, 20834, 20851, 20865, 20867, 20869, 20872, 20883, 20892, 20894, 20895, 20896, 20901, 20919, 20924, 20933, 20937, 20938, 20946, 20947, 20955, 20957, 20958, 20965, 20966, 20970, 20974, 20991, 21001, 21013, 21018, 21020, 21025, 21114, 21142, 21205, 21210, 21227, 21230, 21236, 21253, 21262, 21294, 21300, 21316, 21331, 21339, 21349, 21356, 21365, 21369, 21376, 21399, 21410, 21413, 21418, 21420, 21435, 21447, 21452, 21455, 21468, 21470, 21474, 21486, 21495, 21501, 21517, 21530, 21539, 21546, 21547, 21553, 21556, 21564, 21569, 21579, 21593, 21595, 21599, 21603, 21632, 21643, 21690, 21713, 21726, 21732, 21737, 21738, 21744, 21755, 21773, 21782, 21800, 21807, 21829, 21839, 21848, 21876, 21893, 21923, 21926, 21931, 21958, 21967, 21970, 21973, 21982, 22002, 22029, 22031, 22035, 22044, 22051, 22068, 22073, 22099, 22117, 22121, 22126, 22145, 22146, 22149, 22161, 22166, 22170, 22177, 22205, 22210, 22231, 22233, 22234, 22244, 22246, 22251, 22272, 22280, 22299, 22317, 22321, 22327, 22350, 22355, 22363, 22365, 22378, 22383, 22387, 22391, 22393, 22395, 22403, 22406, 22418, 22423, 22428, 22437, 22444, 22451, 22453, 22460, 22480, 22491, 22510, 22530, 22536, 22554, 22557, 22561, 22562, 22564, 22571, 22588, 22593, 22606, 22607, 22610, 22611, 22642, 22666, 22717, 22729, 22738, 22744, 22745, 22763, 22768, 22769, 22774, 22782, 22789, 22796, 22800, 22836, 22841, 22843, 22858, 22864, 22886, 22889, 22897, 22925, 22928, 22938, 22944, 22945, 22948, 22950, 22956, 22983, 22986, 22988, 22993, 22994, 22997, 23028, 23030, 23039, 23047, 23081, 23091, 23097, 23105, 23108, 23111, 23113, 23133, 23145, 23150, 23161, 23172, 23176, 23199, 23200, 23202, 23204, 23205, 23211, 23212, 23226, 23227, 23231, 23236, 23239, 23263, 23265, 23266, 23269, 23290, 23311, 23315, 23320, 23323, 23340, 23352, 23361, 23372, 23375, 23376, 23381, 23386, 23390, 23401, 23415, 23418, 23428, 23429, 23453, 23467, 23479, 23505, 23508, 23527, 23528, 23535, 23537, 23541, 23553, 23555, 23557, 23559, 23563, 23566, 23581, 23585, 23596, 23598, 23604, 23605, 23617, 23630, 23643, 23658, 23669, 23673, 23680, 23683, 23690, 23691, 23701, 23707, 23714, 23720, 23725, 23733, 23746, 23750, 23751, 23769, 23774, 23784, 23809, 23818, 23831, 23839, 23842, 23857, 23924, 23930, 23944, 23945, 23947, 23959, 23971, 23975, 23987, 23989, 24002, 24003, 24009, 24019, 24026, 24039, 24041, 24046, 24062, 24071, 24077, 24079, 24087, 24096, 24119, 24121, 24137, 24150, 24169, 24181, 24194, 24195, 24199, 24200, 24203, 24210, 24223, 24258, 24272, 24274, 24291, 24293, 24300, 24305, 24318, 24339, 24378, 24380, 24383, 24392, 24402, 24442, 24468, 24476, 24480, 24488, 24490, 24495, 24502, 24504, 24505, 24516, 24562, 24572, 24594, 24595, 24597, 24600, 24606, 24609, 24622, 24623, 24629, 24631, 24632, 24633, 24657, 24662, 24664, 24666, 24682, 24697, 24709, 24753, 24758, 24766, 24833, 24840, 24851, 24858, 24869, 24886, 24909, 24915, 24916, 24929, 24945, 24951, 24966, 24972, 24980, 24996, 25012, 25017, 25043, 25059, 25067, 25104, 25121, 25125, 25127, 25133, 25150, 25191, 25210, 25224, 25228, 25240, 25243, 25255, 25264, 25267, 25272, 25282, 25284, 25290, 25292, 25293, 25334, 25338, 25350, 25365, 25368, 25380, 25389, 25391, 25400, 25413, 25422, 25440, 25444, 25446, 25456, 25460, 25472, 25486, 25495, 25499, 25509, 25512, 25528, 25530, 25532, 25547, 25559, 25591, 25593, 25597, 25601, 25614, 25616, 25638, 25672, 25691, 25698, 25712, 25713, 25721, 25722, 25725, 25732, 25753, 25760, 25775, 25794, 25813, 25819, 25828, 25833, 25901, 25908, 25947, 25983, 25987, 25989, 25996, 26008, 26017, 26025, 26030, 26039, 26047, 26054, 26058, 26077, 26088, 26092, 26112, 26183, 26185, 26190, 26201, 26204, 26237, 26239, 26241, 26248, 26251, 26252, 26256, 26263, 26268, 26282, 26314, 26331, 26346, 26358, 26360, 26364, 26377, 26428, 26434, 26460, 26481, 26485, 26497, 26532, 26533, 26538, 26561, 26567, 26585, 26588, 26595, 26596, 26618, 26625, 26633, 26643, 26651, 26677, 26687, 26715, 26721, 26732, 26733, 26746, 26760, 26762, 26769, 26779, 26781, 26822, 26828, 26830, 26870, 26882, 26883, 26889, 26892, 26894, 26914, 26927, 26932, 26944, 26953, 26957, 26963, 26967, 26973, 26976, 26978, 26985, 27030, 27038, 27044, 27048, 27052, 27067, 27111, 27115, 27121, 27142, 27144, 27153, 27167, 27205, 27209, 27217, 27218, 27229, 27233, 27240, 27250, 27280, 27302, 27332, 27339, 27349, 27362, 27376, 27386, 27396, 27401, 27413, 27426, 27429, 27456, 27470, 27474, 27476, 27477, 27480]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27481, 5)\n",
      "(24857, 5)\n",
      "(24856, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df = df[(~df.index.isin(anomalous_idxs))]\n",
    "print(df.shape)\n",
    "df = df[(~df.text.isna())]\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.copy()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment_code\"] = df[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments = df[\"sentiment_code\"].cat.codes.values\n",
    "\n",
    "test_df[\"sentiment_code\"] = test_df[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments_test = test_df[\"sentiment_code\"].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"selected_text\"] = df[\"selected_text\"].astype(str)\n",
    "test_df[\"text\"] = test_df[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_mod\"] = \"<s> \" + df.text.str.strip() + \" </s> \" + df.sentiment + \" </s>\"\n",
    "test_df[\"text_mod\"] = \"<s> \" + test_df.text.str.strip() + \" </s> \" + test_df.sentiment + \" </s>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCLUDE_NEUTRAL_CLASS: (14017, 7)\n"
     ]
    }
   ],
   "source": [
    "if EXCLUDE_NEUTRAL_CLASS:\n",
    "    df = df.loc[df.sentiment!=\"neutral\"].copy()\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(\"EXCLUDE_NEUTRAL_CLASS:\", df.shape)\n",
    "\n",
    "if RUN_ON_SAMPLE:\n",
    "    df = df.sample(3000).copy()\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(\"RUN_ON_SAMPLE\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../results/tokenizers/roberta_tokenizer/vocab.json',\n",
       " '../results/tokenizers/roberta_tokenizer/merges.txt',\n",
       " '../results/tokenizers/roberta_tokenizer/special_tokens_map.json',\n",
       " '../results/tokenizers/roberta_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"../results/tokenizers/roberta_tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=\"../results/tokenizers/roberta_tokenizer/vocab.json\",\n",
    "    merges_file=\"../results/tokenizers/roberta_tokenizer/merges.txt\",\n",
    "    add_prefix_space=True,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../results/tokenizers/roberta_tokenizer/special_tokens_map.json\") as f:\n",
    "    special_tokens = json.load(f)\n",
    "\n",
    "tokenizer.add_special_tokens([i for i in special_tokens.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens = tokenizer.encode_batch(df.text_mod.tolist())\n",
    "Y_tokens = tokenizer.encode_batch(df.selected_text.tolist())\n",
    "X_tokens_test = tokenizer.encode_batch(test_df.text_mod.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i.ids for i in X_tokens]\n",
    "Y = [i.ids for i in Y_tokens]\n",
    "X_test = [i.ids for i in X_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_att = [i.attention_mask for i in X_tokens]\n",
    "Y_att = [i.attention_mask for i in Y_tokens]\n",
    "X_att_test = [i.attention_mask for i in X_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265 76\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(i) for i in X])\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "print(VOCAB_SIZE, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremities(l_string, s_string, print_it=False):\n",
    "    len_l = len(l_string)\n",
    "    len_s = len(s_string)\n",
    "    \n",
    "    for i in range(len_l - len_s + 1):\n",
    "        if (i + len_s) <= len_l:\n",
    "            substring = l_string[i:i+len_s]\n",
    "            if substring == s_string:\n",
    "                if print_it:\n",
    "                    print(l_string)\n",
    "                    print(substring)\n",
    "                    print(i, i+len_s, substring)\n",
    "                \n",
    "                start_vector, end_vector = [0] * len_l, [0] * len_l\n",
    "                start_vector[i], end_vector[i+len_s-1] = 1, 1\n",
    "                \n",
    "                return (start_vector, end_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14017 \t: #Processed\n",
      "0 \t: # of Anomalies\n"
     ]
    }
   ],
   "source": [
    "Y_starts, Y_stops = [], []\n",
    "anomaly_idx, counter = [], 0\n",
    "for num, (i,j) in enumerate(zip(X_tokens, Y_tokens)):\n",
    "    x,y = i.ids, j.ids\n",
    "    #s,e = get_extremities(x, y)\n",
    "    try:\n",
    "        s,e = get_extremities(x, y)\n",
    "    except TypeError as t:\n",
    "        counter += 1\n",
    "        anomaly_idx.append(num)\n",
    "    Y_starts.append(s)\n",
    "    Y_stops.append(e)\n",
    "print(num + 1, \"\\t: #Processed\")\n",
    "\n",
    "print(counter,\"\\t: # of Anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if counter:\n",
    "    print(anomaly_idx, sep=\",\")\n",
    "    print(df[\"original_index\"].iloc[anomaly_idx].tolist())\n",
    "    df.iloc[anomaly_idx].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcfly gig last nightt omg it was amazin didnt sit down through the whole thing  mcfly did you see me and ma best mate we were in tutus\n",
      "it was amazin\n",
      "[['<s>', 0, 0, 0], ['Ġmc', 44355, 0, 0], ['fly', 19252, 0, 0], ['Ġgig', 10196, 0, 0], ['Ġlast', 94, 0, 0], ['Ġnight', 363, 0, 0], ['t', 90, 0, 0], ['Ġo', 1021, 0, 0], ['mg', 22984, 0, 0], ['Ġit', 24, 1, 0], ['Ġwas', 21, 0, 0], ['Ġamaz', 42402, 0, 0], ['in', 179, 0, 1], ['Ġdidnt', 46405, 0, 0], ['Ġsit', 2662, 0, 0], ['Ġdown', 159, 0, 0], ['Ġthrough', 149, 0, 0], ['Ġthe', 5, 0, 0], ['Ġwhole', 1086, 0, 0], ['Ġthing', 631, 0, 0], ['Ġ', 1437, 0, 0], ['Ġmc', 44355, 0, 0], ['fly', 19252, 0, 0], ['Ġdid', 222, 0, 0], ['Ġyou', 47, 0, 0], ['Ġsee', 192, 0, 0], ['Ġme', 162, 0, 0], ['Ġand', 8, 0, 0], ['Ġma', 9131, 0, 0], ['Ġbest', 275, 0, 0], ['Ġmate', 12563, 0, 0], ['Ġwe', 52, 0, 0], ['Ġwere', 58, 0, 0], ['Ġin', 11, 0, 0], ['Ġtut', 15511, 0, 0], ['us', 687, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0], ['Ġpositive', 1313, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0]]\n",
      "[[24, 'Ġit'], [21, 'Ġwas'], [42402, 'Ġamaz'], [179, 'in']]\n"
     ]
    }
   ],
   "source": [
    "check_idx = 156\n",
    "print(df.text[check_idx])\n",
    "print(df.selected_text[check_idx])\n",
    "print([[i,j,k,l] for i,j,k,l in zip(X_tokens[check_idx].tokens,\n",
    "                                    X_tokens[check_idx].ids,\n",
    "                                    Y_starts[check_idx],\n",
    "                                    Y_stops[check_idx])])\n",
    "print([[i,j] for i,j in zip(Y_tokens[check_idx].ids,\n",
    "                            Y_tokens[check_idx].tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X, maxlen=max_len, padding=\"post\")\n",
    "X_att = pad_sequences(X_att, maxlen=max_len, padding=\"post\")\n",
    "Y = pad_sequences(Y, maxlen=max_len, padding=\"post\")\n",
    "Y_starts = pad_sequences(Y_starts, maxlen=max_len, padding=\"post\").argmax(axis=1)\n",
    "Y_stops = pad_sequences(Y_stops, maxlen=max_len, padding=\"post\").argmax(axis=1)\n",
    "\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding=\"post\")\n",
    "X_att_test = pad_sequences(X_att_test, maxlen=max_len, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_flag = np.isin(Y_stops, np.unique(Y_stops, return_counts=True)[0][np.unique(Y_stops, return_counts=True)[1]>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (14017, 76) \t: X  \n",
      " (14017, 76) \t: X_att  \n",
      " (14017, 76) \t: Y  \n",
      " (14017,) \t: Y_starts  \n",
      " (14017,) \t: Y_stops  \n",
      " (3534, 76) \t: X_test  \n",
      " (3534, 76) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X.shape, \"\\t: X \", \"\\n\",\n",
    "     X_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[keep_flag]\n",
    "X_att = X_att[keep_flag]\n",
    "Y = Y[keep_flag]\n",
    "Y_starts = Y_starts[keep_flag]\n",
    "Y_stops = Y_stops[keep_flag]\n",
    "X_test = X_test\n",
    "X_att_test = X_att_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (14015, 76) \t: X  \n",
      " (14015, 76) \t: X_att  \n",
      " (14015, 76) \t: Y  \n",
      " (14015,) \t: Y_starts  \n",
      " (14015,) \t: Y_stops  \n",
      " (3534, 76) \t: X_test  \n",
      " (3534, 76) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X.shape, \"\\t: X \", \"\\n\",\n",
    "     X_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=NUM_FOLDS, train_size=TRAIN_SPLIT_RATIO, random_state=0)\n",
    "sss.get_n_splits(X, Y_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_words = [tokenizer.decode(i) for i in Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=6a6L_9USZxg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_FOLDS):\n",
    "    if os.path.exists(\"../results/\"+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\"):\n",
    "        os.remove(\"../results/\"+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(pred_dict):\n",
    "    print(\"[INFO] \",\"=\"*15,\"Validation for FOLD#\", num, \"=\"*15)\n",
    "    funcs = [accuracy_score, f1_score, precision_score, recall_score, confusion_matrix]\n",
    "    for f in funcs:\n",
    "        for data_set in [\"train\",\"valid\"]:\n",
    "            for var in [\"starts\", \"stops\"]:\n",
    "                if f in [accuracy_score]:\n",
    "                    res = f(**pred_dict[data_set][var])\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "                elif f in [confusion_matrix]:\n",
    "                    res = f(**pred_dict[data_set][var], labels=np.arange(max_len))\n",
    "                    np.savetxt(X=res, fmt='%i', delimiter=\",\",\n",
    "                               fname=\"../results/ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                    print(\"[INFO] \\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__, \"\\t||\", \n",
    "                          \"../results/ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                else:\n",
    "                    res = f(**pred_dict[data_set][var], average=\"macro\")\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "        print(\"=======================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(string):\n",
    "    string = re.sub(pattern=\" (negative|positive|neutral)[ ]+$\", repl=\"\", string=string)\n",
    "    string = re.sub(pattern=\" +\", repl=\" \", string=string.strip())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_sequences = Input((max_len), dtype=tf.int32, name=\"words\")\n",
    "    input_att_flags = Input((max_len), dtype=tf.int32, name=\"att_flags\")\n",
    "    input_token_ids = Input((max_len), dtype=tf.int32, name=\"token_ids\")\n",
    "\n",
    "    config = RobertaConfig.from_pretrained('roberta-base')\n",
    "    roberta_model = TFRobertaModel.from_pretrained('roberta-base', config=config)\n",
    "    x = roberta_model(inputs=input_sequences, attention_mask=input_att_flags, token_type_ids=input_token_ids)\n",
    "    \n",
    "    x_conv = Reshape((max_len, 768, 1))(x[0])\n",
    "    x_conv = Conv2D(filters=2, kernel_size=(1,768), data_format=\"channels_last\")(x_conv)\n",
    "    x_conv = BatchNormalization()(x_conv)\n",
    "    x_conv = Dropout(DROPOUT)(x_conv)\n",
    "    x_conv = Flatten()(x_conv)\n",
    "    \n",
    "    x_lstm = Bidirectional(LSTM(2, activation='relu', return_sequences=False))(x[0])\n",
    "    x_lstm = BatchNormalization()(x_lstm)\n",
    "    x_lstm = Dropout(DROPOUT)(x_lstm)\n",
    "    x_lstm = Flatten()(x_lstm)\n",
    "    \n",
    "    x_dense = TimeDistributed(Dense(2, activation='relu'))(x[0])\n",
    "    x_dense = BatchNormalization()(x_dense)\n",
    "    x_dense = Dropout(DROPOUT)(x_dense)\n",
    "    x_dense = Flatten()(x_dense)\n",
    "    \n",
    "    x_flat = concatenate([x_conv, x_lstm, x_dense])\n",
    "    \n",
    "    output_starts_0 = Dense(max_len, activation='softmax', name=\"starts_0\")(x_flat)\n",
    "    output_stops_0 = Dense(max_len, activation='softmax', name=\"stops_0\")(x_flat)\n",
    "    \n",
    "    # How do we enforce start index is less than stop index?\n",
    "    output_subtract = tf.keras.layers.Subtract()([output_starts_0, output_stops_0])\n",
    "    output_flat = concatenate([output_starts_0, output_stops_0, output_subtract])\n",
    "    output_starts_1 = Dense(max_len, activation='softmax', name=\"starts_1\")(output_flat)\n",
    "    output_stops_1 = Dense(max_len, activation='softmax', name=\"stops_1\")(output_flat)\n",
    "    \n",
    "    model = Model([input_att_flags, input_sequences, input_token_ids],\n",
    "                  [output_starts_0, output_stops_0, output_starts_1, output_stops_1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 76)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 76)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 76)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 76, 768), (N 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 76, 768, 1)   0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 76, 1, 2)     1538        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 4)            12336       tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 76, 2)        1538        tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 76, 1, 2)     8           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4)            16          bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 76, 2)        8           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 76, 1, 2)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 4)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 76, 2)        0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 152)          0           dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4)            0           dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 152)          0           dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 308)          0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "starts_0 (Dense)                (None, 76)           23484       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "stops_0 (Dense)                 (None, 76)           23484       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 76)           0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 228)          0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "                                                                 subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "starts_1 (Dense)                (None, 76)           17404       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "stops_1 (Dense)                 (None, 76)           17404       concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 124,742,852\n",
      "Trainable params: 124,742,836\n",
      "Non-trainable params: 16\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><h2 id=\"finetuning\">Fine-tuning</h2></a>\n",
    "<p>Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.</p>\n",
    " <p>This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.</p>\n",
    " <p>It is critical to only do this step <em>after</em> the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.</p>\n",
    " <p>It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>\n",
    "\n",
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><p><strong>Important note about <code>compile()</code> and <code>trainable</code></strong></p></a>\n",
    "<p>Calling <code>compile()</code> on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the <code>trainable</code>\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until <code>compile</code> is called again. Hence, if you change any <code>trainable</code> value, make sure\n",
    " to call <code>compile()</code> again on your\n",
    "model for your changes to be taken into account.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ==================== FOLD# 0 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/10\n",
      "11212/11212 [==============================] - 209s 19ms/sample - loss: 10.5630 - starts_0_loss: 2.3214 - stops_0_loss: 2.5630 - starts_1_loss: 2.6387 - stops_1_loss: 3.0394 - starts_0_accuracy: 0.3662 - stops_0_accuracy: 0.3028 - starts_1_accuracy: 0.3517 - stops_1_accuracy: 0.2158 - val_loss: 8.3362 - val_starts_0_loss: 1.8417 - val_stops_0_loss: 1.8830 - val_starts_1_loss: 2.1926 - val_stops_1_loss: 2.4403 - val_starts_0_accuracy: 0.4242 - val_stops_0_accuracy: 0.4449 - val_starts_1_accuracy: 0.3660 - val_stops_1_accuracy: 0.4081\n",
      "Epoch 2/10\n",
      "11212/11212 [==============================] - 190s 17ms/sample - loss: 8.5118 - starts_0_loss: 1.9367 - stops_0_loss: 2.0967 - starts_1_loss: 2.0912 - stops_1_loss: 2.3868 - starts_0_accuracy: 0.4192 - stops_0_accuracy: 0.4063 - starts_1_accuracy: 0.3928 - stops_1_accuracy: 0.3856 - val_loss: 7.4411 - val_starts_0_loss: 1.6812 - val_stops_0_loss: 1.7219 - val_starts_1_loss: 1.9482 - val_stops_1_loss: 2.1096 - val_starts_0_accuracy: 0.4513 - val_stops_0_accuracy: 0.4763 - val_starts_1_accuracy: 0.4078 - val_stops_1_accuracy: 0.4706\n",
      "Epoch 3/10\n",
      "11212/11212 [==============================] - 191s 17ms/sample - loss: 8.1096 - starts_0_loss: 1.8753 - stops_0_loss: 2.0005 - starts_1_loss: 1.9901 - stops_1_loss: 2.2441 - starts_0_accuracy: 0.4233 - stops_0_accuracy: 0.4301 - starts_1_accuracy: 0.4135 - stops_1_accuracy: 0.4159 - val_loss: 7.2390 - val_starts_0_loss: 1.6356 - val_stops_0_loss: 1.7091 - val_starts_1_loss: 1.8571 - val_stops_1_loss: 2.0482 - val_starts_0_accuracy: 0.4724 - val_stops_0_accuracy: 0.4773 - val_starts_1_accuracy: 0.4253 - val_stops_1_accuracy: 0.4773\n",
      "Epoch 4/10\n",
      "11212/11212 [==============================] - 195s 17ms/sample - loss: 7.9088 - starts_0_loss: 1.8311 - stops_0_loss: 1.9607 - starts_1_loss: 1.9262 - stops_1_loss: 2.1910 - starts_0_accuracy: 0.4334 - stops_0_accuracy: 0.4399 - starts_1_accuracy: 0.4252 - stops_1_accuracy: 0.4205 - val_loss: 7.1745 - val_starts_0_loss: 1.6041 - val_stops_0_loss: 1.7295 - val_starts_1_loss: 1.8273 - val_stops_1_loss: 2.0196 - val_starts_0_accuracy: 0.4748 - val_stops_0_accuracy: 0.4702 - val_starts_1_accuracy: 0.4456 - val_stops_1_accuracy: 0.4759\n",
      "Epoch 5/10\n",
      "11212/11212 [==============================] - 196s 18ms/sample - loss: 7.7701 - starts_0_loss: 1.8238 - stops_0_loss: 1.9107 - starts_1_loss: 1.9107 - stops_1_loss: 2.1255 - starts_0_accuracy: 0.4328 - stops_0_accuracy: 0.4478 - starts_1_accuracy: 0.4278 - stops_1_accuracy: 0.4372 - val_loss: 7.0613 - val_starts_0_loss: 1.6096 - val_stops_0_loss: 1.6708 - val_starts_1_loss: 1.7926 - val_stops_1_loss: 1.9978 - val_starts_0_accuracy: 0.4823 - val_stops_0_accuracy: 0.5041 - val_starts_1_accuracy: 0.4467 - val_stops_1_accuracy: 0.4902\n",
      "Epoch 6/10\n",
      "11212/11212 [==============================] - 195s 17ms/sample - loss: 7.6794 - starts_0_loss: 1.7930 - stops_0_loss: 1.8987 - starts_1_loss: 1.8782 - stops_1_loss: 2.1087 - starts_0_accuracy: 0.4443 - stops_0_accuracy: 0.4492 - starts_1_accuracy: 0.4394 - stops_1_accuracy: 0.4399 - val_loss: 6.9385 - val_starts_0_loss: 1.5789 - val_stops_0_loss: 1.6627 - val_starts_1_loss: 1.7421 - val_stops_1_loss: 1.9665 - val_starts_0_accuracy: 0.4916 - val_stops_0_accuracy: 0.4930 - val_starts_1_accuracy: 0.4713 - val_stops_1_accuracy: 0.4902\n",
      "Epoch 7/10\n",
      "11212/11212 [==============================] - 195s 17ms/sample - loss: 7.5956 - starts_0_loss: 1.7842 - stops_0_loss: 1.8735 - starts_1_loss: 1.8537 - stops_1_loss: 2.0840 - starts_0_accuracy: 0.4444 - stops_0_accuracy: 0.4519 - starts_1_accuracy: 0.4398 - stops_1_accuracy: 0.4443 - val_loss: 7.0057 - val_starts_0_loss: 1.6040 - val_stops_0_loss: 1.6759 - val_starts_1_loss: 1.7871 - val_stops_1_loss: 1.9465 - val_starts_0_accuracy: 0.4716 - val_stops_0_accuracy: 0.4870 - val_starts_1_accuracy: 0.4377 - val_stops_1_accuracy: 0.4966\n",
      "Epoch 8/10\n",
      "11212/11212 [==============================] - 192s 17ms/sample - loss: 7.5437 - starts_0_loss: 1.7814 - stops_0_loss: 1.8574 - starts_1_loss: 1.8479 - stops_1_loss: 2.0568 - starts_0_accuracy: 0.4484 - stops_0_accuracy: 0.4542 - starts_1_accuracy: 0.4427 - stops_1_accuracy: 0.4521 - val_loss: 6.9532 - val_starts_0_loss: 1.5929 - val_stops_0_loss: 1.6441 - val_starts_1_loss: 1.7829 - val_stops_1_loss: 1.9476 - val_starts_0_accuracy: 0.4763 - val_stops_0_accuracy: 0.4977 - val_starts_1_accuracy: 0.4506 - val_stops_1_accuracy: 0.4905\n",
      "Epoch 9/10\n",
      "11212/11212 [==============================] - 191s 17ms/sample - loss: 7.5194 - starts_0_loss: 1.7616 - stops_0_loss: 1.8621 - starts_1_loss: 1.8349 - stops_1_loss: 2.0606 - starts_0_accuracy: 0.4468 - stops_0_accuracy: 0.4520 - starts_1_accuracy: 0.4433 - stops_1_accuracy: 0.4495 - val_loss: 6.8685 - val_starts_0_loss: 1.5719 - val_stops_0_loss: 1.6339 - val_starts_1_loss: 1.7425 - val_stops_1_loss: 1.9264 - val_starts_0_accuracy: 0.4848 - val_stops_0_accuracy: 0.4952 - val_starts_1_accuracy: 0.4699 - val_stops_1_accuracy: 0.4938\n",
      "Epoch 10/10\n",
      "11212/11212 [==============================] - 193s 17ms/sample - loss: 7.4115 - starts_0_loss: 1.7431 - stops_0_loss: 1.8313 - starts_1_loss: 1.8146 - stops_1_loss: 2.0231 - starts_0_accuracy: 0.4596 - stops_0_accuracy: 0.4612 - starts_1_accuracy: 0.4559 - stops_1_accuracy: 0.4538 - val_loss: 6.9049 - val_starts_0_loss: 1.5774 - val_stops_0_loss: 1.6463 - val_starts_1_loss: 1.7517 - val_stops_1_loss: 1.9389 - val_starts_0_accuracy: 0.4870 - val_stops_0_accuracy: 0.4909 - val_starts_1_accuracy: 0.4641 - val_stops_1_accuracy: 0.4895\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "11212/11212 [==============================] - 216s 19ms/sample - loss: 7.1793 - starts_0_loss: 1.6838 - stops_0_loss: 1.7742 - starts_1_loss: 1.7559 - stops_1_loss: 1.9657 - starts_0_accuracy: 0.4733 - stops_0_accuracy: 0.4720 - starts_1_accuracy: 0.4692 - stops_1_accuracy: 0.4703 - val_loss: 6.8517 - val_starts_0_loss: 1.5628 - val_stops_0_loss: 1.6311 - val_starts_1_loss: 1.7444 - val_stops_1_loss: 1.9164 - val_starts_0_accuracy: 0.4848 - val_stops_0_accuracy: 0.4930 - val_starts_1_accuracy: 0.4645 - val_stops_1_accuracy: 0.4988\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 195s 17ms/sample - loss: 7.1703 - starts_0_loss: 1.6843 - stops_0_loss: 1.7685 - starts_1_loss: 1.7575 - stops_1_loss: 1.9595 - starts_0_accuracy: 0.4730 - stops_0_accuracy: 0.4732 - starts_1_accuracy: 0.4681 - stops_1_accuracy: 0.4740 - val_loss: 6.8432 - val_starts_0_loss: 1.5617 - val_stops_0_loss: 1.6252 - val_starts_1_loss: 1.7467 - val_stops_1_loss: 1.9128 - val_starts_0_accuracy: 0.4859 - val_stops_0_accuracy: 0.4955 - val_starts_1_accuracy: 0.4652 - val_stops_1_accuracy: 0.4991\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 194s 17ms/sample - loss: 7.1205 - starts_0_loss: 1.6658 - stops_0_loss: 1.7605 - starts_1_loss: 1.7540 - stops_1_loss: 1.9401 - starts_0_accuracy: 0.4689 - stops_0_accuracy: 0.4770 - starts_1_accuracy: 0.4673 - stops_1_accuracy: 0.4781 - val_loss: 6.8380 - val_starts_0_loss: 1.5612 - val_stops_0_loss: 1.6208 - val_starts_1_loss: 1.7483 - val_stops_1_loss: 1.9111 - val_starts_0_accuracy: 0.4845 - val_stops_0_accuracy: 0.4945 - val_starts_1_accuracy: 0.4631 - val_stops_1_accuracy: 0.5020\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 195s 17ms/sample - loss: 7.0829 - starts_0_loss: 1.6708 - stops_0_loss: 1.7397 - starts_1_loss: 1.7488 - stops_1_loss: 1.9233 - starts_0_accuracy: 0.4747 - stops_0_accuracy: 0.4839 - starts_1_accuracy: 0.4693 - stops_1_accuracy: 0.4806 - val_loss: 6.8288 - val_starts_0_loss: 1.5590 - val_stops_0_loss: 1.6158 - val_starts_1_loss: 1.7474 - val_stops_1_loss: 1.9099 - val_starts_0_accuracy: 0.4870 - val_stops_0_accuracy: 0.4927 - val_starts_1_accuracy: 0.4649 - val_stops_1_accuracy: 0.4980\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 193s 17ms/sample - loss: 7.0906 - starts_0_loss: 1.6697 - stops_0_loss: 1.7441 - starts_1_loss: 1.7429 - stops_1_loss: 1.9344 - starts_0_accuracy: 0.4773 - stops_0_accuracy: 0.4821 - starts_1_accuracy: 0.4728 - stops_1_accuracy: 0.4813 - val_loss: 6.8078 - val_starts_0_loss: 1.5529 - val_stops_0_loss: 1.6129 - val_starts_1_loss: 1.7394 - val_stops_1_loss: 1.9052 - val_starts_0_accuracy: 0.4888 - val_stops_0_accuracy: 0.4938 - val_starts_1_accuracy: 0.4699 - val_stops_1_accuracy: 0.4995\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "11212/11212 [==============================] - 350s 31ms/sample - loss: 6.9851 - starts_0_loss: 1.6323 - stops_0_loss: 1.7325 - starts_1_loss: 1.7103 - stops_1_loss: 1.9108 - starts_0_accuracy: 0.4881 - stops_0_accuracy: 0.4842 - starts_1_accuracy: 0.4836 - stops_1_accuracy: 0.4810 - val_loss: 6.6250 - val_starts_0_loss: 1.4928 - val_stops_0_loss: 1.5831 - val_starts_1_loss: 1.6845 - val_stops_1_loss: 1.8612 - val_starts_0_accuracy: 0.4991 - val_stops_0_accuracy: 0.4988 - val_starts_1_accuracy: 0.4784 - val_stops_1_accuracy: 0.5073\n",
      "Epoch 2/2\n",
      "11212/11212 [==============================] - 325s 29ms/sample - loss: 6.7518 - starts_0_loss: 1.5820 - stops_0_loss: 1.6588 - starts_1_loss: 1.6650 - stops_1_loss: 1.8463 - starts_0_accuracy: 0.4932 - stops_0_accuracy: 0.4994 - starts_1_accuracy: 0.4898 - stops_1_accuracy: 0.5021 - val_loss: 6.5811 - val_starts_0_loss: 1.4849 - val_stops_0_loss: 1.5733 - val_starts_1_loss: 1.6637 - val_stops_1_loss: 1.8543 - val_starts_0_accuracy: 0.5045 - val_stops_0_accuracy: 0.4980 - val_starts_1_accuracy: 0.4841 - val_stops_1_accuracy: 0.5134\n",
      "[INFO]  =============== Validation for FOLD# 0 ===============\n",
      "[INFO] 57.89 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 57.84 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 50.45 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 49.80 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 64.21 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 61.20 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 37.75 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 43.92 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 62.78 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 56.83 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 40.82 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 41.49 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 67.57 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 69.06 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 37.71 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 48.90 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t||"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (11212, 76) (11212, 76)\n",
      "[INFO] Prediction shape for validation data:  (2803, 76) (2803, 76)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  9230 out of 11212\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  2342 out of 2803\n",
      "[INFO] Training Jaccard Score:  0.2881444026211017\n",
      "[INFO] Validation Jaccard Score:  0.2931577877274381\n",
      "[INFO] Training for fold: 0 finished at Sun May 17 04:51:47 2020\n",
      "[INFO] ==================== FOLD# 1 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/10\n",
      "11212/11212 [==============================] - 219s 20ms/sample - loss: 10.4183 - starts_0_loss: 2.3008 - stops_0_loss: 2.4951 - starts_1_loss: 2.6098 - stops_1_loss: 3.0120 - starts_0_accuracy: 0.3695 - stops_0_accuracy: 0.3214 - starts_1_accuracy: 0.3531 - stops_1_accuracy: 0.2371 - val_loss: 7.9420 - val_starts_0_loss: 1.7472 - val_stops_0_loss: 1.7946 - val_starts_1_loss: 2.0790 - val_stops_1_loss: 2.3097 - val_starts_0_accuracy: 0.4392 - val_stops_0_accuracy: 0.4527 - val_starts_1_accuracy: 0.3907 - val_stops_1_accuracy: 0.4338\n",
      "Epoch 2/10\n",
      "11212/11212 [==============================] - 200s 18ms/sample - loss: 8.1803 - starts_0_loss: 1.8798 - stops_0_loss: 1.9674 - starts_1_loss: 2.0414 - stops_1_loss: 2.2911 - starts_0_accuracy: 0.4226 - stops_0_accuracy: 0.4393 - starts_1_accuracy: 0.4003 - stops_1_accuracy: 0.4180 - val_loss: 7.4774 - val_starts_0_loss: 1.6794 - val_stops_0_loss: 1.7259 - val_starts_1_loss: 1.9269 - val_stops_1_loss: 2.1309 - val_starts_0_accuracy: 0.4549 - val_stops_0_accuracy: 0.4731 - val_starts_1_accuracy: 0.4121 - val_stops_1_accuracy: 0.4588\n",
      "Epoch 3/10\n",
      "11212/11212 [==============================] - 200s 18ms/sample - loss: 7.6769 - starts_0_loss: 1.7727 - stops_0_loss: 1.8521 - starts_1_loss: 1.9169 - stops_1_loss: 2.1353 - starts_0_accuracy: 0.4470 - stops_0_accuracy: 0.4577 - starts_1_accuracy: 0.4309 - stops_1_accuracy: 0.4443 - val_loss: 7.2171 - val_starts_0_loss: 1.6298 - val_stops_0_loss: 1.7426 - val_starts_1_loss: 1.8262 - val_stops_1_loss: 2.0074 - val_starts_0_accuracy: 0.4549 - val_stops_0_accuracy: 0.4613 - val_starts_1_accuracy: 0.4420 - val_stops_1_accuracy: 0.4763\n",
      "Epoch 4/10\n",
      "11212/11212 [==============================] - 200s 18ms/sample - loss: 7.4551 - starts_0_loss: 1.7337 - stops_0_loss: 1.7996 - starts_1_loss: 1.8591 - stops_1_loss: 2.0629 - starts_0_accuracy: 0.4544 - stops_0_accuracy: 0.4688 - starts_1_accuracy: 0.4387 - stops_1_accuracy: 0.4613 - val_loss: 6.9768 - val_starts_0_loss: 1.6041 - val_stops_0_loss: 1.6395 - val_starts_1_loss: 1.7640 - val_stops_1_loss: 1.9569 - val_starts_0_accuracy: 0.4584 - val_stops_0_accuracy: 0.4781 - val_starts_1_accuracy: 0.4538 - val_stops_1_accuracy: 0.4806\n",
      "Epoch 5/10\n",
      "11212/11212 [==============================] - 197s 18ms/sample - loss: 7.2711 - starts_0_loss: 1.6880 - stops_0_loss: 1.7661 - starts_1_loss: 1.8055 - stops_1_loss: 2.0119 - starts_0_accuracy: 0.4674 - stops_0_accuracy: 0.4715 - starts_1_accuracy: 0.4556 - stops_1_accuracy: 0.4642 - val_loss: 6.9933 - val_starts_0_loss: 1.6057 - val_stops_0_loss: 1.6587 - val_starts_1_loss: 1.7819 - val_stops_1_loss: 1.9371 - val_starts_0_accuracy: 0.4570 - val_stops_0_accuracy: 0.4752 - val_starts_1_accuracy: 0.4374 - val_stops_1_accuracy: 0.4827\n",
      "Epoch 6/10\n",
      "11212/11212 [==============================] - 199s 18ms/sample - loss: 7.1965 - starts_0_loss: 1.6796 - stops_0_loss: 1.7423 - starts_1_loss: 1.7921 - stops_1_loss: 1.9830 - starts_0_accuracy: 0.4639 - stops_0_accuracy: 0.4831 - starts_1_accuracy: 0.4543 - stops_1_accuracy: 0.4749 - val_loss: 6.9353 - val_starts_0_loss: 1.5684 - val_stops_0_loss: 1.6474 - val_starts_1_loss: 1.7542 - val_stops_1_loss: 1.9549 - val_starts_0_accuracy: 0.4613 - val_stops_0_accuracy: 0.4791 - val_starts_1_accuracy: 0.4481 - val_stops_1_accuracy: 0.4802\n",
      "Epoch 7/10\n",
      "11212/11212 [==============================] - 199s 18ms/sample - loss: 7.1342 - starts_0_loss: 1.6553 - stops_0_loss: 1.7338 - starts_1_loss: 1.7687 - stops_1_loss: 1.9760 - starts_0_accuracy: 0.4676 - stops_0_accuracy: 0.4764 - starts_1_accuracy: 0.4634 - stops_1_accuracy: 0.4745 - val_loss: 6.9193 - val_starts_0_loss: 1.5794 - val_stops_0_loss: 1.6469 - val_starts_1_loss: 1.7489 - val_stops_1_loss: 1.9352 - val_starts_0_accuracy: 0.4609 - val_stops_0_accuracy: 0.4816 - val_starts_1_accuracy: 0.4538 - val_stops_1_accuracy: 0.4784\n",
      "Epoch 8/10\n",
      "11212/11212 [==============================] - 196s 18ms/sample - loss: 7.0537 - starts_0_loss: 1.6570 - stops_0_loss: 1.7016 - starts_1_loss: 1.7644 - stops_1_loss: 1.9302 - starts_0_accuracy: 0.4676 - stops_0_accuracy: 0.4841 - starts_1_accuracy: 0.4613 - stops_1_accuracy: 0.4813 - val_loss: 6.8245 - val_starts_0_loss: 1.5605 - val_stops_0_loss: 1.6403 - val_starts_1_loss: 1.6987 - val_stops_1_loss: 1.9216 - val_starts_0_accuracy: 0.4784 - val_stops_0_accuracy: 0.4834 - val_starts_1_accuracy: 0.4752 - val_stops_1_accuracy: 0.4916\n",
      "Epoch 9/10\n",
      "11212/11212 [==============================] - 196s 18ms/sample - loss: 6.9657 - starts_0_loss: 1.6236 - stops_0_loss: 1.6979 - starts_1_loss: 1.7322 - stops_1_loss: 1.9121 - starts_0_accuracy: 0.4821 - stops_0_accuracy: 0.4908 - starts_1_accuracy: 0.4736 - stops_1_accuracy: 0.4906 - val_loss: 6.7519 - val_starts_0_loss: 1.5180 - val_stops_0_loss: 1.6125 - val_starts_1_loss: 1.6991 - val_stops_1_loss: 1.9144 - val_starts_0_accuracy: 0.4691 - val_stops_0_accuracy: 0.4820 - val_starts_1_accuracy: 0.4659 - val_stops_1_accuracy: 0.4909\n",
      "Epoch 10/10\n",
      "11212/11212 [==============================] - 196s 18ms/sample - loss: 6.9881 - starts_0_loss: 1.6290 - stops_0_loss: 1.7007 - starts_1_loss: 1.7396 - stops_1_loss: 1.9188 - starts_0_accuracy: 0.4782 - stops_0_accuracy: 0.4856 - starts_1_accuracy: 0.4697 - stops_1_accuracy: 0.4868 - val_loss: 6.7689 - val_starts_0_loss: 1.5354 - val_stops_0_loss: 1.6207 - val_starts_1_loss: 1.7040 - val_stops_1_loss: 1.9031 - val_starts_0_accuracy: 0.4688 - val_stops_0_accuracy: 0.4827 - val_starts_1_accuracy: 0.4581 - val_stops_1_accuracy: 0.4845\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "11212/11212 [==============================] - 215s 19ms/sample - loss: 6.6607 - starts_0_loss: 1.5523 - stops_0_loss: 1.6160 - starts_1_loss: 1.6716 - stops_1_loss: 1.8207 - starts_0_accuracy: 0.4905 - stops_0_accuracy: 0.5054 - starts_1_accuracy: 0.4765 - stops_1_accuracy: 0.5105 - val_loss: 6.7049 - val_starts_0_loss: 1.5167 - val_stops_0_loss: 1.6077 - val_starts_1_loss: 1.6841 - val_stops_1_loss: 1.8912 - val_starts_0_accuracy: 0.4841 - val_stops_0_accuracy: 0.4820 - val_starts_1_accuracy: 0.4677 - val_stops_1_accuracy: 0.4920\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 196s 18ms/sample - loss: 6.6765 - starts_0_loss: 1.5660 - stops_0_loss: 1.6140 - starts_1_loss: 1.6767 - stops_1_loss: 1.8194 - starts_0_accuracy: 0.4889 - stops_0_accuracy: 0.5091 - starts_1_accuracy: 0.4786 - stops_1_accuracy: 0.5122 - val_loss: 6.6849 - val_starts_0_loss: 1.5101 - val_stops_0_loss: 1.6020 - val_starts_1_loss: 1.6790 - val_stops_1_loss: 1.8881 - val_starts_0_accuracy: 0.4856 - val_stops_0_accuracy: 0.4798 - val_starts_1_accuracy: 0.4709 - val_stops_1_accuracy: 0.4941\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 196s 18ms/sample - loss: 6.5888 - starts_0_loss: 1.5296 - stops_0_loss: 1.6047 - starts_1_loss: 1.6478 - stops_1_loss: 1.8069 - starts_0_accuracy: 0.5004 - stops_0_accuracy: 0.5103 - starts_1_accuracy: 0.4890 - stops_1_accuracy: 0.5143 - val_loss: 6.6711 - val_starts_0_loss: 1.5064 - val_stops_0_loss: 1.5973 - val_starts_1_loss: 1.6752 - val_stops_1_loss: 1.8867 - val_starts_0_accuracy: 0.4852 - val_stops_0_accuracy: 0.4802 - val_starts_1_accuracy: 0.4738 - val_stops_1_accuracy: 0.4948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 197s 18ms/sample - loss: 6.5911 - starts_0_loss: 1.5324 - stops_0_loss: 1.6014 - starts_1_loss: 1.6514 - stops_1_loss: 1.8054 - starts_0_accuracy: 0.4982 - stops_0_accuracy: 0.5084 - starts_1_accuracy: 0.4877 - stops_1_accuracy: 0.5161 - val_loss: 6.6672 - val_starts_0_loss: 1.5049 - val_stops_0_loss: 1.5970 - val_starts_1_loss: 1.6732 - val_stops_1_loss: 1.8866 - val_starts_0_accuracy: 0.4866 - val_stops_0_accuracy: 0.4798 - val_starts_1_accuracy: 0.4766 - val_stops_1_accuracy: 0.4973\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 197s 18ms/sample - loss: 6.5697 - starts_0_loss: 1.5341 - stops_0_loss: 1.5895 - starts_1_loss: 1.6475 - stops_1_loss: 1.7979 - starts_0_accuracy: 0.4945 - stops_0_accuracy: 0.5062 - starts_1_accuracy: 0.4895 - stops_1_accuracy: 0.5142 - val_loss: 6.6689 - val_starts_0_loss: 1.5047 - val_stops_0_loss: 1.5982 - val_starts_1_loss: 1.6736 - val_stops_1_loss: 1.8865 - val_starts_0_accuracy: 0.4863 - val_stops_0_accuracy: 0.4813 - val_starts_1_accuracy: 0.4777 - val_stops_1_accuracy: 0.4959\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "11212/11212 [==============================] - 353s 31ms/sample - loss: 6.4875 - starts_0_loss: 1.5065 - stops_0_loss: 1.5731 - starts_1_loss: 1.6235 - stops_1_loss: 1.7845 - starts_0_accuracy: 0.5066 - stops_0_accuracy: 0.5153 - starts_1_accuracy: 0.4960 - stops_1_accuracy: 0.5162 - val_loss: 6.5320 - val_starts_0_loss: 1.4670 - val_stops_0_loss: 1.5689 - val_starts_1_loss: 1.6328 - val_stops_1_loss: 1.8567 - val_starts_0_accuracy: 0.4966 - val_stops_0_accuracy: 0.4898 - val_starts_1_accuracy: 0.4913 - val_stops_1_accuracy: 0.5030\n",
      "Epoch 2/2\n",
      "11212/11212 [==============================] - 329s 29ms/sample - loss: 6.2836 - starts_0_loss: 1.4604 - stops_0_loss: 1.5227 - starts_1_loss: 1.5752 - stops_1_loss: 1.7251 - starts_0_accuracy: 0.5204 - stops_0_accuracy: 0.5355 - starts_1_accuracy: 0.5103 - stops_1_accuracy: 0.5328 - val_loss: 6.4312 - val_starts_0_loss: 1.4516 - val_stops_0_loss: 1.5334 - val_starts_1_loss: 1.6125 - val_stops_1_loss: 1.8260 - val_starts_0_accuracy: 0.5062 - val_stops_0_accuracy: 0.4963 - val_starts_1_accuracy: 0.4988 - val_stops_1_accuracy: 0.5087\n",
      "[INFO]  =============== Validation for FOLD# 1 ===============\n",
      "[INFO] 60.03 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 60.30 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 50.62 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 49.63 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 68.77 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 65.67 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 42.14 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 42.90 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 69.57 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 62.05 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 43.90 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 41.17 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 68.91 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 71.14 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 42.25 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 46.46 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (11212, 76) (11212, 76)\n",
      "[INFO] Prediction shape for validation data:  (2803, 76) (2803, 76)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  8789 out of 11212\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  2274 out of 2803\n",
      "[INFO] Training Jaccard Score:  0.29435568475063767\n",
      "[INFO] Validation Jaccard Score:  0.29240676455868414\n",
      "[INFO] Training for fold: 1 finished at Sun May 17 05:54:47 2020\n",
      "[INFO] ==================== FOLD# 2 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/10\n",
      "11212/11212 [==============================] - 214s 19ms/sample - loss: 10.7244 - starts_0_loss: 2.3667 - stops_0_loss: 2.6484 - starts_1_loss: 2.6302 - stops_1_loss: 3.0789 - starts_0_accuracy: 0.3520 - stops_0_accuracy: 0.2683 - starts_1_accuracy: 0.3537 - stops_1_accuracy: 0.1872 - val_loss: 8.7651 - val_starts_0_loss: 1.9182 - val_stops_0_loss: 2.0306 - val_starts_1_loss: 2.2255 - val_stops_1_loss: 2.5840 - val_starts_0_accuracy: 0.3996 - val_stops_0_accuracy: 0.3999 - val_starts_1_accuracy: 0.3511 - val_stops_1_accuracy: 0.3428\n",
      "Epoch 2/10\n",
      "11212/11212 [==============================] - 196s 18ms/sample - loss: 8.5834 - starts_0_loss: 1.9871 - stops_0_loss: 2.0871 - starts_1_loss: 2.1109 - stops_1_loss: 2.3983 - starts_0_accuracy: 0.4027 - stops_0_accuracy: 0.4055 - starts_1_accuracy: 0.3849 - stops_1_accuracy: 0.3802 - val_loss: 7.4388 - val_starts_0_loss: 1.7254 - val_stops_0_loss: 1.6997 - val_starts_1_loss: 1.9276 - val_stops_1_loss: 2.0831 - val_starts_0_accuracy: 0.4328 - val_stops_0_accuracy: 0.4880 - val_starts_1_accuracy: 0.4056 - val_stops_1_accuracy: 0.4827\n",
      "Epoch 3/10\n",
      "11212/11212 [==============================] - 196s 17ms/sample - loss: 8.0343 - starts_0_loss: 1.9029 - stops_0_loss: 1.9261 - starts_1_loss: 2.0002 - stops_1_loss: 2.2053 - starts_0_accuracy: 0.4154 - stops_0_accuracy: 0.4362 - starts_1_accuracy: 0.4020 - stops_1_accuracy: 0.4223 - val_loss: 7.1826 - val_starts_0_loss: 1.6810 - val_stops_0_loss: 1.6497 - val_starts_1_loss: 1.8679 - val_stops_1_loss: 1.9826 - val_starts_0_accuracy: 0.4520 - val_stops_0_accuracy: 0.4905 - val_starts_1_accuracy: 0.4188 - val_stops_1_accuracy: 0.4930\n",
      "Epoch 4/10\n",
      "11212/11212 [==============================] - 196s 17ms/sample - loss: 7.7895 - starts_0_loss: 1.8323 - stops_0_loss: 1.8751 - starts_1_loss: 1.9328 - stops_1_loss: 2.1502 - starts_0_accuracy: 0.4294 - stops_0_accuracy: 0.4430 - starts_1_accuracy: 0.4180 - stops_1_accuracy: 0.4333 - val_loss: 6.9946 - val_starts_0_loss: 1.6524 - val_stops_0_loss: 1.6242 - val_starts_1_loss: 1.7938 - val_stops_1_loss: 1.9230 - val_starts_0_accuracy: 0.4724 - val_stops_0_accuracy: 0.4905 - val_starts_1_accuracy: 0.4449 - val_stops_1_accuracy: 0.5009\n",
      "Epoch 5/10\n",
      "11212/11212 [==============================] - 197s 18ms/sample - loss: 7.6557 - starts_0_loss: 1.8259 - stops_0_loss: 1.8287 - starts_1_loss: 1.9078 - stops_1_loss: 2.0931 - starts_0_accuracy: 0.4255 - stops_0_accuracy: 0.4588 - starts_1_accuracy: 0.4250 - stops_1_accuracy: 0.4519 - val_loss: 6.8682 - val_starts_0_loss: 1.6173 - val_stops_0_loss: 1.5923 - val_starts_1_loss: 1.7627 - val_stops_1_loss: 1.8961 - val_starts_0_accuracy: 0.4599 - val_stops_0_accuracy: 0.4941 - val_starts_1_accuracy: 0.4442 - val_stops_1_accuracy: 0.5055\n",
      "Epoch 6/10\n",
      "11212/11212 [==============================] - 199s 18ms/sample - loss: 7.5930 - starts_0_loss: 1.7953 - stops_0_loss: 1.8287 - starts_1_loss: 1.8830 - stops_1_loss: 2.0858 - starts_0_accuracy: 0.4387 - stops_0_accuracy: 0.4547 - starts_1_accuracy: 0.4298 - stops_1_accuracy: 0.4478 - val_loss: 6.9389 - val_starts_0_loss: 1.6447 - val_stops_0_loss: 1.5996 - val_starts_1_loss: 1.7854 - val_stops_1_loss: 1.9086 - val_starts_0_accuracy: 0.4499 - val_stops_0_accuracy: 0.4952 - val_starts_1_accuracy: 0.4370 - val_stops_1_accuracy: 0.5027\n",
      "Epoch 7/10\n",
      "11212/11212 [==============================] - 197s 18ms/sample - loss: 7.4762 - starts_0_loss: 1.7799 - stops_0_loss: 1.7977 - starts_1_loss: 1.8577 - stops_1_loss: 2.0414 - starts_0_accuracy: 0.4464 - stops_0_accuracy: 0.4572 - starts_1_accuracy: 0.4363 - stops_1_accuracy: 0.4520 - val_loss: 6.9407 - val_starts_0_loss: 1.6652 - val_stops_0_loss: 1.5735 - val_starts_1_loss: 1.8247 - val_stops_1_loss: 1.8744 - val_starts_0_accuracy: 0.4413 - val_stops_0_accuracy: 0.4934 - val_starts_1_accuracy: 0.4299 - val_stops_1_accuracy: 0.5127\n",
      "Epoch 8/10\n",
      "11212/11212 [==============================] - 196s 18ms/sample - loss: 7.3757 - starts_0_loss: 1.7523 - stops_0_loss: 1.7688 - starts_1_loss: 1.8359 - stops_1_loss: 2.0179 - starts_0_accuracy: 0.4452 - stops_0_accuracy: 0.4700 - starts_1_accuracy: 0.4385 - stops_1_accuracy: 0.4616 - val_loss: 6.8467 - val_starts_0_loss: 1.6231 - val_stops_0_loss: 1.5620 - val_starts_1_loss: 1.7772 - val_stops_1_loss: 1.8848 - val_starts_0_accuracy: 0.4460 - val_stops_0_accuracy: 0.5034 - val_starts_1_accuracy: 0.4349 - val_stops_1_accuracy: 0.5091\n",
      "Epoch 9/10\n",
      "11212/11212 [==============================] - 197s 18ms/sample - loss: 7.4054 - starts_0_loss: 1.7633 - stops_0_loss: 1.7808 - starts_1_loss: 1.8457 - stops_1_loss: 2.0153 - starts_0_accuracy: 0.4383 - stops_0_accuracy: 0.4611 - starts_1_accuracy: 0.4372 - stops_1_accuracy: 0.4635 - val_loss: 6.8156 - val_starts_0_loss: 1.6256 - val_stops_0_loss: 1.5617 - val_starts_1_loss: 1.7495 - val_stops_1_loss: 1.8820 - val_starts_0_accuracy: 0.4631 - val_stops_0_accuracy: 0.5041 - val_starts_1_accuracy: 0.4474 - val_stops_1_accuracy: 0.5095\n",
      "Epoch 10/10\n",
      "11212/11212 [==============================] - 196s 18ms/sample - loss: 7.3436 - starts_0_loss: 1.7462 - stops_0_loss: 1.7667 - starts_1_loss: 1.8225 - stops_1_loss: 2.0077 - starts_0_accuracy: 0.4484 - stops_0_accuracy: 0.4599 - starts_1_accuracy: 0.4415 - stops_1_accuracy: 0.4581 - val_loss: 6.7506 - val_starts_0_loss: 1.5789 - val_stops_0_loss: 1.5791 - val_starts_1_loss: 1.7259 - val_stops_1_loss: 1.8689 - val_starts_0_accuracy: 0.4641 - val_stops_0_accuracy: 0.5052 - val_starts_1_accuracy: 0.4467 - val_stops_1_accuracy: 0.5102\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "11212/11212 [==============================] - 215s 19ms/sample - loss: 7.1116 - starts_0_loss: 1.7028 - stops_0_loss: 1.7070 - starts_1_loss: 1.7699 - stops_1_loss: 1.9328 - starts_0_accuracy: 0.4613 - stops_0_accuracy: 0.4818 - starts_1_accuracy: 0.4573 - stops_1_accuracy: 0.4801 - val_loss: 6.7063 - val_starts_0_loss: 1.5764 - val_stops_0_loss: 1.5540 - val_starts_1_loss: 1.7265 - val_stops_1_loss: 1.8508 - val_starts_0_accuracy: 0.4674 - val_stops_0_accuracy: 0.5059 - val_starts_1_accuracy: 0.4538 - val_stops_1_accuracy: 0.5152\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 197s 18ms/sample - loss: 7.0822 - starts_0_loss: 1.6944 - stops_0_loss: 1.6980 - starts_1_loss: 1.7678 - stops_1_loss: 1.9227 - starts_0_accuracy: 0.4649 - stops_0_accuracy: 0.4834 - starts_1_accuracy: 0.4566 - stops_1_accuracy: 0.4829 - val_loss: 6.6936 - val_starts_0_loss: 1.5758 - val_stops_0_loss: 1.5472 - val_starts_1_loss: 1.7276 - val_stops_1_loss: 1.8442 - val_starts_0_accuracy: 0.4645 - val_stops_0_accuracy: 0.5073 - val_starts_1_accuracy: 0.4563 - val_stops_1_accuracy: 0.5191\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 197s 18ms/sample - loss: 7.0613 - starts_0_loss: 1.6828 - stops_0_loss: 1.6983 - starts_1_loss: 1.7630 - stops_1_loss: 1.9174 - starts_0_accuracy: 0.4608 - stops_0_accuracy: 0.4844 - starts_1_accuracy: 0.4528 - stops_1_accuracy: 0.4856 - val_loss: 6.6725 - val_starts_0_loss: 1.5700 - val_stops_0_loss: 1.5451 - val_starts_1_loss: 1.7209 - val_stops_1_loss: 1.8380 - val_starts_0_accuracy: 0.4716 - val_stops_0_accuracy: 0.5062 - val_starts_1_accuracy: 0.4577 - val_stops_1_accuracy: 0.5194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 197s 18ms/sample - loss: 7.0306 - starts_0_loss: 1.6850 - stops_0_loss: 1.6814 - starts_1_loss: 1.7667 - stops_1_loss: 1.8979 - starts_0_accuracy: 0.4583 - stops_0_accuracy: 0.4856 - starts_1_accuracy: 0.4526 - stops_1_accuracy: 0.4901 - val_loss: 6.6702 - val_starts_0_loss: 1.5701 - val_stops_0_loss: 1.5401 - val_starts_1_loss: 1.7236 - val_stops_1_loss: 1.8380 - val_starts_0_accuracy: 0.4681 - val_stops_0_accuracy: 0.5055 - val_starts_1_accuracy: 0.4581 - val_stops_1_accuracy: 0.5219\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 199s 18ms/sample - loss: 6.9820 - starts_0_loss: 1.6647 - stops_0_loss: 1.6772 - starts_1_loss: 1.7445 - stops_1_loss: 1.8951 - starts_0_accuracy: 0.4690 - stops_0_accuracy: 0.4873 - starts_1_accuracy: 0.4623 - stops_1_accuracy: 0.4891 - val_loss: 6.6544 - val_starts_0_loss: 1.5638 - val_stops_0_loss: 1.5385 - val_starts_1_loss: 1.7180 - val_stops_1_loss: 1.8357 - val_starts_0_accuracy: 0.4691 - val_stops_0_accuracy: 0.5045 - val_starts_1_accuracy: 0.4588 - val_stops_1_accuracy: 0.5209\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "11212/11212 [==============================] - 351s 31ms/sample - loss: 6.8762 - starts_0_loss: 1.6353 - stops_0_loss: 1.6452 - starts_1_loss: 1.7236 - stops_1_loss: 1.8722 - starts_0_accuracy: 0.4735 - stops_0_accuracy: 0.4953 - starts_1_accuracy: 0.4691 - stops_1_accuracy: 0.4955 - val_loss: 6.3840 - val_starts_0_loss: 1.4898 - val_stops_0_loss: 1.4775 - val_starts_1_loss: 1.6461 - val_stops_1_loss: 1.7736 - val_starts_0_accuracy: 0.4984 - val_stops_0_accuracy: 0.5244 - val_starts_1_accuracy: 0.4823 - val_stops_1_accuracy: 0.5287\n",
      "Epoch 2/2\n",
      "11212/11212 [==============================] - 380s 34ms/sample - loss: 6.5672 - starts_0_loss: 1.5620 - stops_0_loss: 1.5719 - starts_1_loss: 1.6402 - stops_1_loss: 1.7929 - starts_0_accuracy: 0.4953 - stops_0_accuracy: 0.5177 - starts_1_accuracy: 0.4927 - stops_1_accuracy: 0.5202 - val_loss: 6.2760 - val_starts_0_loss: 1.4622 - val_stops_0_loss: 1.4511 - val_starts_1_loss: 1.6199 - val_stops_1_loss: 1.7471 - val_starts_0_accuracy: 0.5048 - val_stops_0_accuracy: 0.5344 - val_starts_1_accuracy: 0.4966 - val_stops_1_accuracy: 0.5316\n",
      "[INFO]  =============== Validation for FOLD# 2 ===============\n",
      "[INFO] 57.22 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 59.16 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 50.48 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 53.44 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 63.12 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 63.85 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 36.11 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 45.90 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 68.42 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 61.13 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 41.32 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 44.92 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 59.96 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 67.85 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 33.70 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 47.76 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (11212, 76) (11212, 76)\n",
      "[INFO] Prediction shape for validation data:  (2803, 76) (2803, 76)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  8581 out of 11212\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  2161 out of 2803\n",
      "[INFO] Training Jaccard Score:  0.2952491745184171\n",
      "[INFO] Validation Jaccard Score:  0.3009135849685619\n",
      "[INFO] Training for fold: 2 finished at Sun May 17 06:58:22 2020\n",
      "[INFO] ==================== FOLD# 3 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/10\n",
      "11212/11212 [==============================] - 216s 19ms/sample - loss: 10.5688 - starts_0_loss: 2.3756 - stops_0_loss: 2.5306 - starts_1_loss: 2.6363 - stops_1_loss: 3.0257 - starts_0_accuracy: 0.3584 - stops_0_accuracy: 0.3016 - starts_1_accuracy: 0.3491 - stops_1_accuracy: 0.2269 - val_loss: 8.2506 - val_starts_0_loss: 1.9000 - val_stops_0_loss: 1.8096 - val_starts_1_loss: 2.1782 - val_stops_1_loss: 2.3551 - val_starts_0_accuracy: 0.3874 - val_stops_0_accuracy: 0.4538 - val_starts_1_accuracy: 0.3607 - val_stops_1_accuracy: 0.4160\n",
      "Epoch 2/10\n",
      "11212/11212 [==============================] - 197s 18ms/sample - loss: 8.5270 - starts_0_loss: 1.9778 - stops_0_loss: 2.0790 - starts_1_loss: 2.1056 - stops_1_loss: 2.3642 - starts_0_accuracy: 0.4042 - stops_0_accuracy: 0.4062 - starts_1_accuracy: 0.3809 - stops_1_accuracy: 0.3827 - val_loss: 8.0449 - val_starts_0_loss: 1.9675 - val_stops_0_loss: 1.7269 - val_starts_1_loss: 2.1387 - val_stops_1_loss: 2.2013 - val_starts_0_accuracy: 0.3914 - val_stops_0_accuracy: 0.4720 - val_starts_1_accuracy: 0.3717 - val_stops_1_accuracy: 0.4417\n",
      "Epoch 3/10\n",
      "11212/11212 [==============================] - 198s 18ms/sample - loss: 8.1116 - starts_0_loss: 1.9042 - stops_0_loss: 1.9801 - starts_1_loss: 1.9966 - stops_1_loss: 2.2311 - starts_0_accuracy: 0.4153 - stops_0_accuracy: 0.4205 - starts_1_accuracy: 0.4020 - stops_1_accuracy: 0.4050 - val_loss: 7.7337 - val_starts_0_loss: 1.8347 - val_stops_0_loss: 1.7274 - val_starts_1_loss: 2.0300 - val_stops_1_loss: 2.1293 - val_starts_0_accuracy: 0.4014 - val_stops_0_accuracy: 0.4666 - val_starts_1_accuracy: 0.3857 - val_stops_1_accuracy: 0.4410\n",
      "Epoch 4/10\n",
      "11212/11212 [==============================] - 203s 18ms/sample - loss: 7.8693 - starts_0_loss: 1.8414 - stops_0_loss: 1.9326 - starts_1_loss: 1.9284 - stops_1_loss: 2.1659 - starts_0_accuracy: 0.4310 - stops_0_accuracy: 0.4287 - starts_1_accuracy: 0.4166 - stops_1_accuracy: 0.4186 - val_loss: 7.1102 - val_starts_0_loss: 1.6903 - val_stops_0_loss: 1.6264 - val_starts_1_loss: 1.8502 - val_stops_1_loss: 1.9306 - val_starts_0_accuracy: 0.4527 - val_stops_0_accuracy: 0.4920 - val_starts_1_accuracy: 0.4278 - val_stops_1_accuracy: 0.4955\n",
      "Epoch 5/10\n",
      "11212/11212 [==============================] - 199s 18ms/sample - loss: 7.7637 - starts_0_loss: 1.8259 - stops_0_loss: 1.9017 - starts_1_loss: 1.9071 - stops_1_loss: 2.1285 - starts_0_accuracy: 0.4356 - stops_0_accuracy: 0.4379 - starts_1_accuracy: 0.4250 - stops_1_accuracy: 0.4267 - val_loss: 7.1472 - val_starts_0_loss: 1.6945 - val_stops_0_loss: 1.6281 - val_starts_1_loss: 1.8788 - val_stops_1_loss: 1.9363 - val_starts_0_accuracy: 0.4438 - val_stops_0_accuracy: 0.4870 - val_starts_1_accuracy: 0.4196 - val_stops_1_accuracy: 0.4834\n",
      "Epoch 6/10\n",
      "11212/11212 [==============================] - 198s 18ms/sample - loss: 7.6814 - starts_0_loss: 1.8096 - stops_0_loss: 1.8810 - starts_1_loss: 1.8895 - stops_1_loss: 2.1013 - starts_0_accuracy: 0.4378 - stops_0_accuracy: 0.4460 - starts_1_accuracy: 0.4280 - stops_1_accuracy: 0.4332 - val_loss: 7.0514 - val_starts_0_loss: 1.6564 - val_stops_0_loss: 1.6458 - val_starts_1_loss: 1.8224 - val_stops_1_loss: 1.9172 - val_starts_0_accuracy: 0.4609 - val_stops_0_accuracy: 0.4866 - val_starts_1_accuracy: 0.4270 - val_stops_1_accuracy: 0.4902\n",
      "Epoch 7/10\n",
      "11212/11212 [==============================] - 197s 18ms/sample - loss: 7.5990 - starts_0_loss: 1.7870 - stops_0_loss: 1.8693 - starts_1_loss: 1.8612 - stops_1_loss: 2.0811 - starts_0_accuracy: 0.4436 - stops_0_accuracy: 0.4403 - starts_1_accuracy: 0.4359 - stops_1_accuracy: 0.4339 - val_loss: 7.0261 - val_starts_0_loss: 1.6590 - val_stops_0_loss: 1.6234 - val_starts_1_loss: 1.8256 - val_stops_1_loss: 1.9114 - val_starts_0_accuracy: 0.4545 - val_stops_0_accuracy: 0.4984 - val_starts_1_accuracy: 0.4217 - val_stops_1_accuracy: 0.4945\n",
      "Epoch 8/10\n",
      "11212/11212 [==============================] - 197s 18ms/sample - loss: 7.5472 - starts_0_loss: 1.7815 - stops_0_loss: 1.8534 - starts_1_loss: 1.8552 - stops_1_loss: 2.0563 - starts_0_accuracy: 0.4446 - stops_0_accuracy: 0.4467 - starts_1_accuracy: 0.4388 - stops_1_accuracy: 0.4403 - val_loss: 6.9589 - val_starts_0_loss: 1.6490 - val_stops_0_loss: 1.5990 - val_starts_1_loss: 1.8103 - val_stops_1_loss: 1.8912 - val_starts_0_accuracy: 0.4517 - val_stops_0_accuracy: 0.5059 - val_starts_1_accuracy: 0.4328 - val_stops_1_accuracy: 0.4948\n",
      "Epoch 9/10\n",
      "11212/11212 [==============================] - 198s 18ms/sample - loss: 7.5479 - starts_0_loss: 1.7884 - stops_0_loss: 1.8429 - starts_1_loss: 1.8637 - stops_1_loss: 2.0535 - starts_0_accuracy: 0.4425 - stops_0_accuracy: 0.4445 - starts_1_accuracy: 0.4405 - stops_1_accuracy: 0.4402 - val_loss: 7.2194 - val_starts_0_loss: 1.7212 - val_stops_0_loss: 1.6265 - val_starts_1_loss: 1.8979 - val_stops_1_loss: 1.9691 - val_starts_0_accuracy: 0.4310 - val_stops_0_accuracy: 0.4791 - val_starts_1_accuracy: 0.4178 - val_stops_1_accuracy: 0.4784\n",
      "Epoch 10/10\n",
      "11212/11212 [==============================] - 204s 18ms/sample - loss: 7.4637 - starts_0_loss: 1.7595 - stops_0_loss: 1.8343 - starts_1_loss: 1.8299 - stops_1_loss: 2.0393 - starts_0_accuracy: 0.4490 - stops_0_accuracy: 0.4484 - starts_1_accuracy: 0.4394 - stops_1_accuracy: 0.4451 - val_loss: 6.8850 - val_starts_0_loss: 1.6372 - val_stops_0_loss: 1.5904 - val_starts_1_loss: 1.7831 - val_stops_1_loss: 1.8662 - val_starts_0_accuracy: 0.4624 - val_stops_0_accuracy: 0.4905 - val_starts_1_accuracy: 0.4470 - val_stops_1_accuracy: 0.4888\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "11212/11212 [==============================] - 216s 19ms/sample - loss: 7.2601 - starts_0_loss: 1.7159 - stops_0_loss: 1.7806 - starts_1_loss: 1.7887 - stops_1_loss: 1.9752 - starts_0_accuracy: 0.4641 - stops_0_accuracy: 0.4613 - starts_1_accuracy: 0.4558 - stops_1_accuracy: 0.4594 - val_loss: 6.8497 - val_starts_0_loss: 1.6223 - val_stops_0_loss: 1.5759 - val_starts_1_loss: 1.7828 - val_stops_1_loss: 1.8607 - val_starts_0_accuracy: 0.4627 - val_stops_0_accuracy: 0.4934 - val_starts_1_accuracy: 0.4470 - val_stops_1_accuracy: 0.4988\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 198s 18ms/sample - loss: 7.2358 - starts_0_loss: 1.7061 - stops_0_loss: 1.7715 - starts_1_loss: 1.7855 - stops_1_loss: 1.9733 - starts_0_accuracy: 0.4636 - stops_0_accuracy: 0.4604 - starts_1_accuracy: 0.4572 - stops_1_accuracy: 0.4574 - val_loss: 6.8215 - val_starts_0_loss: 1.6157 - val_stops_0_loss: 1.5645 - val_starts_1_loss: 1.7783 - val_stops_1_loss: 1.8552 - val_starts_0_accuracy: 0.4652 - val_stops_0_accuracy: 0.4970 - val_starts_1_accuracy: 0.4467 - val_stops_1_accuracy: 0.4984\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 198s 18ms/sample - loss: 7.1837 - starts_0_loss: 1.6882 - stops_0_loss: 1.7662 - starts_1_loss: 1.7690 - stops_1_loss: 1.9599 - starts_0_accuracy: 0.4687 - stops_0_accuracy: 0.4609 - starts_1_accuracy: 0.4589 - stops_1_accuracy: 0.4637 - val_loss: 6.8111 - val_starts_0_loss: 1.6138 - val_stops_0_loss: 1.5607 - val_starts_1_loss: 1.7768 - val_stops_1_loss: 1.8520 - val_starts_0_accuracy: 0.4645 - val_stops_0_accuracy: 0.4970 - val_starts_1_accuracy: 0.4456 - val_stops_1_accuracy: 0.4984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 200s 18ms/sample - loss: 7.1625 - starts_0_loss: 1.6932 - stops_0_loss: 1.7561 - starts_1_loss: 1.7694 - stops_1_loss: 1.9434 - starts_0_accuracy: 0.4641 - stops_0_accuracy: 0.4694 - starts_1_accuracy: 0.4606 - stops_1_accuracy: 0.4682 - val_loss: 6.7867 - val_starts_0_loss: 1.6058 - val_stops_0_loss: 1.5566 - val_starts_1_loss: 1.7691 - val_stops_1_loss: 1.8475 - val_starts_0_accuracy: 0.4645 - val_stops_0_accuracy: 0.4952 - val_starts_1_accuracy: 0.4502 - val_stops_1_accuracy: 0.4995\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 200s 18ms/sample - loss: 7.0846 - starts_0_loss: 1.6776 - stops_0_loss: 1.7348 - starts_1_loss: 1.7474 - stops_1_loss: 1.9249 - starts_0_accuracy: 0.4716 - stops_0_accuracy: 0.4728 - starts_1_accuracy: 0.4644 - stops_1_accuracy: 0.4710 - val_loss: 6.7820 - val_starts_0_loss: 1.6065 - val_stops_0_loss: 1.5561 - val_starts_1_loss: 1.7682 - val_stops_1_loss: 1.8434 - val_starts_0_accuracy: 0.4609 - val_stops_0_accuracy: 0.4952 - val_starts_1_accuracy: 0.4506 - val_stops_1_accuracy: 0.4984\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "11212/11212 [==============================] - 349s 31ms/sample - loss: 6.9712 - starts_0_loss: 1.6472 - stops_0_loss: 1.7110 - starts_1_loss: 1.7171 - stops_1_loss: 1.8961 - starts_0_accuracy: 0.4798 - stops_0_accuracy: 0.4791 - starts_1_accuracy: 0.4740 - stops_1_accuracy: 0.4826 - val_loss: 6.5640 - val_starts_0_loss: 1.5473 - val_stops_0_loss: 1.4954 - val_starts_1_loss: 1.7160 - val_stops_1_loss: 1.7984 - val_starts_0_accuracy: 0.4784 - val_stops_0_accuracy: 0.5205 - val_starts_1_accuracy: 0.4684 - val_stops_1_accuracy: 0.5241\n",
      "Epoch 2/2\n",
      "11212/11212 [==============================] - 328s 29ms/sample - loss: 6.8161 - starts_0_loss: 1.5967 - stops_0_loss: 1.6725 - starts_1_loss: 1.6802 - stops_1_loss: 1.8672 - starts_0_accuracy: 0.4922 - stops_0_accuracy: 0.4853 - starts_1_accuracy: 0.4830 - stops_1_accuracy: 0.4878 - val_loss: 6.4881 - val_starts_0_loss: 1.5203 - val_stops_0_loss: 1.4956 - val_starts_1_loss: 1.6901 - val_stops_1_loss: 1.7748 - val_starts_0_accuracy: 0.4880 - val_stops_0_accuracy: 0.5159 - val_starts_1_accuracy: 0.4699 - val_stops_1_accuracy: 0.5287\n",
      "[INFO]  =============== Validation for FOLD# 3 ===============\n",
      "[INFO] 57.48 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 57.30 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 48.80 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 51.59 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 62.23 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 62.08 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 37.06 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 47.30 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 65.31 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 58.70 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 41.86 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 44.71 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 60.64 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 67.89 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 35.22 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 53.16 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t||"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (11212, 76) (11212, 76)\n",
      "[INFO] Prediction shape for validation data:  (2803, 76) (2803, 76)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  8737 out of 11212\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  2233 out of 2803\n",
      "[INFO] Training Jaccard Score:  0.29487799198387005\n",
      "[INFO] Validation Jaccard Score:  0.3022413532541259\n",
      "[INFO] Training for fold: 3 finished at Sun May 17 08:01:35 2020\n",
      "[INFO] ==================== FOLD# 4 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/10\n",
      "11212/11212 [==============================] - 213s 19ms/sample - loss: 10.5122 - starts_0_loss: 2.2919 - stops_0_loss: 2.5585 - starts_1_loss: 2.6236 - stops_1_loss: 3.0381 - starts_0_accuracy: 0.3714 - stops_0_accuracy: 0.3105 - starts_1_accuracy: 0.3461 - stops_1_accuracy: 0.2286 - val_loss: 8.1758 - val_starts_0_loss: 1.7685 - val_stops_0_loss: 1.9186 - val_starts_1_loss: 2.0823 - val_stops_1_loss: 2.4070 - val_starts_0_accuracy: 0.4303 - val_stops_0_accuracy: 0.4481 - val_starts_1_accuracy: 0.3899 - val_stops_1_accuracy: 0.4163\n",
      "Epoch 2/10\n",
      "11212/11212 [==============================] - 195s 17ms/sample - loss: 8.5137 - starts_0_loss: 1.9737 - stops_0_loss: 2.0734 - starts_1_loss: 2.0868 - stops_1_loss: 2.3791 - starts_0_accuracy: 0.4106 - stops_0_accuracy: 0.4217 - starts_1_accuracy: 0.3882 - stops_1_accuracy: 0.3932 - val_loss: 7.6212 - val_starts_0_loss: 1.7191 - val_stops_0_loss: 1.7933 - val_starts_1_loss: 1.9343 - val_stops_1_loss: 2.1696 - val_starts_0_accuracy: 0.4292 - val_stops_0_accuracy: 0.4492 - val_starts_1_accuracy: 0.4003 - val_stops_1_accuracy: 0.4374\n",
      "Epoch 3/10\n",
      "11212/11212 [==============================] - 196s 17ms/sample - loss: 8.0440 - starts_0_loss: 1.8794 - stops_0_loss: 1.9599 - starts_1_loss: 1.9738 - stops_1_loss: 2.2310 - starts_0_accuracy: 0.4285 - stops_0_accuracy: 0.4311 - starts_1_accuracy: 0.4149 - stops_1_accuracy: 0.4163 - val_loss: 7.4447 - val_starts_0_loss: 1.6688 - val_stops_0_loss: 1.7663 - val_starts_1_loss: 1.8709 - val_stops_1_loss: 2.1326 - val_starts_0_accuracy: 0.4492 - val_stops_0_accuracy: 0.4748 - val_starts_1_accuracy: 0.4213 - val_stops_1_accuracy: 0.4506\n",
      "Epoch 4/10\n",
      "11212/11212 [==============================] - 198s 18ms/sample - loss: 7.8249 - starts_0_loss: 1.8266 - stops_0_loss: 1.9193 - starts_1_loss: 1.9082 - stops_1_loss: 2.1713 - starts_0_accuracy: 0.4466 - stops_0_accuracy: 0.4445 - starts_1_accuracy: 0.4312 - stops_1_accuracy: 0.4321 - val_loss: 7.2366 - val_starts_0_loss: 1.6351 - val_stops_0_loss: 1.7443 - val_starts_1_loss: 1.8202 - val_stops_1_loss: 2.0401 - val_starts_0_accuracy: 0.4567 - val_stops_0_accuracy: 0.4609 - val_starts_1_accuracy: 0.4356 - val_stops_1_accuracy: 0.4677\n",
      "Epoch 5/10\n",
      "11212/11212 [==============================] - 195s 17ms/sample - loss: 7.6845 - starts_0_loss: 1.7971 - stops_0_loss: 1.8989 - starts_1_loss: 1.8732 - stops_1_loss: 2.1157 - starts_0_accuracy: 0.4475 - stops_0_accuracy: 0.4549 - starts_1_accuracy: 0.4399 - stops_1_accuracy: 0.4447 - val_loss: 7.1478 - val_starts_0_loss: 1.6191 - val_stops_0_loss: 1.7290 - val_starts_1_loss: 1.7665 - val_stops_1_loss: 2.0396 - val_starts_0_accuracy: 0.4592 - val_stops_0_accuracy: 0.4741 - val_starts_1_accuracy: 0.4438 - val_stops_1_accuracy: 0.4616\n",
      "Epoch 6/10\n",
      "11212/11212 [==============================] - 195s 17ms/sample - loss: 7.6235 - starts_0_loss: 1.7909 - stops_0_loss: 1.8744 - starts_1_loss: 1.8580 - stops_1_loss: 2.0995 - starts_0_accuracy: 0.4520 - stops_0_accuracy: 0.4588 - starts_1_accuracy: 0.4426 - stops_1_accuracy: 0.4525 - val_loss: 7.2373 - val_starts_0_loss: 1.6738 - val_stops_0_loss: 1.7448 - val_starts_1_loss: 1.8042 - val_stops_1_loss: 2.0133 - val_starts_0_accuracy: 0.4559 - val_stops_0_accuracy: 0.4659 - val_starts_1_accuracy: 0.4431 - val_stops_1_accuracy: 0.4734\n",
      "Epoch 7/10\n",
      "11212/11212 [==============================] - 195s 17ms/sample - loss: 7.5397 - starts_0_loss: 1.7745 - stops_0_loss: 1.8615 - starts_1_loss: 1.8354 - stops_1_loss: 2.0682 - starts_0_accuracy: 0.4545 - stops_0_accuracy: 0.4577 - starts_1_accuracy: 0.4445 - stops_1_accuracy: 0.4498 - val_loss: 7.1669 - val_starts_0_loss: 1.6461 - val_stops_0_loss: 1.7215 - val_starts_1_loss: 1.7834 - val_stops_1_loss: 2.0129 - val_starts_0_accuracy: 0.4606 - val_stops_0_accuracy: 0.4684 - val_starts_1_accuracy: 0.4517 - val_stops_1_accuracy: 0.4763\n",
      "Epoch 8/10\n",
      "11212/11212 [==============================] - 195s 17ms/sample - loss: 7.5194 - starts_0_loss: 1.7694 - stops_0_loss: 1.8525 - starts_1_loss: 1.8312 - stops_1_loss: 2.0663 - starts_0_accuracy: 0.4542 - stops_0_accuracy: 0.4615 - starts_1_accuracy: 0.4460 - stops_1_accuracy: 0.4533 - val_loss: 7.0954 - val_starts_0_loss: 1.6303 - val_stops_0_loss: 1.6878 - val_starts_1_loss: 1.7670 - val_stops_1_loss: 2.0083 - val_starts_0_accuracy: 0.4688 - val_stops_0_accuracy: 0.4773 - val_starts_1_accuracy: 0.4509 - val_stops_1_accuracy: 0.4798\n",
      "Epoch 9/10\n",
      "11212/11212 [==============================] - 195s 17ms/sample - loss: 7.4346 - starts_0_loss: 1.7478 - stops_0_loss: 1.8369 - starts_1_loss: 1.8079 - stops_1_loss: 2.0426 - starts_0_accuracy: 0.4597 - stops_0_accuracy: 0.4642 - starts_1_accuracy: 0.4550 - stops_1_accuracy: 0.4604 - val_loss: 7.0675 - val_starts_0_loss: 1.6100 - val_stops_0_loss: 1.7069 - val_starts_1_loss: 1.7374 - val_stops_1_loss: 2.0172 - val_starts_0_accuracy: 0.4624 - val_stops_0_accuracy: 0.4713 - val_starts_1_accuracy: 0.4502 - val_stops_1_accuracy: 0.4709\n",
      "Epoch 10/10\n",
      "11212/11212 [==============================] - 195s 17ms/sample - loss: 7.4548 - starts_0_loss: 1.7536 - stops_0_loss: 1.8392 - starts_1_loss: 1.8063 - stops_1_loss: 2.0564 - starts_0_accuracy: 0.4580 - stops_0_accuracy: 0.4628 - starts_1_accuracy: 0.4523 - stops_1_accuracy: 0.4489 - val_loss: 7.0739 - val_starts_0_loss: 1.6173 - val_stops_0_loss: 1.7111 - val_starts_1_loss: 1.7484 - val_stops_1_loss: 1.9969 - val_starts_0_accuracy: 0.4624 - val_stops_0_accuracy: 0.4699 - val_starts_1_accuracy: 0.4517 - val_stops_1_accuracy: 0.4777\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "11212/11212 [==============================] - 216s 19ms/sample - loss: 7.2134 - starts_0_loss: 1.7050 - stops_0_loss: 1.7759 - starts_1_loss: 1.7565 - stops_1_loss: 1.9761 - starts_0_accuracy: 0.4682 - stops_0_accuracy: 0.4766 - starts_1_accuracy: 0.4594 - stops_1_accuracy: 0.4704 - val_loss: 7.0034 - val_starts_0_loss: 1.6025 - val_stops_0_loss: 1.6786 - val_starts_1_loss: 1.7394 - val_stops_1_loss: 1.9836 - val_starts_0_accuracy: 0.4588 - val_stops_0_accuracy: 0.4784 - val_starts_1_accuracy: 0.4520 - val_stops_1_accuracy: 0.4848\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 198s 18ms/sample - loss: 7.1411 - starts_0_loss: 1.6832 - stops_0_loss: 1.7525 - starts_1_loss: 1.7464 - stops_1_loss: 1.9586 - starts_0_accuracy: 0.4727 - stops_0_accuracy: 0.4812 - starts_1_accuracy: 0.4668 - stops_1_accuracy: 0.4743 - val_loss: 6.9727 - val_starts_0_loss: 1.5930 - val_stops_0_loss: 1.6698 - val_starts_1_loss: 1.7340 - val_stops_1_loss: 1.9775 - val_starts_0_accuracy: 0.4592 - val_stops_0_accuracy: 0.4798 - val_starts_1_accuracy: 0.4545 - val_stops_1_accuracy: 0.4866\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 200s 18ms/sample - loss: 7.1345 - starts_0_loss: 1.6872 - stops_0_loss: 1.7494 - starts_1_loss: 1.7402 - stops_1_loss: 1.9575 - starts_0_accuracy: 0.4746 - stops_0_accuracy: 0.4810 - starts_1_accuracy: 0.4681 - stops_1_accuracy: 0.4796 - val_loss: 6.9517 - val_starts_0_loss: 1.5915 - val_stops_0_loss: 1.6665 - val_starts_1_loss: 1.7285 - val_stops_1_loss: 1.9675 - val_starts_0_accuracy: 0.4602 - val_stops_0_accuracy: 0.4756 - val_starts_1_accuracy: 0.4531 - val_stops_1_accuracy: 0.4863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 198s 18ms/sample - loss: 7.0790 - starts_0_loss: 1.6787 - stops_0_loss: 1.7309 - starts_1_loss: 1.7401 - stops_1_loss: 1.9293 - starts_0_accuracy: 0.4747 - stops_0_accuracy: 0.4880 - starts_1_accuracy: 0.4690 - stops_1_accuracy: 0.4852 - val_loss: 6.9422 - val_starts_0_loss: 1.5864 - val_stops_0_loss: 1.6639 - val_starts_1_loss: 1.7265 - val_stops_1_loss: 1.9683 - val_starts_0_accuracy: 0.4606 - val_stops_0_accuracy: 0.4770 - val_starts_1_accuracy: 0.4527 - val_stops_1_accuracy: 0.4859\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 199s 18ms/sample - loss: 7.1065 - starts_0_loss: 1.6834 - stops_0_loss: 1.7428 - starts_1_loss: 1.7413 - stops_1_loss: 1.9380 - starts_0_accuracy: 0.4728 - stops_0_accuracy: 0.4856 - starts_1_accuracy: 0.4633 - stops_1_accuracy: 0.4856 - val_loss: 6.9340 - val_starts_0_loss: 1.5850 - val_stops_0_loss: 1.6631 - val_starts_1_loss: 1.7231 - val_stops_1_loss: 1.9661 - val_starts_0_accuracy: 0.4631 - val_stops_0_accuracy: 0.4752 - val_starts_1_accuracy: 0.4563 - val_stops_1_accuracy: 0.4866\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "11212/11212 [==============================] - 349s 31ms/sample - loss: 7.0077 - starts_0_loss: 1.6492 - stops_0_loss: 1.7236 - starts_1_loss: 1.7127 - stops_1_loss: 1.9220 - starts_0_accuracy: 0.4867 - stops_0_accuracy: 0.4852 - starts_1_accuracy: 0.4779 - stops_1_accuracy: 0.4872 - val_loss: 6.7532 - val_starts_0_loss: 1.5386 - val_stops_0_loss: 1.6097 - val_starts_1_loss: 1.6889 - val_stops_1_loss: 1.9184 - val_starts_0_accuracy: 0.4788 - val_stops_0_accuracy: 0.4866 - val_starts_1_accuracy: 0.4620 - val_stops_1_accuracy: 0.4973\n",
      "Epoch 2/2\n",
      "11212/11212 [==============================] - 327s 29ms/sample - loss: 6.7815 - starts_0_loss: 1.6078 - stops_0_loss: 1.6512 - starts_1_loss: 1.6698 - stops_1_loss: 1.8527 - starts_0_accuracy: 0.4928 - stops_0_accuracy: 0.5009 - starts_1_accuracy: 0.4903 - stops_1_accuracy: 0.5066 - val_loss: 6.6422 - val_starts_0_loss: 1.5106 - val_stops_0_loss: 1.5807 - val_starts_1_loss: 1.6620 - val_stops_1_loss: 1.8906 - val_starts_0_accuracy: 0.4870 - val_stops_0_accuracy: 0.4948 - val_starts_1_accuracy: 0.4691 - val_stops_1_accuracy: 0.5023\n",
      "[INFO]  =============== Validation for FOLD# 4 ===============\n",
      "[INFO] 57.64 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 57.86 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 48.70 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 49.48 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 63.54 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 62.51 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 31.72 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 41.80 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 65.89 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 59.32 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 34.10 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 40.29 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 62.42 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 67.48 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 31.50 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 45.24 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t||"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V21_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (11212, 76) (11212, 76)\n",
      "[INFO] Prediction shape for validation data:  (2803, 76) (2803, 76)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  8994 out of 11212\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  2268 out of 2803\n",
      "[INFO] Training Jaccard Score:  0.29211980717447567\n",
      "[INFO] Validation Jaccard Score:  0.3031659745239013\n",
      "[INFO] Training for fold: 4 finished at Sun May 17 09:04:11 2020\n",
      "Sun May 17 09:04:11 2020\n"
     ]
    }
   ],
   "source": [
    "for num, (t_index, v_index) in enumerate(sss.split(X, Y_stops)):\n",
    "    print(\"[INFO] ==================== FOLD#\", num, \"====================\")\n",
    "    \n",
    "    if num > 0:\n",
    "        del history\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    \n",
    "    model = build_model()\n",
    "    \n",
    "    mcp = ModelCheckpoint(filepath=\"../results/\"+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\", monitor='val_loss',\n",
    "                          verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "    \n",
    "    csvl = CSVLogger(filename=\"../results/\"+MODEL_PREFIX+\"_LossLogs_\"+str(num)+\".csv\",\n",
    "                     separator=\",\",\n",
    "                     append=True)\n",
    "    \n",
    "    print(\"[INFO] Training only the final layers at higher learning rates.\")\n",
    "    model.layers[3].trainable = False\n",
    "    adam = Adam(learning_rate=MAX_LR)\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'],\n",
    "                  loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    history = model.fit(x={\"att_flags\":X_att[t_index],\n",
    "                           \"words\":X[t_index],\n",
    "                           \"token_ids\":np.zeros_like(X_att[t_index])},\n",
    "                        y={\"starts_0\":Y_starts[t_index],\n",
    "                           \"stops_0\":Y_stops[t_index], \n",
    "                           \"starts_1\":Y_starts[t_index],\n",
    "                           \"stops_1\":Y_stops[t_index]},\n",
    "                        shuffle=True,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS[0],\n",
    "                        validation_data=({\"att_flags\":X_att[v_index],\n",
    "                                          \"words\":X[v_index],\n",
    "                                          \"token_ids\":np.zeros_like(X_att[v_index])},\n",
    "                                         {\"starts_0\":Y_starts[v_index],\n",
    "                                          \"stops_0\":Y_stops[v_index], \n",
    "                                          \"starts_1\":Y_starts[v_index],\n",
    "                                          \"stops_1\":Y_stops[v_index]}),\n",
    "                        verbose=1,\n",
    "                        callbacks=[mcp, csvl])\n",
    "    \n",
    "    print(\"[INFO] Training only the final layers at lower learning rates.\")\n",
    "    adam = Adam(learning_rate=MID_LR)\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'],\n",
    "                  loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    history = model.fit(x={\"att_flags\":X_att[t_index],\n",
    "                           \"words\":X[t_index],\n",
    "                           \"token_ids\":np.zeros_like(X_att[t_index])},\n",
    "                        y={\"starts_0\":Y_starts[t_index],\n",
    "                           \"stops_0\":Y_stops[t_index], \n",
    "                           \"starts_1\":Y_starts[t_index],\n",
    "                           \"stops_1\":Y_stops[t_index]},\n",
    "                        shuffle=True,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS[1],\n",
    "                        validation_data=({\"att_flags\":X_att[v_index],\n",
    "                                          \"words\":X[v_index],\n",
    "                                          \"token_ids\":np.zeros_like(X_att[v_index])},\n",
    "                                         {\"starts_0\":Y_starts[v_index],\n",
    "                                          \"stops_0\":Y_stops[v_index], \n",
    "                                          \"starts_1\":Y_starts[v_index],\n",
    "                                          \"stops_1\":Y_stops[v_index]}),\n",
    "                        verbose=1,\n",
    "                        callbacks=[mcp, csvl])\n",
    "    \n",
    "    print(\"[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\")\n",
    "    model.layers[3].trainable = True\n",
    "    adam = Adam(learning_rate=MIN_LR)\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'],\n",
    "                  loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    history = model.fit(x={\"att_flags\":X_att[t_index],\n",
    "                           \"words\":X[t_index],\n",
    "                           \"token_ids\":np.zeros_like(X_att[t_index])},\n",
    "                        y={\"starts_0\":Y_starts[t_index],\n",
    "                           \"stops_0\":Y_stops[t_index], \n",
    "                           \"starts_1\":Y_starts[t_index],\n",
    "                           \"stops_1\":Y_stops[t_index]},\n",
    "                        shuffle=True,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS[2],\n",
    "                        validation_data=({\"att_flags\":X_att[v_index],\n",
    "                                          \"words\":X[v_index],\n",
    "                                          \"token_ids\":np.zeros_like(X_att[v_index])},\n",
    "                                         {\"starts_0\":Y_starts[v_index],\n",
    "                                          \"stops_0\":Y_stops[v_index], \n",
    "                                          \"starts_1\":Y_starts[v_index],\n",
    "                                          \"stops_1\":Y_stops[v_index]}),\n",
    "                        verbose=1,\n",
    "                        callbacks=[mcp, csvl])\n",
    "    \n",
    "    #model = build_model()\n",
    "    #model.load_weights(\"../results/\"+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\")  \n",
    "    \n",
    "    pred_train = model.predict(x = {\"att_flags\":X_att[t_index],\n",
    "                                    \"words\":X[t_index],\n",
    "                                    \"token_ids\":np.zeros_like(X_att[t_index])},\n",
    "                               batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "    pred_val = model.predict(x = {\"att_flags\":X_att[v_index],\n",
    "                                  \"words\":X[v_index],\n",
    "                                  \"token_ids\":np.zeros_like(X_att[v_index])},\n",
    "                             batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    pred_starts_train, pred_stops_train = (pred_train[0]/2.0 + pred_train[2]/2.0), (pred_train[1]/2.0 + pred_train[3]/2.0)\n",
    "    pred_starts_val, pred_stops_val = (pred_val[0]/2.0 + pred_val[2]/2.0), (pred_val[1]/2.0 + pred_val[3]/2.0)\n",
    "    \n",
    "    # Accumulate test results after training every fold\n",
    "    pred_test_fold = model.predict(x = {\"att_flags\":X_att_test,\n",
    "                                        \"words\":X_test,\n",
    "                                        \"token_ids\":np.zeros_like(X_att_test)},\n",
    "                                   batch_size=PREDICT_BATCH_SIZE)\n",
    "    if num==0:\n",
    "        pred_test = []\n",
    "        pred_test.append(pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test.append(pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    else:\n",
    "        pred_test[0] += (pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test[1] += (pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    \n",
    "    # Tabulate\n",
    "    preds = {\n",
    "        \"train\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_starts[t_index],\n",
    "                \"y_pred\":pred_train[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_stops[t_index],\n",
    "                \"y_pred\":pred_train[1].argmax(axis=1)\n",
    "            }\n",
    "        },\n",
    "        \"valid\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_starts[v_index],\n",
    "                \"y_pred\":pred_val[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_stops[v_index],\n",
    "                \"y_pred\":pred_val[1].argmax(axis=1)\n",
    "            }        \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print_metrics(pred_dict=preds)\n",
    "\n",
    "    print(\"[INFO] Prediction shape for training data: \", pred_starts_train.shape, pred_stops_train.shape)\n",
    "    print(\"[INFO] Prediction shape for validation data: \", pred_starts_val.shape, pred_stops_val.shape)\n",
    "\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for training data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_train.argmax(axis=1),\n",
    "                                  pred_stops_train.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_train.shape[0])\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for validation data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_val.argmax(axis=1),\n",
    "                                  pred_stops_val.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_val.shape[0],)\n",
    "\n",
    "    pred_words_train = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X[t_index],\n",
    "                                                                                                                                   pred_starts_train.argmax(axis=1),\n",
    "                                                                                                                                   pred_stops_train.argmax(axis=1))]\n",
    "    \n",
    "    pred_words_val = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X[v_index],\n",
    "                                                                                                                                 pred_starts_val.argmax(axis=1),\n",
    "                                                                                                                                 pred_stops_val.argmax(axis=1))]\n",
    "\n",
    "    print(\"[INFO] Training Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_words) if n in t_index],\n",
    "                                                          pred_words_train)]))\n",
    "    print(\"[INFO] Validation Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_words) if n in v_index],\n",
    "                                                          pred_words_val)]))\n",
    "    print(\"[INFO] Training for fold:\", num, \"finished at\", ctime(time()))\n",
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Prediction shape for testing data:  (3534, 76) (3534, 76)\n"
     ]
    }
   ],
   "source": [
    "pred_starts_test, pred_stops_test = pred_test[0]/NUM_FOLDS, pred_test[1]/NUM_FOLDS\n",
    "print(\"[INFO] Prediction shape for testing data: \", pred_starts_test.shape, pred_stops_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal predictions (StartIndex less than EndIndex) for testing data: 2988 out of 3534\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal predictions (StartIndex less than EndIndex) for testing data:\",\n",
    "      sum([s<e for s,e in zip(pred_starts_test.argmax(axis=1),\n",
    "                              pred_stops_test.argmax(axis=1))]), \n",
    "      \"out of\",\n",
    "      pred_starts_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words_test = [\n",
    "    post_process(tokenizer.decode(t[s:e+1])) if s<e else post_process(tokenizer.decode(t[e:])) for t,s,e in zip(X_test,\n",
    "                                                                                                              pred_starts_test.argmax(axis=1),\n",
    "                                                                                                              pred_stops_test.argmax(axis=1))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " movie then sleep! today was good day  [{h!--d3ff}]  positive \n",
      "7\n",
      "8\n",
      "good day\n"
     ]
    }
   ],
   "source": [
    "check_idx = 753\n",
    "#print([[t,i,j,k] for t,i,j,k in zip(tokenizer.decode(),X_test[check_idx],pred_starts_test[check_idx],pred_stops_test[check_idx])])\n",
    "print(tokenizer.decode(X_test[check_idx]))\n",
    "print(pred_starts_test.argmax(axis=1)[check_idx])\n",
    "print(pred_stops_test.argmax(axis=1)[check_idx])\n",
    "print(post_process(tokenizer.decode(X_test[check_idx][pred_starts_test.argmax(axis=1)[check_idx]:1+pred_stops_test.argmax(axis=1)[check_idx]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['selected_text'] = pred_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"selected_text\"] = np.where(test_df[\"sentiment\"] == \"neutral\", test_df[\"text\"], test_df[\"selected_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>i make them good girls go bad</td>\n",
       "      <td>negative</td>\n",
       "      <td>i make them good girls go bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>the problem is, superstars always deliver huge files! too much energy pixels</td>\n",
       "      <td>negative</td>\n",
       "      <td>the problem is, superstars always deliver huge files! too much energy pixels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>did you tweet me? i thought i remembered seeing one but i cant see it now! SO glad teatree is ok!</td>\n",
       "      <td>positive</td>\n",
       "      <td>glad teatree is ok!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>Namaste! Hooray for Monday! Undaunted by a 3am wakeup, I consider it is 9am in London and feel justified. Bright side to everything!</td>\n",
       "      <td>positive</td>\n",
       "      <td>justified. bright side to everything!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>Proud 17-year-old here that loves The Carpenters!!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>loves the carpenters!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3315</th>\n",
       "      <td>thank u sweetie! Can`t wait to set sail with u next week</td>\n",
       "      <td>positive</td>\n",
       "      <td>thank u sweetie!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>heyy i have a sis of 1 year with 5 months and still she don t want to walk..</td>\n",
       "      <td>negative</td>\n",
       "      <td>still she don t want to walk..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3504</th>\n",
       "      <td>so what was said??  I`m so bummed I missed it!.  *sigh* I want to go to california</td>\n",
       "      <td>negative</td>\n",
       "      <td>bummed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>that is a bummer</td>\n",
       "      <td>negative</td>\n",
       "      <td>that is a bummer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>forcing myself to watch the movie 1984 for my book report  im lazy to read</td>\n",
       "      <td>negative</td>\n",
       "      <td>im lazy to read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Im Slowing on My Tweets..Cuase I Lost My Phone</td>\n",
       "      <td>negative</td>\n",
       "      <td>lost my phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Download movie  'A Good Day to Be Black &amp;#38 Sexy' http://tinyurl.com/ckmkul cool #movie</td>\n",
       "      <td>positive</td>\n",
       "      <td>good day to be black &amp;#38 sexy' http://tinyurl.com/ckmkul cool #movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>Great exercise for a Sunday morning: 'stopping the wheel of karma' (R.A.W.) and transforming negativity into love... Life!</td>\n",
       "      <td>positive</td>\n",
       "      <td>great exercise for a sunday morning: 'stopping the wheel of karma' (r.a.w.) and transforming negativity into love... life!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>Awww, that wasn`t very nice.</td>\n",
       "      <td>negative</td>\n",
       "      <td>awww, that wasn`t very nice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>watching men n blk 2, wishin all the mums happy mother`s day</td>\n",
       "      <td>positive</td>\n",
       "      <td>wishin all the mums happy mother`s day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                      text  \\\n",
       "643                                                                                                          i make them good girls go bad   \n",
       "3252                                                          the problem is, superstars always deliver huge files! too much energy pixels   \n",
       "967                                      did you tweet me? i thought i remembered seeing one but i cant see it now! SO glad teatree is ok!   \n",
       "2008  Namaste! Hooray for Monday! Undaunted by a 3am wakeup, I consider it is 9am in London and feel justified. Bright side to everything!   \n",
       "1610                                                                                   Proud 17-year-old here that loves The Carpenters!!!   \n",
       "3315                                                                              thank u sweetie! Can`t wait to set sail with u next week   \n",
       "2966                                                          heyy i have a sis of 1 year with 5 months and still she don t want to walk..   \n",
       "3504                                                    so what was said??  I`m so bummed I missed it!.  *sigh* I want to go to california   \n",
       "215                                                                                                                       that is a bummer   \n",
       "1134                                                            forcing myself to watch the movie 1984 for my book report  im lazy to read   \n",
       "141                                                                                         Im Slowing on My Tweets..Cuase I Lost My Phone   \n",
       "320                                               Download movie  'A Good Day to Be Black &#38 Sexy' http://tinyurl.com/ckmkul cool #movie   \n",
       "1451            Great exercise for a Sunday morning: 'stopping the wheel of karma' (R.A.W.) and transforming negativity into love... Life!   \n",
       "1489                                                                                                          Awww, that wasn`t very nice.   \n",
       "2522                                                                          watching men n blk 2, wishin all the mums happy mother`s day   \n",
       "\n",
       "     sentiment  \\\n",
       "643   negative   \n",
       "3252  negative   \n",
       "967   positive   \n",
       "2008  positive   \n",
       "1610  positive   \n",
       "3315  positive   \n",
       "2966  negative   \n",
       "3504  negative   \n",
       "215   negative   \n",
       "1134  negative   \n",
       "141   negative   \n",
       "320   positive   \n",
       "1451  positive   \n",
       "1489  negative   \n",
       "2522  positive   \n",
       "\n",
       "                                                                                                                   selected_text  \n",
       "643                                                                                                i make them good girls go bad  \n",
       "3252                                                the problem is, superstars always deliver huge files! too much energy pixels  \n",
       "967                                                                                                          glad teatree is ok!  \n",
       "2008                                                                                       justified. bright side to everything!  \n",
       "1610                                                                                                     loves the carpenters!!!  \n",
       "3315                                                                                                            thank u sweetie!  \n",
       "2966                                                                                              still she don t want to walk..  \n",
       "3504                                                                                                                      bummed  \n",
       "215                                                                                                             that is a bummer  \n",
       "1134                                                                                                             im lazy to read  \n",
       "141                                                                                                                lost my phone  \n",
       "320                                                        good day to be black &#38 sexy' http://tinyurl.com/ckmkul cool #movie  \n",
       "1451  great exercise for a sunday morning: 'stopping the wheel of karma' (r.a.w.) and transforming negativity into love... life!  \n",
       "1489                                                                                                awww, that wasn`t very nice.  \n",
       "2522                                                                                      wishin all the mums happy mother`s day  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[test_df.sentiment!=\"neutral\"][[\"text\", \"sentiment\",\"selected_text\"]].sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[[\"textID\", \"selected_text\"]].to_csv(\"../results/submission_\"+MODEL_PREFIX+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 17 09:04:11 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
