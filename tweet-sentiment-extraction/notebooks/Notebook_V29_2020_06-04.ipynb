{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V29\"\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 16\n",
    "PREDICT_BATCH_SIZE = 512\n",
    "DROPOUT = 0.3\n",
    "\n",
    "RUN_ON_SAMPLE = True\n",
    "EXCLUDE_NEUTRAL_CLASS = True\n",
    "SENTIMENT_MAX_LR = 5e-4\n",
    "SENTIMENT_MIN_LR = 5e-6\n",
    "SENTIMENT_NUM_EPOCHS = [3, 3]\n",
    "MAX_LR = 5e-3\n",
    "MID_LR = 1e-4\n",
    "MIN_LR = 1e-6\n",
    "NUM_EPOCHS = [3, 3, 3]\n",
    "NUM_FOLDS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"../results/\"\n",
    "DATA_DIR = \"../data/\"\n",
    "MODEL_DIR = \"../data/models/\"\n",
    "EXT_MODEL_DIR = \"../data/models/deotte/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, Layer, LeakyReLU, Subtract\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    a = set(str1)\n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeded_value = 88888\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun  4 20:48:58 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0', '2.8.0', '0.5.2']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\"  target=\"_blank\"><h2 id=\"limiting_gpu_memory_growth\" data-text=\"Limiting GPU memory growth\" tabindex=\"0\">Limiting GPU memory growth</h2></a>\n",
    "<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\"><code translate=\"no\" dir=\"ltr\">CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the <code translate=\"no\" dir=\"ltr\">tf.config.experimental.set_visible_devices</code> method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it okay to exclude neutral text from models and force it later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVERAGE: 0.5334592552617378\n",
      "MEAN JACCARD: 0.9808316180492903\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>c68863089e</td>\n",
       "      <td>_tifullyTragic ... in London last night so give it a few hours &amp; I`m sure a few will appear</td>\n",
       "      <td>in London last night so give it a few hours &amp; I`m sure a few will appear</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2211</th>\n",
       "      <td>edd2aceb1c</td>\n",
       "      <td>who sings `I Remember`? i alwaysss hear it on Radio 1 but never catch the artist</td>\n",
       "      <td>who sings `I Remember`? i alwaysss hear it on Radio 1 but never catch the artist</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8286</th>\n",
       "      <td>ee53bf0e43</td>\n",
       "      <td>this time there is a theme and it is 'purple'</td>\n",
       "      <td>this time there is a theme and it is 'purple'</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4240</th>\n",
       "      <td>2736a522fa</td>\n",
       "      <td>why isnt everyone with you?</td>\n",
       "      <td>why isnt everyone with you?</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10069</th>\n",
       "      <td>8ba7a1720e</td>\n",
       "      <td>and by the way it`s short stack, not sway sway  sway sway baby is a song of theirs</td>\n",
       "      <td>and by the way it`s short stack, not sway sway  sway sway baby is a song of theirs</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID  \\\n",
       "4896   c68863089e   \n",
       "2211   edd2aceb1c   \n",
       "8286   ee53bf0e43   \n",
       "4240   2736a522fa   \n",
       "10069  8ba7a1720e   \n",
       "\n",
       "                                                                                              text  \\\n",
       "4896   _tifullyTragic ... in London last night so give it a few hours & I`m sure a few will appear   \n",
       "2211              who sings `I Remember`? i alwaysss hear it on Radio 1 but never catch the artist   \n",
       "8286                                                 this time there is a theme and it is 'purple'   \n",
       "4240                                                                   why isnt everyone with you?   \n",
       "10069           and by the way it`s short stack, not sway sway  sway sway baby is a song of theirs   \n",
       "\n",
       "                                                                            selected_text  \\\n",
       "4896             in London last night so give it a few hours & I`m sure a few will appear   \n",
       "2211     who sings `I Remember`? i alwaysss hear it on Radio 1 but never catch the artist   \n",
       "8286                                        this time there is a theme and it is 'purple'   \n",
       "4240                                                          why isnt everyone with you?   \n",
       "10069  and by the way it`s short stack, not sway sway  sway sway baby is a song of theirs   \n",
       "\n",
       "      sentiment  \n",
       "4896    neutral  \n",
       "2211    neutral  \n",
       "8286    neutral  \n",
       "4240    neutral  \n",
       "10069   neutral  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\")\n",
    "# cases where neutral's text and selected text columns are the same\n",
    "neutrals = df2.loc[df2.sentiment==\"neutral\"].copy()\n",
    "neutrals = neutrals.reset_index(drop=True)\n",
    "neutrals[\"text\"] = neutrals[\"text\"].astype(str)\n",
    "neutrals[\"selected_text\"] = neutrals[\"selected_text\"].astype(str)\n",
    "print(\"COVERAGE:\", np.sum(np.where((neutrals.text == neutrals.selected_text), 1, 0))/neutrals.shape[0])\n",
    "print(\"MEAN JACCARD:\", np.mean([jaccard(str1=i, str2=j) for i,j in zip(neutrals.selected_text,neutrals.text)]))\n",
    "\n",
    "neutrals.loc[neutrals.text != neutrals.selected_text].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  1\n",
      "selected_text  object  1\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27480, 'selected_text': 22463, 'sentiment': 3}\n",
      "            textID                                                text  \\\n",
      "count        27481                                               27480   \n",
      "unique       27481                                               27480   \n",
      "top     e2f90b07df   F...you very very much  willkommen im Twitterland   \n",
      "freq             1                                                   1   \n",
      "\n",
      "       selected_text sentiment  \n",
      "count          27480     27481  \n",
      "unique         22463         3  \n",
      "top             good   neutral  \n",
      "freq             199     11118  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\")\n",
    "\n",
    "print(pd.concat((df.dtypes, df.isna().sum()), axis=1))\n",
    "print(df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df[i].nunique() for i in df.columns})\n",
    "print(df.describe())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count         3534   \n",
      "unique        3534   \n",
      "top     245e5f25e2   \n",
      "freq             1   \n",
      "\n",
      "                                                                                                                                             text  \\\n",
      "count                                                                                                                                        3534   \n",
      "unique                                                                                                                                       3534   \n",
      "top     WTF? Advertising gone mad, they want access to my  camera and microphone? What for? Amazon should really know better  http://bit.ly/5FoKQ   \n",
      "freq                                                                                                                                            1   \n",
      "\n",
      "       sentiment  \n",
      "count       3534  \n",
      "unique         3  \n",
      "top      neutral  \n",
      "freq        1430  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(DATA_DIR+\"test.csv\", encoding=\"utf8\")\n",
    "print(pd.concat((test_df.dtypes, test_df.isna().sum()), axis=1))\n",
    "print(test_df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df[i].nunique() for i in test_df.columns})\n",
    "print(test_df.describe())\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>c77717b103</td>\n",
       "      <td>I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½</td>\n",
       "      <td>I love to!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>28dbada620</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "44   c77717b103   \n",
       "192  28dbada620   \n",
       "\n",
       "                                                                                                       text  \\\n",
       "44    I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½   \n",
       "192                                *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "                                                                selected_text  \\\n",
       "44                                                                 I love to!   \n",
       "192  *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "    sentiment  \n",
       "44   positive  \n",
       "192   neutral  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>7223fdccc2</td>\n",
       "      <td>tikcets are only ï¿½91...each...BUT I SO WANT TO GO</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>43ad351369</td>\n",
       "      <td>AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "145  7223fdccc2   \n",
       "618  43ad351369   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "145                                                                                             tikcets are only ï¿½91...each...BUT I SO WANT TO GO   \n",
       "618  AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s   \n",
       "\n",
       "    sentiment  \n",
       "145  positive  \n",
       "618  positive  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[test_df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"set\"], test_df[\"set\"] = \"train\", \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create smaller sample for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_ON_SAMPLE:\n",
    "    df = df.sample(500).reset_index(drop=True)\n",
    "    test_df = test_df.sample(500).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine datasets for pretraining using sentiment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat((df[[\"text\",\"set\",\"sentiment\"]],\n",
    "                  test_df[[\"text\",\"set\",\"sentiment\"]]), axis=0)\n",
    "data[\"text\"] = data[\"text\"].astype(str)\n",
    "data = data.sample(frac=1.0).reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment count in combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">test</th>\n",
       "      <th>negative</th>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">train</th>\n",
       "      <th>negative</th>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 text\n",
       "set   sentiment      \n",
       "test  negative    140\n",
       "      neutral     196\n",
       "      positive    164\n",
       "train negative    148\n",
       "      neutral     195\n",
       "      positive    157"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"set\",\"sentiment\"])[[\"text\"]].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file=EXT_MODEL_DIR + 'vocab-roberta-base.json',\n",
    "                                             merges_file=EXT_MODEL_DIR + 'merges-roberta-base.txt',                                         \n",
    "                                             add_prefix_space=True,\n",
    "                                             lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(RESULTS_DIR+\"tokenizers/roberta_tokenizer/special_tokens_map.json\") as f:\n",
    "    special_tokens = json.load(f)\n",
    "\n",
    "tokenizer.add_special_tokens([i for i in special_tokens.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for sentiment detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lookup = {\"positive\":2,\"neutral\":1,\"negative\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': (1000, 51), 'X_att': (1000, 51), 'Y': (1000,), 'VOCAB_SIZE': 50265, 'MAX_SEQ_LEN': 51}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentiment(text_series=data.text.tolist(), sentiment_series=data.sentiment):\n",
    "\n",
    "    X_tokens = tokenizer.encode_batch(text_series)\n",
    "\n",
    "    X = [i.ids for i in X_tokens]\n",
    "    MAX_SEQ_LEN = max([len(i) for i in X])\n",
    "    X = pad_sequences(X, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "    X_att = [i.attention_mask for i in X_tokens]\n",
    "    X_att = pad_sequences(X_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "    Y = sentiment_series.apply(lambda x: sentiment_lookup[x]).values\n",
    "\n",
    "    VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "\n",
    "    print({\n",
    "        \"X\":X.shape,\n",
    "        \"X_att\":X_att.shape,\n",
    "        \"Y\":Y.shape,\n",
    "        \"VOCAB_SIZE\":VOCAB_SIZE,\n",
    "        \"MAX_SEQ_LEN\":MAX_SEQ_LEN\n",
    "    })\n",
    "    \n",
    "    return X_tokens, X, X_att, Y, VOCAB_SIZE, MAX_SEQ_LEN\n",
    "\n",
    "X_sent_tokens, X_sent, X_sent_att, Y_sent, VOCAB_SIZE, MAX_SEQ_LEN_SENT = preprocess_sentiment(**{\n",
    "    \"text_series\" : data.text.tolist(),\n",
    "    \"sentiment_series\" : data.sentiment\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  1\n",
      "selected_text  object  1\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27480, 'selected_text': 22463, 'sentiment': 3}\n",
      "            textID                                                text  \\\n",
      "count        27481                                               27480   \n",
      "unique       27481                                               27480   \n",
      "top     e2f90b07df   F...you very very much  willkommen im Twitterland   \n",
      "freq             1                                                   1   \n",
      "\n",
      "       selected_text sentiment  \n",
      "count          27480     27481  \n",
      "unique         22463         3  \n",
      "top             good   neutral  \n",
      "freq             199     11118  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\")\n",
    "\n",
    "print(pd.concat((df_span.dtypes, df_span.isna().sum()), axis=1))\n",
    "print(df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df_span[i].nunique() for i in df_span.columns})\n",
    "print(df_span.describe())\n",
    "df_span.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count         3534   \n",
      "unique        3534   \n",
      "top     245e5f25e2   \n",
      "freq             1   \n",
      "\n",
      "                                                                                                                                             text  \\\n",
      "count                                                                                                                                        3534   \n",
      "unique                                                                                                                                       3534   \n",
      "top     WTF? Advertising gone mad, they want access to my  camera and microphone? What for? Amazon should really know better  http://bit.ly/5FoKQ   \n",
      "freq                                                                                                                                            1   \n",
      "\n",
      "       sentiment  \n",
      "count       3534  \n",
      "unique         3  \n",
      "top      neutral  \n",
      "freq        1430  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_span = pd.read_csv(DATA_DIR+\"test.csv\")\n",
    "print(pd.concat((test_df_span.dtypes, test_df_span.isna().sum()), axis=1))\n",
    "print(test_df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df_span[i].nunique() for i in test_df_span.columns})\n",
    "print(test_df_span.describe())\n",
    "test_df_span.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for span detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For traceability\n",
    "df_span[\"original_index\"] = df_span.index\n",
    "test_df_span[\"original_index\"] = test_df_span.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_idxs = [513, 1060, 5543, 8410, 13605, 24778, 25258] + [18, 27, 32, 39, 48, 49, 64, 66, 84, 92, 102, 116, 129, 132, 134, 149, 160, 166, 168, 189, 196, 197, 210, 223, 247, 251, 260, 285, 295, 296, 297, 309, 349, 362, 366, 382, 396, 398, 406, 410, 425, 458, 460, 482, 492, 500, 504, 517, 533, 537, 543, 569, 573, 581, 604, 605, 608, 634, 636, 637, 639, 642, 665, 670, 674, 678, 685, 702, 707, 709, 710, 727, 737, 746, 751, 757, 768, 778, 787, 794, 804, 809, 853, 854, 856, 863, 871, 872, 912, 931, 937, 942, 950, 956, 957, 963, 992, 993, 999, 1001, 1011, 1012, 1036, 1039, 1057, 1062, 1076, 1077, 1083, 1096, 1105, 1116, 1117, 1122, 1134, 1137, 1150, 1159, 1199, 1200, 1209, 1217, 1264, 1271, 1280, 1283, 1298, 1300, 1303, 1319, 1327, 1342, 1360, 1363, 1365, 1372, 1374, 1376, 1382, 1393, 1416, 1417, 1420, 1447, 1454, 1468, 1472, 1513, 1515, 1518, 1531, 1548, 1558, 1567, 1580, 1588, 1605, 1623, 1648, 1649, 1657, 1659, 1661, 1665, 1672, 1674, 1678, 1683, 1693, 1696, 1713, 1723, 1728, 1739, 1747, 1754, 1798, 1806, 1808, 1814, 1822, 1828, 1830, 1835, 1850, 1857, 1860, 1866, 1888, 1901, 1902, 1929, 1938, 1940, 1987, 1994, 2003, 2004, 2008, 2011, 2023, 2054, 2056, 2065, 2068, 2083, 2086, 2094, 2106, 2113, 2124, 2125, 2136, 2139, 2145, 2168, 2180, 2181, 2187, 2213, 2225, 2227, 2230, 2239, 2273, 2279, 2286, 2288, 2298, 2323, 2324, 2335, 2343, 2359, 2364, 2371, 2385, 2388, 2392, 2398, 2401, 2404, 2410, 2415, 2421, 2430, 2438, 2439, 2440, 2444, 2445, 2448, 2470, 2478, 2489, 2499, 2505, 2515, 2520, 2559, 2574, 2576, 2589, 2598, 2600, 2640, 2643, 2650, 2671, 2679, 2696, 2699, 2706, 2716, 2718, 2731, 2743, 2749, 2750, 2785, 2787, 2789, 2805, 2809, 2825, 2838, 2856, 2888, 2898, 2903, 2923, 2925, 2931, 2933, 2942, 2957, 2961, 2986, 2995, 3034, 3068, 3075, 3077, 3080, 3088, 3089, 3096, 3097, 3101, 3121, 3147, 3160, 3161, 3176, 3182, 3187, 3211, 3225, 3234, 3257, 3263, 3267, 3281, 3290, 3295, 3317, 3319, 3325, 3333, 3359, 3364, 3369, 3374, 3390, 3399, 3400, 3411, 3424, 3473, 3475, 3476, 3479, 3492, 3499, 3504, 3509, 3517, 3519, 3532, 3553, 3578, 3588, 3590, 3593, 3598, 3605, 3606, 3607, 3622, 3634, 3669, 3672, 3678, 3682, 3685, 3694, 3708, 3720, 3729, 3750, 3755, 3784, 3794, 3798, 3801, 3806, 3807, 3813, 3826, 3834, 3872, 3886, 3895, 3903, 3925, 3937, 3946, 3962, 3969, 3998, 4031, 4047, 4053, 4074, 4091, 4093, 4100, 4108, 4113, 4119, 4143, 4148, 4163, 4164, 4183, 4196, 4254, 4260, 4265, 4307, 4308, 4309, 4312, 4318, 4325, 4327, 4328, 4329, 4344, 4347, 4369, 4371, 4377, 4384, 4402, 4404, 4405, 4407, 4412, 4421, 4422, 4437, 4455, 4457, 4471, 4473, 4479, 4483, 4486, 4514, 4532, 4537, 4542, 4543, 4545, 4550, 4554, 4563, 4575, 4576, 4591, 4595, 4631, 4636, 4637, 4639, 4654, 4655, 4704, 4706, 4714, 4718, 4724, 4729, 4735, 4747, 4749, 4750, 4764, 4800, 4814, 4841, 4842, 4844, 4864, 4866, 4869, 4875, 4891, 4908, 4918, 4924, 4928, 4933, 4943, 4946, 4947, 4956, 4968, 4980, 4994, 5009, 5011, 5038, 5056, 5079, 5083, 5101, 5133, 5167, 5173, 5174, 5188, 5189, 5196, 5197, 5213, 5218, 5229, 5241, 5245, 5258, 5279, 5290, 5294, 5297, 5308, 5317, 5331, 5342, 5343, 5353, 5355, 5356, 5358, 5379, 5385, 5386, 5399, 5418, 5433, 5436, 5450, 5456, 5466, 5489, 5504, 5509, 5510, 5530, 5542, 5560, 5583, 5603, 5607, 5618, 5620, 5624, 5626, 5678, 5697, 5712, 5725, 5751, 5757, 5775, 5785, 5799, 5822, 5832, 5836, 5844, 5895, 5902, 5904, 5913, 5918, 5938, 5939, 5961, 5984, 5990, 6025, 6031, 6033, 6060, 6077, 6084, 6090, 6113, 6131, 6134, 6151, 6152, 6176, 6203, 6229, 6230, 6248, 6261, 6277, 6289, 6296, 6304, 6308, 6317, 6319, 6321, 6335, 6338, 6343, 6353, 6356, 6360, 6377, 6382, 6395, 6404, 6425, 6433, 6468, 6474, 6475, 6476, 6477, 6510, 6528, 6540, 6574, 6580, 6582, 6593, 6609, 6627, 6628, 6633, 6635, 6650, 6656, 6657, 6670, 6673, 6676, 6677, 6679, 6686, 6702, 6724, 6740, 6745, 6759, 6801, 6804, 6830, 6833, 6834, 6842, 6859, 6871, 6875, 6878, 6880, 6885, 6896, 6934, 6939, 6948, 6950, 6958, 6970, 6979, 6987, 7010, 7017, 7024, 7040, 7099, 7107, 7112, 7136, 7147, 7149, 7158, 7184, 7225, 7227, 7251, 7274, 7288, 7305, 7306, 7308, 7331, 7344, 7355, 7373, 7407, 7408, 7409, 7410, 7423, 7436, 7438, 7442, 7444, 7450, 7469, 7473, 7479, 7487, 7492, 7506, 7513, 7521, 7528, 7530, 7546, 7567, 7581, 7582, 7597, 7599, 7602, 7632, 7635, 7636, 7642, 7662, 7663, 7701, 7752, 7758, 7765, 7769, 7778, 7780, 7788, 7801, 7816, 7818, 7831, 7836, 7837, 7849, 7893, 7895, 7896, 7922, 7927, 7945, 7948, 7968, 7997, 8003, 8005, 8010, 8030, 8049, 8051, 8052, 8059, 8060, 8085, 8094, 8142, 8143, 8151, 8153, 8161, 8169, 8170, 8196, 8208, 8212, 8215, 8224, 8235, 8241, 8249, 8250, 8276, 8282, 8285, 8286, 8288, 8289, 8299, 8314, 8338, 8343, 8345, 8349, 8373, 8379, 8390, 8397, 8399, 8407, 8427, 8428, 8432, 8439, 8442, 8444, 8484, 8493, 8497, 8500, 8507, 8510, 8534, 8546, 8559, 8569, 8579, 8588, 8594, 8604, 8617, 8620, 8624, 8631, 8638, 8661, 8671, 8674, 8687, 8691, 8696, 8706, 8720, 8729, 8749, 8756, 8758, 8769, 8774, 8780, 8800, 8803, 8809, 8815, 8827, 8830, 8875, 8902, 8906, 8910, 8917, 8919, 8922, 8951, 8954, 8968, 8985, 8986, 8991, 8999, 9014, 9034, 9050, 9057, 9063, 9070, 9088, 9098, 9112, 9113, 9122, 9127, 9140, 9155, 9157, 9169, 9179, 9190, 9216, 9223, 9226, 9264, 9272, 9281, 9297, 9300, 9303, 9322, 9354, 9361, 9374, 9377, 9381, 9385, 9400, 9403, 9419, 9422, 9425, 9428, 9436, 9442, 9449, 9454, 9456, 9458, 9470, 9482, 9496, 9500, 9535, 9539, 9541, 9554, 9556, 9560, 9565, 9574, 9594, 9605, 9610, 9631, 9632, 9660, 9689, 9691, 9696, 9708, 9710, 9712, 9725, 9726, 9734, 9742, 9753, 9770, 9789, 9799, 9801, 9812, 9817, 9836, 9839, 9850, 9852, 9881, 9882, 9891, 9906, 9912, 9920, 9922, 9928, 9976, 9980, 9984, 10002, 10004, 10007, 10029, 10039, 10042, 10050, 10052, 10057, 10067, 10070, 10092, 10093, 10123, 10131, 10138, 10145, 10151, 10164, 10167, 10170, 10179, 10184, 10185, 10190, 10197, 10200, 10211, 10218, 10219, 10222, 10228, 10245, 10247, 10255, 10259, 10276, 10293, 10298, 10299, 10306, 10319, 10327, 10342, 10348, 10377, 10379, 10449, 10465, 10469, 10492, 10504, 10505, 10508, 10521, 10528, 10529, 10530, 10570, 10573, 10577, 10578, 10597, 10611, 10639, 10651, 10666, 10672, 10675, 10690, 10706, 10711, 10738, 10742, 10747, 10749, 10779, 10782, 10783, 10795, 10813, 10849, 10852, 10853, 10884, 10885, 10888, 10892, 10901, 10914, 10918, 10919, 10933, 10935, 10956, 10966, 10968, 10970, 10981, 10986, 10996, 10999, 11005, 11032, 11038, 11051, 11055, 11060, 11067, 11097, 11102, 11155, 11157, 11159, 11162, 11167, 11176, 11187, 11215, 11222, 11228, 11230, 11257, 11263, 11265, 11274, 11286, 11289, 11297, 11300, 11308, 11323, 11337, 11339, 11341, 11347, 11349, 11353, 11371, 11381, 11386, 11426, 11431, 11448, 11449, 11474, 11480, 11500, 11507, 11553, 11562, 11582, 11588, 11594, 11612, 11616, 11618, 11636, 11643, 11650, 11658, 11666, 11679, 11683, 11693, 11695, 11698, 11699, 11706, 11716, 11723, 11730, 11732, 11741, 11745, 11780, 11794, 11798, 11808, 11812, 11828, 11829, 11837, 11840, 11841, 11844, 11849, 11853, 11854, 11855, 11861, 11862, 11872, 11875, 11878, 11886, 11890, 11899, 11905, 11912, 11938, 11963, 11967, 11973, 11980, 11985, 11999, 12026, 12027, 12029, 12039, 12054, 12067, 12115, 12124, 12134, 12136, 12138, 12150, 12187, 12205, 12206, 12224, 12242, 12258, 12269, 12277, 12283, 12295, 12305, 12314, 12322, 12331, 12333, 12334, 12350, 12355, 12356, 12360, 12370, 12372, 12389, 12395, 12396, 12397, 12405, 12410, 12416, 12429, 12439, 12440, 12442, 12446, 12474, 12482, 12486, 12507, 12516, 12521, 12522, 12526, 12527, 12537, 12550, 12563, 12576, 12585, 12586, 12587, 12598, 12606, 12630, 12631, 12635, 12639, 12650, 12657, 12662, 12669, 12687, 12703, 12718, 12724, 12732, 12736, 12745, 12767, 12787, 12803, 12804, 12808, 12818, 12830, 12843, 12855, 12856, 12870, 12884, 12903, 12914, 12920, 12923, 12924, 12927, 12936, 12972, 12977, 12998, 13003, 13004, 13009, 13044, 13068, 13069, 13074, 13080, 13095, 13098, 13124, 13130, 13153, 13154, 13176, 13219, 13222, 13227, 13237, 13267, 13274, 13293, 13301, 13304, 13307, 13330, 13333, 13354, 13365, 13373, 13379, 13381, 13389, 13397, 13422, 13430, 13433, 13445, 13466, 13473, 13476, 13482, 13490, 13493, 13528, 13529, 13535, 13543, 13558, 13559, 13586, 13637, 13643, 13646, 13665, 13674, 13678, 13679, 13704, 13707, 13710, 13718, 13727, 13733, 13747, 13762, 13772, 13785, 13796, 13803, 13812, 13814, 13817, 13827, 13844, 13846, 13847, 13848, 13854, 13855, 13861, 13864, 13873, 13899, 13907, 13936, 13939, 13946, 13949, 13954, 13964, 13965, 13974, 13975, 13978, 13980, 14002, 14006, 14007, 14030, 14043, 14044, 14057, 14058, 14060, 14073, 14091, 14109, 14110, 14112, 14117, 14131, 14132, 14159, 14172, 14173, 14176, 14184, 14194, 14197, 14200, 14204, 14207, 14213, 14230, 14257, 14273, 14275, 14287, 14299, 14301, 14307, 14308, 14347, 14362, 14365, 14381, 14386, 14400, 14416, 14428, 14436, 14446, 14457, 14458, 14462, 14487, 14499, 14509, 14532, 14548, 14551, 14571, 14596, 14598, 14611, 14621, 14639, 14640, 14648, 14662, 14669, 14671, 14676, 14680, 14687, 14689, 14691, 14718, 14731, 14747, 14779, 14787, 14825, 14839, 14844, 14847, 14855, 14870, 14880, 14891, 14908, 14946, 14959, 14960, 14967, 14971, 14986, 15007, 15010, 15016, 15044, 15049, 15055, 15056, 15112, 15131, 15158, 15165, 15174, 15177, 15196, 15203, 15206, 15207, 15213, 15217, 15233, 15236, 15253, 15259, 15286, 15296, 15307, 15308, 15321, 15323, 15327, 15339, 15363, 15372, 15374, 15379, 15380, 15383, 15385, 15392, 15410, 15449, 15452, 15462, 15464, 15465, 15477, 15483, 15503, 15505, 15528, 15532, 15548, 15562, 15566, 15571, 15605, 15610, 15622, 15651, 15660, 15666, 15674, 15709, 15721, 15731, 15735, 15736, 15741, 15767, 15774, 15785, 15792, 15793, 15811, 15851, 15879, 15883, 15884, 15912, 15926, 15927, 15931, 15940, 15959, 15965, 15985, 15998, 16020, 16027, 16031, 16032, 16044, 16070, 16100, 16117, 16129, 16136, 16137, 16140, 16149, 16152, 16175, 16186, 16201, 16241, 16261, 16269, 16272, 16279, 16290, 16308, 16319, 16324, 16325, 16327, 16338, 16341, 16345, 16352, 16354, 16371, 16372, 16385, 16391, 16416, 16423, 16448, 16455, 16468, 16470, 16477, 16481, 16484, 16491, 16493, 16500, 16509, 16510, 16516, 16541, 16559, 16577, 16589, 16632, 16638, 16642, 16643, 16649, 16650, 16654, 16665, 16677, 16684, 16704, 16720, 16726, 16734, 16765, 16774, 16775, 16777, 16780, 16797, 16798, 16799, 16803, 16807, 16809, 16830, 16832, 16861, 16863, 16868, 16876, 16889, 16890, 16902, 16915, 16927, 16939, 16949, 16962, 16968, 17002, 17010, 17013, 17021, 17023, 17029, 17062, 17066, 17084, 17093, 17094, 17102, 17152, 17160, 17167, 17173, 17183, 17221, 17239, 17250, 17267, 17277, 17278, 17282, 17296, 17307, 17315, 17333, 17336, 17344, 17346, 17348, 17354, 17365, 17374, 17385, 17387, 17397, 17401, 17404, 17412, 17417, 17432, 17436, 17444, 17452, 17467, 17469, 17501, 17502, 17513, 17530, 17531, 17547, 17550, 17555, 17573, 17577, 17578, 17586, 17600, 17602, 17611, 17627, 17640, 17645, 17648, 17649, 17652, 17667, 17676, 17682, 17721, 17724, 17729, 17731, 17751, 17754, 17762, 17770, 17772, 17781, 17802, 17820, 17823, 17833, 17848, 17857, 17858, 17865, 17867, 17887, 17893, 17909, 17924, 17926, 17945, 17971, 17977, 17986, 17990, 18003, 18011, 18042, 18046, 18052, 18077, 18086, 18091, 18099, 18103, 18123, 18128, 18131, 18142, 18153, 18165, 18167, 18173, 18181, 18206, 18226, 18252, 18263, 18276, 18277, 18284, 18314, 18319, 18327, 18330, 18342, 18352, 18355, 18362, 18364, 18375, 18383, 18398, 18403, 18432, 18447, 18453, 18465, 18467, 18485, 18486, 18492, 18505, 18508, 18510, 18515, 18536, 18540, 18548, 18550, 18557, 18563, 18570, 18574, 18576, 18616, 18622, 18627, 18639, 18649, 18655, 18673, 18681, 18712, 18745, 18746, 18758, 18773, 18778, 18782, 18826, 18832, 18834, 18842, 18851, 18853, 18858, 18862, 18867, 18872, 18877, 18908, 18910, 18912, 18913, 18919, 18925, 18929, 18930, 18933, 18936, 18944, 18974, 18975, 18997, 19009, 19011, 19014, 19016, 19028, 19057, 19098, 19106, 19114, 19130, 19133, 19147, 19157, 19162, 19166, 19173, 19179, 19198, 19213, 19225, 19230, 19239, 19241, 19266, 19274, 19279, 19283, 19285, 19316, 19327, 19335, 19352, 19358, 19362, 19366, 19377, 19379, 19405, 19423, 19428, 19431, 19437, 19444, 19456, 19469, 19475, 19476, 19481, 19486, 19492, 19506, 19507, 19536, 19537, 19562, 19586, 19589, 19622, 19634, 19651, 19663, 19679, 19685, 19690, 19697, 19707, 19712, 19714, 19715, 19717, 19724, 19734, 19744, 19754, 19769, 19779, 19792, 19807, 19830, 19850, 19863, 19875, 19903, 19904, 19908, 19963, 19985, 19988, 20003, 20012, 20014, 20018, 20040, 20042, 20054, 20078, 20079, 20083, 20087, 20089, 20114, 20121, 20122, 20138, 20155, 20162, 20186, 20199, 20216, 20222, 20234, 20238, 20254, 20285, 20299, 20302, 20305, 20337, 20345, 20364, 20365, 20374, 20382, 20394, 20403, 20440, 20450, 20451, 20476, 20506, 20508, 20516, 20527, 20536, 20540, 20541, 20542, 20557, 20566, 20584, 20608, 20611, 20613, 20618, 20627, 20644, 20656, 20663, 20674, 20682, 20690, 20699, 20707, 20711, 20737, 20751, 20765, 20771, 20774, 20778, 20779, 20789, 20795, 20798, 20834, 20851, 20865, 20867, 20869, 20872, 20883, 20892, 20894, 20895, 20896, 20901, 20919, 20924, 20933, 20937, 20938, 20946, 20947, 20955, 20957, 20958, 20965, 20966, 20970, 20974, 20991, 21001, 21013, 21018, 21020, 21025, 21114, 21142, 21205, 21210, 21227, 21230, 21236, 21253, 21262, 21294, 21300, 21316, 21331, 21339, 21349, 21356, 21365, 21369, 21376, 21399, 21410, 21413, 21418, 21420, 21435, 21447, 21452, 21455, 21468, 21470, 21474, 21486, 21495, 21501, 21517, 21530, 21539, 21546, 21547, 21553, 21556, 21564, 21569, 21579, 21593, 21595, 21599, 21603, 21632, 21643, 21690, 21713, 21726, 21732, 21737, 21738, 21744, 21755, 21773, 21782, 21800, 21807, 21829, 21839, 21848, 21876, 21893, 21923, 21926, 21931, 21958, 21967, 21970, 21973, 21982, 22002, 22029, 22031, 22035, 22044, 22051, 22068, 22073, 22099, 22117, 22121, 22126, 22145, 22146, 22149, 22161, 22166, 22170, 22177, 22205, 22210, 22231, 22233, 22234, 22244, 22246, 22251, 22272, 22280, 22299, 22317, 22321, 22327, 22350, 22355, 22363, 22365, 22378, 22383, 22387, 22391, 22393, 22395, 22403, 22406, 22418, 22423, 22428, 22437, 22444, 22451, 22453, 22460, 22480, 22491, 22510, 22530, 22536, 22554, 22557, 22561, 22562, 22564, 22571, 22588, 22593, 22606, 22607, 22610, 22611, 22642, 22666, 22717, 22729, 22738, 22744, 22745, 22763, 22768, 22769, 22774, 22782, 22789, 22796, 22800, 22836, 22841, 22843, 22858, 22864, 22886, 22889, 22897, 22925, 22928, 22938, 22944, 22945, 22948, 22950, 22956, 22983, 22986, 22988, 22993, 22994, 22997, 23028, 23030, 23039, 23047, 23081, 23091, 23097, 23105, 23108, 23111, 23113, 23133, 23145, 23150, 23161, 23172, 23176, 23199, 23200, 23202, 23204, 23205, 23211, 23212, 23226, 23227, 23231, 23236, 23239, 23263, 23265, 23266, 23269, 23290, 23311, 23315, 23320, 23323, 23340, 23352, 23361, 23372, 23375, 23376, 23381, 23386, 23390, 23401, 23415, 23418, 23428, 23429, 23453, 23467, 23479, 23505, 23508, 23527, 23528, 23535, 23537, 23541, 23553, 23555, 23557, 23559, 23563, 23566, 23581, 23585, 23596, 23598, 23604, 23605, 23617, 23630, 23643, 23658, 23669, 23673, 23680, 23683, 23690, 23691, 23701, 23707, 23714, 23720, 23725, 23733, 23746, 23750, 23751, 23769, 23774, 23784, 23809, 23818, 23831, 23839, 23842, 23857, 23924, 23930, 23944, 23945, 23947, 23959, 23971, 23975, 23987, 23989, 24002, 24003, 24009, 24019, 24026, 24039, 24041, 24046, 24062, 24071, 24077, 24079, 24087, 24096, 24119, 24121, 24137, 24150, 24169, 24181, 24194, 24195, 24199, 24200, 24203, 24210, 24223, 24258, 24272, 24274, 24291, 24293, 24300, 24305, 24318, 24339, 24378, 24380, 24383, 24392, 24402, 24442, 24468, 24476, 24480, 24488, 24490, 24495, 24502, 24504, 24505, 24516, 24562, 24572, 24594, 24595, 24597, 24600, 24606, 24609, 24622, 24623, 24629, 24631, 24632, 24633, 24657, 24662, 24664, 24666, 24682, 24697, 24709, 24753, 24758, 24766, 24833, 24840, 24851, 24858, 24869, 24886, 24909, 24915, 24916, 24929, 24945, 24951, 24966, 24972, 24980, 24996, 25012, 25017, 25043, 25059, 25067, 25104, 25121, 25125, 25127, 25133, 25150, 25191, 25210, 25224, 25228, 25240, 25243, 25255, 25264, 25267, 25272, 25282, 25284, 25290, 25292, 25293, 25334, 25338, 25350, 25365, 25368, 25380, 25389, 25391, 25400, 25413, 25422, 25440, 25444, 25446, 25456, 25460, 25472, 25486, 25495, 25499, 25509, 25512, 25528, 25530, 25532, 25547, 25559, 25591, 25593, 25597, 25601, 25614, 25616, 25638, 25672, 25691, 25698, 25712, 25713, 25721, 25722, 25725, 25732, 25753, 25760, 25775, 25794, 25813, 25819, 25828, 25833, 25901, 25908, 25947, 25983, 25987, 25989, 25996, 26008, 26017, 26025, 26030, 26039, 26047, 26054, 26058, 26077, 26088, 26092, 26112, 26183, 26185, 26190, 26201, 26204, 26237, 26239, 26241, 26248, 26251, 26252, 26256, 26263, 26268, 26282, 26314, 26331, 26346, 26358, 26360, 26364, 26377, 26428, 26434, 26460, 26481, 26485, 26497, 26532, 26533, 26538, 26561, 26567, 26585, 26588, 26595, 26596, 26618, 26625, 26633, 26643, 26651, 26677, 26687, 26715, 26721, 26732, 26733, 26746, 26760, 26762, 26769, 26779, 26781, 26822, 26828, 26830, 26870, 26882, 26883, 26889, 26892, 26894, 26914, 26927, 26932, 26944, 26953, 26957, 26963, 26967, 26973, 26976, 26978, 26985, 27030, 27038, 27044, 27048, 27052, 27067, 27111, 27115, 27121, 27142, 27144, 27153, 27167, 27205, 27209, 27217, 27218, 27229, 27233, 27240, 27250, 27280, 27302, 27332, 27339, 27349, 27362, 27376, 27386, 27396, 27401, 27413, 27426, 27429, 27456, 27470, 27474, 27476, 27477, 27480]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27481, 5)\n",
      "(24857, 5)\n",
      "(24856, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df_span.shape)\n",
    "df_span = df_span[(~df_span.index.isin(anomalous_idxs))]\n",
    "print(df_span.shape)\n",
    "df_span = df_span[(~df_span.text.isna())]\n",
    "df_span = df_span.reset_index(drop=True)\n",
    "df_span = df_span.copy()\n",
    "print(df_span.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"sentiment_code\"] = df_span[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments = df_span[\"sentiment_code\"].cat.codes.values\n",
    "\n",
    "test_df_span[\"sentiment_code\"] = test_df_span[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments_test = test_df_span[\"sentiment_code\"].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text\"] = df_span[\"text\"].astype(str)\n",
    "df_span[\"selected_text\"] = df_span[\"selected_text\"].astype(str)\n",
    "test_df_span[\"text\"] = test_df_span[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text_mod\"] = \"<s> \" + df_span.text.str.strip() + \" </s> </s> \" + df_span.sentiment + \" </s>\"\n",
    "test_df_span[\"text_mod\"] = \"<s> \" + test_df_span.text.str.strip() + \" </s> </s> \" + test_df_span.sentiment + \" </s>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclusions for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCLUDE_NEUTRAL_CLASS: (14017, 7)\n",
      "Train RUN_ON_SAMPLE (1000, 7)\n",
      "Test  RUN_ON_SAMPLE (1000, 6)\n"
     ]
    }
   ],
   "source": [
    "if EXCLUDE_NEUTRAL_CLASS:\n",
    "    df_span = df_span.loc[df_span.sentiment!=\"neutral\"].copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"EXCLUDE_NEUTRAL_CLASS:\", df_span.shape)\n",
    "\n",
    "if RUN_ON_SAMPLE:\n",
    "    df_span = df_span.sample(1000).copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"Train RUN_ON_SAMPLE\", df_span.shape)\n",
    "    test_df_span = test_df_span.sample(1000).copy()\n",
    "    test_df_span = test_df_span.reset_index(drop=True)\n",
    "    print(\"Test  RUN_ON_SAMPLE\", test_df_span.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_tokens = tokenizer.encode_batch(df_span.text_mod.tolist())\n",
    "Y_span_tokens = tokenizer.encode_batch(df_span.selected_text.tolist())\n",
    "X_span_tokens_test = tokenizer.encode_batch(test_df_span.text_mod.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = [i.ids for i in X_span_tokens]\n",
    "Y_span = [i.ids for i in Y_span_tokens]\n",
    "X_span_test = [i.ids for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_att = [i.attention_mask for i in X_span_tokens]\n",
    "Y_span_att = [i.attention_mask for i in Y_span_tokens]\n",
    "X_span_att_test = [i.attention_mask for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN_SPAN = max([len(i) for i in X_span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremities(l_string, s_string, print_it=False):\n",
    "    len_l = len(l_string)\n",
    "    len_s = len(s_string)\n",
    "    \n",
    "    for i in range(len_l - len_s + 1):\n",
    "        if (i + len_s) <= len_l:\n",
    "            substring = l_string[i:i+len_s]\n",
    "            if substring == s_string:\n",
    "                if print_it:\n",
    "                    print(l_string)\n",
    "                    print(substring)\n",
    "                    print(i, i+len_s, substring)\n",
    "                \n",
    "                start_vector, end_vector = [0] * len_l, [0] * len_l\n",
    "                start_vector[i], end_vector[i+len_s-1] = 1, 1\n",
    "                \n",
    "                return (start_vector, end_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 \t: #Processed\n",
      "0 \t: # of Anomalies\n"
     ]
    }
   ],
   "source": [
    "Y_span_starts, Y_span_stops = [], []\n",
    "anomaly_idx, counter = [], 0\n",
    "for num, (i,j) in enumerate(zip(X_span_tokens, Y_span_tokens)):\n",
    "    x,y = i.ids, j.ids\n",
    "    try:\n",
    "        s,e = get_extremities(x, y)\n",
    "    except TypeError as t:\n",
    "        counter += 1\n",
    "        anomaly_idx.append(num)\n",
    "    Y_span_starts.append(s)\n",
    "    Y_span_stops.append(e)\n",
    "print(num + 1, \"\\t: #Processed\")\n",
    "\n",
    "print(counter,\"\\t: # of Anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if counter:\n",
    "    print(anomaly_idx, sep=\",\")\n",
    "    print(df_span[\"original_index\"].iloc[anomaly_idx].tolist())\n",
    "    df_span.iloc[anomaly_idx].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greg Pritchard was ROBBED of a place in the final of BGT... Cry baby ****-face got through instead\n",
      "****-face\n",
      "[['<s>', 0, 0, 0], ['Ġg', 821, 0, 0], ['reg', 4950, 0, 0], ['Ġpr', 3349, 0, 0], ['itch', 3239, 0, 0], ['ard', 1120, 0, 0], ['Ġwas', 21, 0, 0], ['Ġrobbed', 13266, 0, 0], ['Ġof', 9, 0, 0], ['Ġa', 10, 0, 0], ['Ġplace', 317, 0, 0], ['Ġin', 11, 0, 0], ['Ġthe', 5, 0, 0], ['Ġfinal', 507, 0, 0], ['Ġof', 9, 0, 0], ['Ġb', 741, 0, 0], ['gt', 19377, 0, 0], ['...', 734, 0, 0], ['Ġcry', 8930, 0, 0], ['Ġbaby', 1928, 0, 0], ['Ġ****', 31095, 1, 0], ['-', 12, 0, 0], ['face', 9021, 0, 1], ['Ġgot', 300, 0, 0], ['Ġthrough', 149, 0, 0], ['Ġinstead', 1386, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0], ['Ġnegative', 2430, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0]]\n",
      "[[31095, 'Ġ****'], [12, '-'], [9021, 'face']]\n"
     ]
    }
   ],
   "source": [
    "check_idx = 156\n",
    "print(df_span.text[check_idx])\n",
    "print(df_span.selected_text[check_idx])\n",
    "print([[i,j,k,l] for i,j,k,l in zip(X_span_tokens[check_idx].tokens,\n",
    "                                    X_span_tokens[check_idx].ids,\n",
    "                                    Y_span_starts[check_idx],\n",
    "                                    Y_span_stops[check_idx])])\n",
    "print([[i,j] for i,j in zip(Y_span_tokens[check_idx].ids,\n",
    "                            Y_span_tokens[check_idx].tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "\n",
    "Y_span_starts = pad_sequences(Y_span_starts, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\").argmax(axis=1)\n",
    "Y_span_stops = pad_sequences(Y_span_stops, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\").argmax(axis=1)\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAX_SEQ_LEN_SPAN': 56,\n",
      " 'VOCAB_SIZE': 50265,\n",
      " 'X_span': (1000, 56),\n",
      " 'X_span_att': (1000, 56),\n",
      " 'X_span_att_test': (1000, 56),\n",
      " 'X_span_test': (1000, 56),\n",
      " 'Y_span': (1000, 56),\n",
      " 'Y_span_starts': (1000,),\n",
      " 'Y_span_stops': (1000,)}\n"
     ]
    }
   ],
   "source": [
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \"Y_span_starts\" : Y_span_starts.shape,\n",
    "    \"Y_span_stops\" : Y_span_stops.shape,\n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "    \"VOCAB_SIZE\":VOCAB_SIZE,\n",
    "    \"MAX_SEQ_LEN_SPAN\":MAX_SEQ_LEN_SPAN\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_flag = np.isin(Y_span_stops,\n",
    "                    np.unique(Y_span_stops,\n",
    "                              return_counts=True)[0][np.unique(Y_span_stops,\n",
    "                                                               return_counts=True)[1]>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (1000, 56) \t: X  \n",
      " (1000, 56) \t: X_att  \n",
      " (1000, 56) \t: Y  \n",
      " (1000,) \t: Y_starts  \n",
      " (1000,) \t: Y_stops  \n",
      " (1000, 56) \t: X_test  \n",
      " (1000, 56) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = X_span[keep_flag]\n",
    "X_span_att = X_span_att[keep_flag]\n",
    "Y_span = Y_span[keep_flag]\n",
    "Y_span_starts = Y_span_starts[keep_flag]\n",
    "Y_span_stops = Y_span_stops[keep_flag]\n",
    "X_span_test = X_span_test\n",
    "X_span_att_test = X_span_att_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (997, 56) \t: X  \n",
      " (997, 56) \t: X_att  \n",
      " (997, 56) \t: Y  \n",
      " (997,) \t: Y_starts  \n",
      " (997,) \t: Y_stops  \n",
      " (1000, 56) \t: X_test  \n",
      " (1000, 56) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#span_kf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=seeded_value)\n",
    "span_kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_span_words = [tokenizer.decode(i) for i in Y_span]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = max(MAX_SEQ_LEN_SENT, MAX_SEQ_LEN_SPAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X_sent': (1000, 56),\n",
      " 'X_sent_att': (1000, 56),\n",
      " 'X_span': (997, 56),\n",
      " 'X_span_att': (997, 56),\n",
      " 'X_span_att_test': (1000, 56),\n",
      " 'X_span_test': (1000, 56),\n",
      " 'Y_span': (997, 56)}\n"
     ]
    }
   ],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "X_sent = pad_sequences(X_sent, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_sent_att = pad_sequences(X_sent_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "    \n",
    "    \"X_sent\" : X_sent.shape,\n",
    "    \"X_sent_att\" : X_sent_att.shape,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_sequences = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"words\")\n",
    "    input_att_flags = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"att_flags\")\n",
    "    input_token_ids = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"token_ids\")\n",
    "    \n",
    "    config = RobertaConfig.from_pretrained(EXT_MODEL_DIR+'config-roberta-base.json')\n",
    "    roberta_model = TFRobertaModel.from_pretrained(EXT_MODEL_DIR+'pretrained-roberta-base.h5', config=config)\n",
    "    x = roberta_model(inputs=input_sequences, attention_mask=input_att_flags, token_type_ids=input_token_ids)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(DROPOUT)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    output_starts_0 = tf.keras.layers.Activation('softmax', name=\"starts_0\")(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(DROPOUT)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    output_stops_0 = tf.keras.layers.Activation('softmax', name=\"stops_0\")(x2)\n",
    "    \n",
    "    x3 = tf.keras.layers.Dropout(DROPOUT)(x[0]) \n",
    "    x3 = tf.keras.layers.Conv1D(768, 2,padding='same')(x3)\n",
    "    x3 = tf.keras.layers.LeakyReLU()(x3)\n",
    "    x3 = tf.keras.layers.Dense(1)(x3)\n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "    x3 = tf.keras.layers.Dense(3)(x3)\n",
    "    output_sentiment = tf.keras.layers.Activation('softmax', name=\"output_sentiments\")(x3)\n",
    "    \n",
    "    sentiment_model = Model([input_att_flags, input_sequences, input_token_ids], [output_sentiment])\n",
    "    \n",
    "    output_subtract = tf.keras.layers.Subtract()([output_starts_0, output_stops_0])\n",
    "    output_flat = concatenate([output_starts_0, output_stops_0, output_subtract, output_sentiment])\n",
    "    output_starts_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"starts_1\")(output_flat)\n",
    "    output_stops_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"stops_1\")(output_flat)\n",
    "    \n",
    "    span_detection_model = Model([input_att_flags, input_sequences, input_token_ids],\n",
    "                                 [output_starts_0, output_stops_0, output_starts_1, output_stops_1])\n",
    "    \n",
    "    return sentiment_model, span_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model, span_detection_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 56)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 56)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 56)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 56, 768), (N 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 56, 768)      0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 56, 768)      1180416     dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 56, 768)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 56, 1)        769         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 56)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            171         flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_sentiments (Activation)  (None, 3)            0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 125,826,988\n",
      "Trainable params: 125,826,988\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 56)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 56)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 56)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 56, 768), (N 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 56, 768)      0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 56, 768)      0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 56, 768)      0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 56, 768)      1180416     dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 56, 768)      1180416     dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 56, 768)      1180416     dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 56, 768)      0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 56, 768)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 56, 768)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 56, 1)        769         leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 56, 1)        769         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 56, 1)        769         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 56)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 56)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 56)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "starts_0 (Activation)           (None, 56)           0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "stops_0 (Activation)            (None, 56)           0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            171         flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 56)           0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_sentiments (Activation)  (None, 3)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 171)          0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "                                                                 subtract[0][0]                   \n",
      "                                                                 output_sentiments[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "starts_1 (Dense)                (None, 56)           9632        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "stops_1 (Dense)                 (None, 56)           9632        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 128,208,622\n",
      "Trainable params: 128,208,622\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "span_detection_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_sentiment(x):\n",
    "    encoded_repr = tokenizer.encode_batch(x.tolist())\n",
    "\n",
    "    sample_text_ids = pad_sequences([i.ids for i in encoded_repr],\n",
    "                                    maxlen=MAX_SEQ_LEN,\n",
    "                                    padding=\"post\")\n",
    "    sample_text_att = pad_sequences([i.attention_mask for i in encoded_repr],\n",
    "                                    maxlen=MAX_SEQ_LEN,\n",
    "                                    padding=\"post\")\n",
    "    pred = sentiment_model.predict({\"att_flags\":sample_text_att,\n",
    "                                    \"words\":sample_text_ids,\n",
    "                                    \"token_ids\":np.zeros_like(sample_text_att)},\n",
    "                                  batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    res = pd.DataFrame({\"predicted_sentiment\":pred.argmax(axis=1)})\n",
    "    \n",
    "    return res.predicted_sentiment.apply(lambda x:[k for k,v in sentiment_lookup.items() if v==x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_results(data):\n",
    "    data[\"predicted_sentiment\"] = infer_sentiment(x=data.text)\n",
    "\n",
    "    print(classification_report(y_true=data.sentiment[data.index.isin(tr_index)],\n",
    "                                y_pred=data.predicted_sentiment[data.index.isin(tr_index)]))\n",
    "\n",
    "    print(classification_report(y_true=data.sentiment[data.index.isin(va_index)],\n",
    "                                y_pred=data.predicted_sentiment[data.index.isin(va_index)]))\n",
    "\n",
    "    print(confusion_matrix(y_true=data.sentiment,\n",
    "                     y_pred=data.predicted_sentiment,\n",
    "                     labels=['positive', 'neutral', 'negative']))\n",
    "\n",
    "    print(confusion_matrix(y_true=data.sentiment,\n",
    "                     y_pred=data.predicted_sentiment,\n",
    "                     labels=['positive', 'neutral', 'negative'],\n",
    "                     normalize=\"all\"))\n",
    "\n",
    "    data[\"set2\"] = np.where(data.index.isin(tr_index), \"train\", \"valid\")\n",
    "    print(data.groupby(\"set2\").apply(lambda x : accuracy_score(y_true=x.sentiment, y_pred=x.predicted_sentiment)))\n",
    "\n",
    "    print(data.groupby(\"set\").apply(lambda x : accuracy_score(y_true=x.sentiment, y_pred=x.predicted_sentiment)))\n",
    "\n",
    "    print(pd.concat({\n",
    "        \"accuracy\" : data.groupby([\"set\", \"set2\"]).apply(lambda x : accuracy_score(y_true=x.sentiment,\n",
    "                                                                                   y_pred=x.predicted_sentiment)),\n",
    "        \"count\" : data.groupby([\"set\", \"set2\"])[\"sentiment\"].count()\n",
    "    }, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,c = np.unique(Y_sent, return_counts=True)\n",
    "cw = class_weight.compute_class_weight('balanced', np.unique(Y_sent), Y_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_kf = KFold(n_splits=NUM_FOLDS,shuffle=True,random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mcp = ModelCheckpoint(filepath=RESULTS_DIR+\"Sentiment_\"+MODEL_PREFIX+\"BestCheckpoint.h5\", monitor='val_loss',\n",
    "                                verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "sentiment_csvl = CSVLogger(filename=RESULTS_DIR+\"Sentiment_\"+MODEL_PREFIX+\"_LossLogs.csv\",\n",
    "                           separator=\",\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ==================== FOLD# 0 ====================\n",
      "Train on 666 samples, validate on 334 samples\n",
      "Epoch 1/3\n",
      "666/666 [==============================] - 24s 36ms/sample - loss: 1.1772 - accuracy: 0.3829 - val_loss: 1.0197 - val_accuracy: 0.4970\n",
      "Epoch 2/3\n",
      "666/666 [==============================] - 7s 11ms/sample - loss: 1.0007 - accuracy: 0.5060 - val_loss: 1.0265 - val_accuracy: 0.4521\n",
      "Epoch 3/3\n",
      "666/666 [==============================] - 7s 11ms/sample - loss: 0.9916 - accuracy: 0.5255 - val_loss: 0.9864 - val_accuracy: 0.5090\n",
      "Train on 666 samples, validate on 334 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "666/666 [==============================] - 32s 48ms/sample - loss: 0.8871 - accuracy: 0.5976 - val_loss: 0.9926 - val_accuracy: 0.5120\n",
      "Epoch 2/3\n",
      "666/666 [==============================] - 14s 21ms/sample - loss: 0.8133 - accuracy: 0.5916 - val_loss: 0.9175 - val_accuracy: 0.5539\n",
      "Epoch 3/3\n",
      "666/666 [==============================] - 14s 21ms/sample - loss: 0.7190 - accuracy: 0.6877 - val_loss: 0.9204 - val_accuracy: 0.5778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.83      0.86       200\n",
      "     neutral       0.82      0.74      0.78       261\n",
      "    positive       0.78      0.92      0.85       205\n",
      "\n",
      "    accuracy                           0.82       666\n",
      "   macro avg       0.83      0.83      0.83       666\n",
      "weighted avg       0.83      0.82      0.82       666\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.49      0.52        88\n",
      "     neutral       0.54      0.44      0.48       130\n",
      "    positive       0.62      0.80      0.70       116\n",
      "\n",
      "    accuracy                           0.58       334\n",
      "   macro avg       0.57      0.58      0.57       334\n",
      "weighted avg       0.57      0.58      0.57       334\n",
      "\n",
      "[[282  30   9]\n",
      " [ 92 251  48]\n",
      " [ 17  62 209]]\n",
      "[[0.282 0.03  0.009]\n",
      " [0.092 0.251 0.048]\n",
      " [0.017 0.062 0.209]]\n",
      "set2\n",
      "train    0.824324\n",
      "valid    0.577844\n",
      "dtype: float64\n",
      "set\n",
      "test     0.752\n",
      "train    0.732\n",
      "dtype: float64\n",
      "             accuracy  count\n",
      "set   set2                  \n",
      "test  train  0.843077    325\n",
      "      valid  0.582857    175\n",
      "train train  0.806452    341\n",
      "      valid  0.572327    159\n",
      "[INFO] ==================== FOLD# 1 ====================\n",
      "Train on 667 samples, validate on 333 samples\n",
      "Epoch 1/3\n",
      "667/667 [==============================] - 22s 33ms/sample - loss: 0.8885 - accuracy: 0.5922 - val_loss: 0.6047 - val_accuracy: 0.7147\n",
      "Epoch 2/3\n",
      "667/667 [==============================] - 8s 11ms/sample - loss: 0.8221 - accuracy: 0.6177 - val_loss: 0.6307 - val_accuracy: 0.7057\n",
      "Epoch 3/3\n",
      "667/667 [==============================] - 7s 11ms/sample - loss: 0.7460 - accuracy: 0.6567 - val_loss: 0.6608 - val_accuracy: 0.6847\n",
      "Train on 667 samples, validate on 333 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "667/667 [==============================] - 34s 51ms/sample - loss: 0.7112 - accuracy: 0.6747 - val_loss: 0.6332 - val_accuracy: 0.7147\n",
      "Epoch 2/3\n",
      "667/667 [==============================] - 14s 20ms/sample - loss: 0.6167 - accuracy: 0.7436 - val_loss: 0.6338 - val_accuracy: 0.7057\n",
      "Epoch 3/3\n",
      "667/667 [==============================] - 14s 20ms/sample - loss: 0.5467 - accuracy: 0.7571 - val_loss: 0.6584 - val_accuracy: 0.6997\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.89      0.88       184\n",
      "     neutral       0.88      0.89      0.88       261\n",
      "    positive       0.96      0.94      0.95       222\n",
      "\n",
      "    accuracy                           0.90       667\n",
      "   macro avg       0.90      0.90      0.90       667\n",
      "weighted avg       0.90      0.90      0.90       667\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.70      0.69       104\n",
      "     neutral       0.64      0.67      0.65       130\n",
      "    positive       0.81      0.74      0.77        99\n",
      "\n",
      "    accuracy                           0.70       333\n",
      "   macro avg       0.71      0.70      0.71       333\n",
      "weighted avg       0.70      0.70      0.70       333\n",
      "\n",
      "[[281  32   8]\n",
      " [ 22 319  50]\n",
      " [  3  49 236]]\n",
      "[[0.281 0.032 0.008]\n",
      " [0.022 0.319 0.05 ]\n",
      " [0.003 0.049 0.236]]\n",
      "set2\n",
      "train    0.904048\n",
      "valid    0.699700\n",
      "dtype: float64\n",
      "set\n",
      "test     0.836\n",
      "train    0.836\n",
      "dtype: float64\n",
      "             accuracy  count\n",
      "set   set2                  \n",
      "test  train  0.900000    340\n",
      "      valid  0.700000    160\n",
      "train train  0.908257    327\n",
      "      valid  0.699422    173\n",
      "[INFO] ==================== FOLD# 2 ====================\n",
      "Train on 667 samples, validate on 333 samples\n",
      "Epoch 1/3\n",
      "667/667 [==============================] - 24s 36ms/sample - loss: 0.7409 - accuracy: 0.6732 - val_loss: 0.3403 - val_accuracy: 0.9129\n",
      "Epoch 2/3\n",
      "667/667 [==============================] - 8s 12ms/sample - loss: 0.6003 - accuracy: 0.7466 - val_loss: 0.3266 - val_accuracy: 0.9129\n",
      "Epoch 3/3\n",
      "667/667 [==============================] - 8s 11ms/sample - loss: 0.6259 - accuracy: 0.7271 - val_loss: 0.3293 - val_accuracy: 0.9039\n",
      "Train on 667 samples, validate on 333 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "667/667 [==============================] - 34s 50ms/sample - loss: 0.5691 - accuracy: 0.7556 - val_loss: 0.3387 - val_accuracy: 0.8979\n",
      "Epoch 2/3\n",
      "667/667 [==============================] - 14s 20ms/sample - loss: 0.4754 - accuracy: 0.7901 - val_loss: 0.3551 - val_accuracy: 0.8739\n",
      "Epoch 3/3\n",
      "667/667 [==============================] - 14s 21ms/sample - loss: 0.3917 - accuracy: 0.8381 - val_loss: 0.3466 - val_accuracy: 0.8829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.92      0.94       192\n",
      "     neutral       0.93      0.96      0.95       260\n",
      "    positive       0.98      0.98      0.98       215\n",
      "\n",
      "    accuracy                           0.96       667\n",
      "   macro avg       0.96      0.95      0.96       667\n",
      "weighted avg       0.96      0.96      0.96       667\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        96\n",
      "     neutral       0.89      0.83      0.86       131\n",
      "    positive       0.91      0.92      0.91       106\n",
      "\n",
      "    accuracy                           0.88       333\n",
      "   macro avg       0.88      0.89      0.88       333\n",
      "weighted avg       0.88      0.88      0.88       333\n",
      "\n",
      "[[307  11   3]\n",
      " [ 13 359  19]\n",
      " [  1  22 265]]\n",
      "[[0.307 0.011 0.003]\n",
      " [0.013 0.359 0.019]\n",
      " [0.001 0.022 0.265]]\n",
      "set2\n",
      "train    0.955022\n",
      "valid    0.882883\n",
      "dtype: float64\n",
      "set\n",
      "test     0.934\n",
      "train    0.928\n",
      "dtype: float64\n",
      "             accuracy  count\n",
      "set   set2                  \n",
      "test  train  0.961194    335\n",
      "      valid  0.878788    165\n",
      "train train  0.948795    332\n",
      "      valid  0.886905    168\n"
     ]
    }
   ],
   "source": [
    "for num, (tr_index, va_index) in enumerate(sentiment_kf.split(X_sent, Y_sent)):\n",
    "    print(\"[INFO] ==================== FOLD#\", num, \"====================\")\n",
    "\n",
    "    sentiment_model.layers[3].trainable = False\n",
    "    adam = Adam(learning_rate=SENTIMENT_MAX_LR)\n",
    "    sentiment_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                            optimizer=adam,\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "    sent_history = sentiment_model.fit(x={\"att_flags\":X_sent_att[tr_index],\n",
    "                                          \"words\":X_sent[tr_index],\n",
    "                                          \"token_ids\":np.zeros_like(X_sent_att[tr_index])},\n",
    "                                       y={\"output_sentiments\":Y_sent[tr_index]},\n",
    "                                       shuffle=True,\n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       epochs=SENTIMENT_NUM_EPOCHS[0],\n",
    "                                       validation_data=({\"att_flags\":X_sent_att[va_index],\n",
    "                                                         \"words\":X_sent[va_index],\n",
    "                                                         \"token_ids\":np.zeros_like(X_sent_att[va_index])},\n",
    "                                                        {\"output_sentiments\":Y_sent[va_index]}),\n",
    "                                       verbose=1,\n",
    "                                       class_weight=cw,\n",
    "                                       callbacks=[sentiment_mcp, sentiment_csvl])\n",
    "\n",
    "    sentiment_model.layers[3].trainable = True\n",
    "    adam = Adam(learning_rate=SENTIMENT_MIN_LR)\n",
    "    sentiment_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                            optimizer=adam,\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "    sent_history_finetuned = sentiment_model.fit(x={\"att_flags\":X_sent_att[tr_index],\n",
    "                                                    \"words\":X_sent[tr_index],\n",
    "                                                    \"token_ids\":np.zeros_like(X_sent_att[tr_index])},\n",
    "                                                 y={\"output_sentiments\":Y_sent[tr_index]},\n",
    "                                                 shuffle=True,\n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                 epochs=SENTIMENT_NUM_EPOCHS[1],\n",
    "                                                 validation_data=({\"att_flags\":X_sent_att[va_index],\n",
    "                                                                   \"words\":X_sent[va_index],\n",
    "                                                                   \"token_ids\":np.zeros_like(X_sent_att[va_index])},\n",
    "                                                                  {\"output_sentiments\":Y_sent[va_index]}),\n",
    "                                                 verbose=1,\n",
    "                                                 class_weight=cw,\n",
    "                                                 callbacks=[sentiment_mcp, sentiment_csvl])\n",
    "    \n",
    "    get_sentiment_results(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'predicted_sentiment': 'neutral',\n",
      "     'text': ' I think it`ll be more like a casual attendance instead of a '\n",
      "             'review, as there might not be another issue of felix by then.  '\n",
      "             'Thanks.'},\n",
      " 1: {'predicted_sentiment': 'neutral',\n",
      "     'text': ' Eeee! Hiya!  Haven`t spoken to you in ages! How you doing? '\n",
      "             '*enormous hug*'},\n",
      " 2: {'predicted_sentiment': 'positive',\n",
      "     'text': '**** just spent a week whole of pay on shopping  oh well i`m '\n",
      "             'happy!!!'},\n",
      " 3: {'predicted_sentiment': 'positive',\n",
      "     'text': ' i wanted to see that.  enjoy it!'},\n",
      " 4: {'predicted_sentiment': 'positive',\n",
      "     'text': '59 may be my new FAV number!!!'}}\n"
     ]
    }
   ],
   "source": [
    "sample_text = data.text.sample(5).tolist()\n",
    "\n",
    "encoded_repr = tokenizer.encode_batch(sample_text)\n",
    "\n",
    "sample_text_ids = pad_sequences([i.ids for i in encoded_repr],\n",
    "                                maxlen=MAX_SEQ_LEN,\n",
    "                                padding=\"post\")\n",
    "sample_text_att = pad_sequences([i.attention_mask for i in encoded_repr],\n",
    "                                maxlen=MAX_SEQ_LEN,\n",
    "                                padding=\"post\")\n",
    "pred = sentiment_model.predict({\"att_flags\":sample_text_att,\n",
    "                                \"words\":sample_text_ids,\n",
    "                                \"token_ids\":np.zeros_like(sample_text_att)})\n",
    "\n",
    "pprint({\n",
    "    num:{\n",
    "        \"text\":i,\n",
    "        \"predicted_sentiment\":[k for k,v in sentiment_lookup.items() if v==j][0]\n",
    "    } for num,(i,j) in enumerate(zip(sample_text, pred.argmax(axis=1)))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 2, 'neutral': 1, 'negative': 0}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>set</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "      <th>set2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>****  lol....where u headed to?</td>\n",
       "      <td>test</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>there`s just no air</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>- halla!!! doing ok- got a cold but trying to ignore it  how about you?</td>\n",
       "      <td>test</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>wants new hair now! Just so frank won`t talk to me</td>\n",
       "      <td>test</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>My logic to getting a short prom dress? Last year it was so freaking hot I was dying... Look at the weather...  hahah just my luck</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>25-man Ulduar is hard...</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>Wow, my bed is SO comfy &amp; my nap has been much too short. Getting up for work = do not want.</td>\n",
       "      <td>test</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>Going to clean my room but thats not hard. i keep my room pretty clean really....anyway, i could still be doing better things!!!!</td>\n",
       "      <td>test</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>awesome i just ruined all my white clothes i never remember to separate</td>\n",
       "      <td>test</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>bored. cold. tired. lethargic feeling. Man! great holiday monday!</td>\n",
       "      <td>test</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                   text  \\\n",
       "726                                                                                                     ****  lol....where u headed to?   \n",
       "771                                                                                                                 there`s just no air   \n",
       "5                                                               - halla!!! doing ok- got a cold but trying to ignore it  how about you?   \n",
       "69                                                                                   wants new hair now! Just so frank won`t talk to me   \n",
       "173  My logic to getting a short prom dress? Last year it was so freaking hot I was dying... Look at the weather...  hahah just my luck   \n",
       "435                                                                                                            25-man Ulduar is hard...   \n",
       "948                                        Wow, my bed is SO comfy & my nap has been much too short. Getting up for work = do not want.   \n",
       "841   Going to clean my room but thats not hard. i keep my room pretty clean really....anyway, i could still be doing better things!!!!   \n",
       "143                                                             awesome i just ruined all my white clothes i never remember to separate   \n",
       "713                                                                   bored. cold. tired. lethargic feeling. Man! great holiday monday!   \n",
       "\n",
       "       set sentiment predicted_sentiment   set2  \n",
       "726   test   neutral            negative  train  \n",
       "771  train   neutral            negative  train  \n",
       "5     test   neutral            negative  train  \n",
       "69    test   neutral            negative  valid  \n",
       "173  train   neutral            negative  valid  \n",
       "435  train   neutral            negative  valid  \n",
       "948   test   neutral            negative  train  \n",
       "841   test   neutral            negative  valid  \n",
       "143   test   neutral            negative  train  \n",
       "713   test   neutral            negative  valid  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[(data.sentiment == \"neutral\") & (data.predicted_sentiment == \"negative\")].sample(10) # most incorrect in this cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span Detection Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_FOLDS):\n",
    "    if os.path.exists(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\"):\n",
    "        os.remove(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(pred_dict):\n",
    "    print(\"[INFO] \",\"=\"*15,\"Validation for FOLD#\", num, \"=\"*15)\n",
    "    funcs = [accuracy_score, f1_score, precision_score, recall_score, confusion_matrix]\n",
    "    for f in funcs:\n",
    "        for data_set in [\"train\",\"valid\"]:\n",
    "            for var in [\"starts\", \"stops\"]:\n",
    "                if f in [accuracy_score]:\n",
    "                    res = f(**pred_dict[data_set][var])\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "                elif f in [confusion_matrix]:\n",
    "                    res = f(**pred_dict[data_set][var], labels=np.arange(MAX_SEQ_LEN))\n",
    "                    np.savetxt(X=res, fmt='%i', delimiter=\",\",\n",
    "                               fname=RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                    print(\"[INFO] \\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__, \"\\t||\", \n",
    "                          RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                else:\n",
    "                    res = f(**pred_dict[data_set][var], average=\"macro\")\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "        print(\"=======================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(string):\n",
    "    string = re.sub(pattern=\" (negative|positive|neutral)[ ]+$\", repl=\"\", string=string)\n",
    "    string = re.sub(pattern=\" +\", repl=\" \", string=string.strip())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><h2 id=\"finetuning\">Fine-tuning</h2></a>\n",
    "<p>Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.</p>\n",
    " <p>This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.</p>\n",
    " <p>It is critical to only do this step <em>after</em> the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.</p>\n",
    " <p>It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>\n",
    "\n",
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><p><strong>Important note about <code>compile()</code> and <code>trainable</code></strong></p></a>\n",
    "<p>Calling <code>compile()</code> on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the <code>trainable</code>\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until <code>compile</code> is called again. Hence, if you change any <code>trainable</code> value, make sure\n",
    " to call <code>compile()</code> again on your\n",
    "model for your changes to be taken into account.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_detection_model_bkup = span_detection_model\n",
    "span_detection_model.save_weights(filepath=RESULTS_DIR+\"FinalSentimentModel.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ==================== FOLD# 0 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 664 samples, validate on 333 samples\n",
      "Epoch 1/3\n",
      "664/664 [==============================] - 25s 38ms/sample - loss: 13.0084 - starts_0_loss: 2.6541 - stops_0_loss: 2.8482 - starts_1_loss: 3.6847 - stops_1_loss: 3.7928 - starts_0_accuracy: 0.3313 - stops_0_accuracy: 0.3117 - starts_1_accuracy: 0.2801 - stops_1_accuracy: 0.0452 - val_loss: 10.3521 - val_starts_0_loss: 1.6242 - val_stops_0_loss: 1.7838 - val_starts_1_loss: 3.3361 - val_stops_1_loss: 3.6002 - val_starts_0_accuracy: 0.4474 - val_stops_0_accuracy: 0.3964 - val_starts_1_accuracy: 0.3483 - val_stops_1_accuracy: 0.0811\n",
      "Epoch 2/3\n",
      "664/664 [==============================] - 8s 12ms/sample - loss: 10.2866 - starts_0_loss: 1.8265 - stops_0_loss: 1.9848 - starts_1_loss: 3.0819 - stops_1_loss: 3.3932 - starts_0_accuracy: 0.3991 - stops_0_accuracy: 0.4036 - starts_1_accuracy: 0.3358 - stops_1_accuracy: 0.1401 - val_loss: 9.7407 - val_starts_0_loss: 1.7127 - val_stops_0_loss: 1.7100 - val_starts_1_loss: 2.9288 - val_stops_1_loss: 3.3832 - val_starts_0_accuracy: 0.4595 - val_stops_0_accuracy: 0.4354 - val_starts_1_accuracy: 0.3483 - val_stops_1_accuracy: 0.1471\n",
      "Epoch 3/3\n",
      "664/664 [==============================] - 8s 12ms/sample - loss: 9.3800 - starts_0_loss: 1.6744 - stops_0_loss: 1.8363 - starts_1_loss: 2.7161 - stops_1_loss: 3.1522 - starts_0_accuracy: 0.4684 - stops_0_accuracy: 0.4383 - starts_1_accuracy: 0.3358 - stops_1_accuracy: 0.2319 - val_loss: 9.2033 - val_starts_0_loss: 1.7112 - val_stops_0_loss: 1.6164 - val_starts_1_loss: 2.6852 - val_stops_1_loss: 3.1776 - val_starts_0_accuracy: 0.3784 - val_stops_0_accuracy: 0.4655 - val_starts_1_accuracy: 0.3483 - val_stops_1_accuracy: 0.2222\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 664 samples, validate on 333 samples\n",
      "Epoch 1/3\n",
      "664/664 [==============================] - 25s 38ms/sample - loss: 9.0465 - starts_0_loss: 1.7183 - stops_0_loss: 1.6873 - starts_1_loss: 2.5974 - stops_1_loss: 3.0260 - starts_0_accuracy: 0.4352 - stops_0_accuracy: 0.4608 - starts_1_accuracy: 0.3358 - stops_1_accuracy: 0.2666 - val_loss: 9.0523 - val_starts_0_loss: 1.5964 - val_stops_0_loss: 1.6209 - val_starts_1_loss: 2.6562 - val_stops_1_loss: 3.1676 - val_starts_0_accuracy: 0.4474 - val_stops_0_accuracy: 0.4535 - val_starts_1_accuracy: 0.3483 - val_stops_1_accuracy: 0.2372\n",
      "Epoch 2/3\n",
      "664/664 [==============================] - 8s 12ms/sample - loss: 8.8980 - starts_0_loss: 1.6292 - stops_0_loss: 1.6816 - starts_1_loss: 2.5823 - stops_1_loss: 3.0193 - starts_0_accuracy: 0.4639 - stops_0_accuracy: 0.4608 - starts_1_accuracy: 0.3358 - stops_1_accuracy: 0.2711 - val_loss: 8.9858 - val_starts_0_loss: 1.5549 - val_stops_0_loss: 1.6195 - val_starts_1_loss: 2.6392 - val_stops_1_loss: 3.1624 - val_starts_0_accuracy: 0.4715 - val_stops_0_accuracy: 0.4505 - val_starts_1_accuracy: 0.3483 - val_stops_1_accuracy: 0.2282\n",
      "Epoch 3/3\n",
      "664/664 [==============================] - 8s 12ms/sample - loss: 8.6676 - starts_0_loss: 1.5061 - stops_0_loss: 1.6062 - starts_1_loss: 2.5556 - stops_1_loss: 2.9984 - starts_0_accuracy: 0.4744 - stops_0_accuracy: 0.4849 - starts_1_accuracy: 0.3358 - stops_1_accuracy: 0.2831 - val_loss: 8.9661 - val_starts_0_loss: 1.5394 - val_stops_0_loss: 1.6297 - val_starts_1_loss: 2.6302 - val_stops_1_loss: 3.1579 - val_starts_0_accuracy: 0.4925 - val_stops_0_accuracy: 0.4384 - val_starts_1_accuracy: 0.3483 - val_stops_1_accuracy: 0.2282\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 664 samples, validate on 333 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "664/664 [==============================] - 37s 55ms/sample - loss: 8.7640 - starts_0_loss: 1.5081 - stops_0_loss: 1.6940 - starts_1_loss: 2.5487 - stops_1_loss: 2.9999 - starts_0_accuracy: 0.4789 - stops_0_accuracy: 0.4563 - starts_1_accuracy: 0.3358 - stops_1_accuracy: 0.2861 - val_loss: 8.9314 - val_starts_0_loss: 1.5264 - val_stops_0_loss: 1.6126 - val_starts_1_loss: 2.6276 - val_stops_1_loss: 3.1559 - val_starts_0_accuracy: 0.4895 - val_stops_0_accuracy: 0.4324 - val_starts_1_accuracy: 0.3483 - val_stops_1_accuracy: 0.2372\n",
      "Epoch 2/3\n",
      "664/664 [==============================] - 14s 21ms/sample - loss: 8.6142 - starts_0_loss: 1.4737 - stops_0_loss: 1.6146 - starts_1_loss: 2.5459 - stops_1_loss: 2.9854 - starts_0_accuracy: 0.4819 - stops_0_accuracy: 0.4925 - starts_1_accuracy: 0.3358 - stops_1_accuracy: 0.2907 - val_loss: 8.9051 - val_starts_0_loss: 1.5184 - val_stops_0_loss: 1.6016 - val_starts_1_loss: 2.6240 - val_stops_1_loss: 3.1526 - val_starts_0_accuracy: 0.4865 - val_stops_0_accuracy: 0.4384 - val_starts_1_accuracy: 0.3483 - val_stops_1_accuracy: 0.2372\n",
      "Epoch 3/3\n",
      "664/664 [==============================] - 15s 22ms/sample - loss: 8.4635 - starts_0_loss: 1.4127 - stops_0_loss: 1.5475 - starts_1_loss: 2.5228 - stops_1_loss: 2.9803 - starts_0_accuracy: 0.5196 - stops_0_accuracy: 0.5015 - starts_1_accuracy: 0.3358 - stops_1_accuracy: 0.3117 - val_loss: 8.8960 - val_starts_0_loss: 1.5173 - val_stops_0_loss: 1.6004 - val_starts_1_loss: 2.6205 - val_stops_1_loss: 3.1495 - val_starts_0_accuracy: 0.4865 - val_stops_0_accuracy: 0.4384 - val_starts_1_accuracy: 0.3483 - val_stops_1_accuracy: 0.2402\n",
      "[INFO]  =============== Validation for FOLD# 0 ===============\n",
      "[INFO] 59.19 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 57.23 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 48.65 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 43.84 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 47.87 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 47.51 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 44.13 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 33.67 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 51.05 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 46.72 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 46.26 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 33.19 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 47.51 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 51.64 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 46.52 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 37.48 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V29_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V29_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V29_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V29_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (664, 56) (664, 56)\n",
      "[INFO] Prediction shape for validation data:  (333, 56) (333, 56)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  515 out of 664\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  258 out of 333\n",
      "[INFO] Training Jaccard Score:  0.5557076678510083\n",
      "[INFO] Validation Jaccard Score:  0.5148121398244414\n",
      "[INFO] Training for fold: 0 finished at Thu Jun  4 20:57:13 2020\n",
      "[INFO] ==================== FOLD# 1 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 665 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "665/665 [==============================] - 26s 39ms/sample - loss: 12.9254 - starts_0_loss: 2.6067 - stops_0_loss: 2.7547 - starts_1_loss: 3.7024 - stops_1_loss: 3.8232 - starts_0_accuracy: 0.3429 - stops_0_accuracy: 0.3158 - starts_1_accuracy: 0.2511 - stops_1_accuracy: 0.0511 - val_loss: 10.6021 - val_starts_0_loss: 1.7222 - val_stops_0_loss: 1.9270 - val_starts_1_loss: 3.3641 - val_stops_1_loss: 3.5773 - val_starts_0_accuracy: 0.4307 - val_stops_0_accuracy: 0.3584 - val_starts_1_accuracy: 0.3313 - val_stops_1_accuracy: 0.1084\n",
      "Epoch 2/3\n",
      "665/665 [==============================] - 8s 12ms/sample - loss: 10.0897 - starts_0_loss: 1.7699 - stops_0_loss: 1.8204 - starts_1_loss: 3.0736 - stops_1_loss: 3.4120 - starts_0_accuracy: 0.4451 - stops_0_accuracy: 0.4241 - starts_1_accuracy: 0.3444 - stops_1_accuracy: 0.1429 - val_loss: 9.7946 - val_starts_0_loss: 1.6841 - val_stops_0_loss: 1.9294 - val_starts_1_loss: 2.8943 - val_stops_1_loss: 3.2754 - val_starts_0_accuracy: 0.4307 - val_stops_0_accuracy: 0.3705 - val_starts_1_accuracy: 0.3313 - val_stops_1_accuracy: 0.1627\n",
      "Epoch 3/3\n",
      "665/665 [==============================] - 8s 12ms/sample - loss: 9.4141 - starts_0_loss: 1.6789 - stops_0_loss: 1.8622 - starts_1_loss: 2.7108 - stops_1_loss: 3.1731 - starts_0_accuracy: 0.4932 - stops_0_accuracy: 0.4241 - starts_1_accuracy: 0.3444 - stops_1_accuracy: 0.2090 - val_loss: 9.2958 - val_starts_0_loss: 1.6055 - val_stops_0_loss: 1.8583 - val_starts_1_loss: 2.6890 - val_stops_1_loss: 3.1330 - val_starts_0_accuracy: 0.4247 - val_stops_0_accuracy: 0.4127 - val_starts_1_accuracy: 0.3313 - val_stops_1_accuracy: 0.2108\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 665 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "665/665 [==============================] - 26s 38ms/sample - loss: 8.7387 - starts_0_loss: 1.4811 - stops_0_loss: 1.6453 - starts_1_loss: 2.5516 - stops_1_loss: 3.0501 - starts_0_accuracy: 0.5008 - stops_0_accuracy: 0.4767 - starts_1_accuracy: 0.3444 - stops_1_accuracy: 0.2707 - val_loss: 9.2115 - val_starts_0_loss: 1.5837 - val_stops_0_loss: 1.8323 - val_starts_1_loss: 2.6761 - val_stops_1_loss: 3.1090 - val_starts_0_accuracy: 0.4337 - val_stops_0_accuracy: 0.4066 - val_starts_1_accuracy: 0.3313 - val_stops_1_accuracy: 0.2349\n",
      "Epoch 2/3\n",
      "665/665 [==============================] - 8s 12ms/sample - loss: 8.6946 - starts_0_loss: 1.4899 - stops_0_loss: 1.6216 - starts_1_loss: 2.5336 - stops_1_loss: 3.0369 - starts_0_accuracy: 0.5158 - stops_0_accuracy: 0.4902 - starts_1_accuracy: 0.3444 - stops_1_accuracy: 0.2917 - val_loss: 9.1661 - val_starts_0_loss: 1.5700 - val_stops_0_loss: 1.8155 - val_starts_1_loss: 2.6700 - val_stops_1_loss: 3.1000 - val_starts_0_accuracy: 0.4398 - val_stops_0_accuracy: 0.4127 - val_starts_1_accuracy: 0.3313 - val_stops_1_accuracy: 0.2410\n",
      "Epoch 3/3\n",
      "665/665 [==============================] - 8s 12ms/sample - loss: 8.5302 - starts_0_loss: 1.4252 - stops_0_loss: 1.5655 - starts_1_loss: 2.5259 - stops_1_loss: 3.0155 - starts_0_accuracy: 0.5098 - stops_0_accuracy: 0.4767 - starts_1_accuracy: 0.3444 - stops_1_accuracy: 0.3008 - val_loss: 9.1431 - val_starts_0_loss: 1.5631 - val_stops_0_loss: 1.8113 - val_starts_1_loss: 2.6646 - val_stops_1_loss: 3.0932 - val_starts_0_accuracy: 0.4367 - val_stops_0_accuracy: 0.4127 - val_starts_1_accuracy: 0.3313 - val_stops_1_accuracy: 0.2410\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 665 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "665/665 [==============================] - 38s 57ms/sample - loss: 8.5847 - starts_0_loss: 1.4623 - stops_0_loss: 1.5900 - starts_1_loss: 2.5117 - stops_1_loss: 3.0157 - starts_0_accuracy: 0.5158 - stops_0_accuracy: 0.4842 - starts_1_accuracy: 0.3444 - stops_1_accuracy: 0.2992 - val_loss: 9.1177 - val_starts_0_loss: 1.5468 - val_stops_0_loss: 1.8116 - val_starts_1_loss: 2.6604 - val_stops_1_loss: 3.0876 - val_starts_0_accuracy: 0.4337 - val_stops_0_accuracy: 0.4066 - val_starts_1_accuracy: 0.3313 - val_stops_1_accuracy: 0.2440\n",
      "Epoch 2/3\n",
      "665/665 [==============================] - 15s 22ms/sample - loss: 8.4435 - starts_0_loss: 1.3980 - stops_0_loss: 1.5626 - starts_1_loss: 2.4952 - stops_1_loss: 3.0028 - starts_0_accuracy: 0.5308 - stops_0_accuracy: 0.5023 - starts_1_accuracy: 0.3444 - stops_1_accuracy: 0.3128 - val_loss: 9.0729 - val_starts_0_loss: 1.5296 - val_stops_0_loss: 1.7922 - val_starts_1_loss: 2.6565 - val_stops_1_loss: 3.0831 - val_starts_0_accuracy: 0.4398 - val_stops_0_accuracy: 0.4066 - val_starts_1_accuracy: 0.3313 - val_stops_1_accuracy: 0.2470\n",
      "Epoch 3/3\n",
      "665/665 [==============================] - 15s 22ms/sample - loss: 8.4852 - starts_0_loss: 1.4619 - stops_0_loss: 1.5246 - starts_1_loss: 2.5097 - stops_1_loss: 2.9938 - starts_0_accuracy: 0.5233 - stops_0_accuracy: 0.5053 - starts_1_accuracy: 0.3444 - stops_1_accuracy: 0.3173 - val_loss: 9.0537 - val_starts_0_loss: 1.5178 - val_stops_0_loss: 1.7899 - val_starts_1_loss: 2.6544 - val_stops_1_loss: 3.0799 - val_starts_0_accuracy: 0.4398 - val_stops_0_accuracy: 0.4187 - val_starts_1_accuracy: 0.3313 - val_stops_1_accuracy: 0.2530\n",
      "[INFO]  =============== Validation for FOLD# 1 ===============\n",
      "[INFO] 64.06 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 55.19 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 43.98 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 41.87 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 60.95 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 48.97 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 32.23 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 36.13 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 61.99 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 47.72 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 34.40 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 36.58 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 63.08 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 54.99 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 35.41 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 45.28 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V29_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V29_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V29_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V29_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (665, 56) (665, 56)\n",
      "[INFO] Prediction shape for validation data:  (332, 56) (332, 56)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  534 out of 665\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  269 out of 332\n",
      "[INFO] Training Jaccard Score:  0.5604574036905151\n",
      "[INFO] Validation Jaccard Score:  0.5109616108624037\n",
      "[INFO] Training for fold: 1 finished at Thu Jun  4 20:59:59 2020\n",
      "[INFO] ==================== FOLD# 2 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 665 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "665/665 [==============================] - 27s 40ms/sample - loss: 12.9522 - starts_0_loss: 2.6544 - stops_0_loss: 2.7731 - starts_1_loss: 3.6954 - stops_1_loss: 3.8084 - starts_0_accuracy: 0.3308 - stops_0_accuracy: 0.3203 - starts_1_accuracy: 0.2451 - stops_1_accuracy: 0.0617 - val_loss: 10.5294 - val_starts_0_loss: 1.7866 - val_stops_0_loss: 1.8021 - val_starts_1_loss: 3.3619 - val_stops_1_loss: 3.5872 - val_starts_0_accuracy: 0.4217 - val_stops_0_accuracy: 0.4157 - val_starts_1_accuracy: 0.3404 - val_stops_1_accuracy: 0.0994\n",
      "Epoch 2/3\n",
      "665/665 [==============================] - 8s 12ms/sample - loss: 10.2659 - starts_0_loss: 1.8284 - stops_0_loss: 1.9096 - starts_1_loss: 3.1090 - stops_1_loss: 3.4252 - starts_0_accuracy: 0.4075 - stops_0_accuracy: 0.4211 - starts_1_accuracy: 0.3398 - stops_1_accuracy: 0.1414 - val_loss: 9.6646 - val_starts_0_loss: 1.6859 - val_stops_0_loss: 1.7202 - val_starts_1_loss: 2.9439 - val_stops_1_loss: 3.3267 - val_starts_0_accuracy: 0.4157 - val_stops_0_accuracy: 0.4398 - val_starts_1_accuracy: 0.3404 - val_stops_1_accuracy: 0.1627\n",
      "Epoch 3/3\n",
      "665/665 [==============================] - 8s 12ms/sample - loss: 9.4485 - starts_0_loss: 1.7261 - stops_0_loss: 1.8246 - starts_1_loss: 2.7370 - stops_1_loss: 3.1786 - starts_0_accuracy: 0.4556 - stops_0_accuracy: 0.4391 - starts_1_accuracy: 0.3398 - stops_1_accuracy: 0.2211 - val_loss: 9.4116 - val_starts_0_loss: 1.7911 - val_stops_0_loss: 1.7115 - val_starts_1_loss: 2.7450 - val_stops_1_loss: 3.1762 - val_starts_0_accuracy: 0.3916 - val_stops_0_accuracy: 0.4307 - val_starts_1_accuracy: 0.3404 - val_stops_1_accuracy: 0.2289\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 665 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "665/665 [==============================] - 24s 36ms/sample - loss: 9.0130 - starts_0_loss: 1.6350 - stops_0_loss: 1.7037 - starts_1_loss: 2.6042 - stops_1_loss: 3.0704 - starts_0_accuracy: 0.4722 - stops_0_accuracy: 0.4526 - starts_1_accuracy: 0.3398 - stops_1_accuracy: 0.2947 - val_loss: 9.2099 - val_starts_0_loss: 1.6802 - val_stops_0_loss: 1.6916 - val_starts_1_loss: 2.7028 - val_stops_1_loss: 3.1501 - val_starts_0_accuracy: 0.4006 - val_stops_0_accuracy: 0.4307 - val_starts_1_accuracy: 0.3404 - val_stops_1_accuracy: 0.2349\n",
      "Epoch 2/3\n",
      "665/665 [==============================] - 8s 12ms/sample - loss: 8.8686 - starts_0_loss: 1.5364 - stops_0_loss: 1.7080 - starts_1_loss: 2.5755 - stops_1_loss: 3.0538 - starts_0_accuracy: 0.4887 - stops_0_accuracy: 0.4617 - starts_1_accuracy: 0.3398 - stops_1_accuracy: 0.2842 - val_loss: 9.1757 - val_starts_0_loss: 1.6752 - val_stops_0_loss: 1.6887 - val_starts_1_loss: 2.6869 - val_stops_1_loss: 3.1411 - val_starts_0_accuracy: 0.4127 - val_stops_0_accuracy: 0.4337 - val_starts_1_accuracy: 0.3404 - val_stops_1_accuracy: 0.2259\n",
      "Epoch 3/3\n",
      "665/665 [==============================] - 8s 12ms/sample - loss: 8.7327 - starts_0_loss: 1.5622 - stops_0_loss: 1.5919 - starts_1_loss: 2.5504 - stops_1_loss: 3.0259 - starts_0_accuracy: 0.5068 - stops_0_accuracy: 0.4752 - starts_1_accuracy: 0.3398 - stops_1_accuracy: 0.3008 - val_loss: 9.1598 - val_starts_0_loss: 1.6752 - val_stops_0_loss: 1.6871 - val_starts_1_loss: 2.6789 - val_stops_1_loss: 3.1355 - val_starts_0_accuracy: 0.4247 - val_stops_0_accuracy: 0.4307 - val_starts_1_accuracy: 0.3404 - val_stops_1_accuracy: 0.2349\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 665 samples, validate on 332 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "665/665 [==============================] - 36s 54ms/sample - loss: 8.5575 - starts_0_loss: 1.4147 - stops_0_loss: 1.6000 - starts_1_loss: 2.5258 - stops_1_loss: 3.0218 - starts_0_accuracy: 0.5519 - stops_0_accuracy: 0.4647 - starts_1_accuracy: 0.3398 - stops_1_accuracy: 0.2992 - val_loss: 9.1448 - val_starts_0_loss: 1.6727 - val_stops_0_loss: 1.6910 - val_starts_1_loss: 2.6727 - val_stops_1_loss: 3.1261 - val_starts_0_accuracy: 0.4217 - val_stops_0_accuracy: 0.4367 - val_starts_1_accuracy: 0.3404 - val_stops_1_accuracy: 0.2349\n",
      "Epoch 2/3\n",
      "665/665 [==============================] - 14s 21ms/sample - loss: 8.5704 - starts_0_loss: 1.4635 - stops_0_loss: 1.5706 - starts_1_loss: 2.5312 - stops_1_loss: 3.0085 - starts_0_accuracy: 0.5038 - stops_0_accuracy: 0.4977 - starts_1_accuracy: 0.3398 - stops_1_accuracy: 0.3188 - val_loss: 9.1181 - val_starts_0_loss: 1.6677 - val_stops_0_loss: 1.6775 - val_starts_1_loss: 2.6678 - val_stops_1_loss: 3.1229 - val_starts_0_accuracy: 0.4247 - val_stops_0_accuracy: 0.4307 - val_starts_1_accuracy: 0.3404 - val_stops_1_accuracy: 0.2470\n",
      "Epoch 3/3\n",
      "665/665 [==============================] - 14s 21ms/sample - loss: 8.4553 - starts_0_loss: 1.3715 - stops_0_loss: 1.5575 - starts_1_loss: 2.5244 - stops_1_loss: 2.9949 - starts_0_accuracy: 0.5398 - stops_0_accuracy: 0.4782 - starts_1_accuracy: 0.3398 - stops_1_accuracy: 0.3203 - val_loss: 9.0915 - val_starts_0_loss: 1.6568 - val_stops_0_loss: 1.6702 - val_starts_1_loss: 2.6643 - val_stops_1_loss: 3.1184 - val_starts_0_accuracy: 0.4247 - val_stops_0_accuracy: 0.4488 - val_starts_1_accuracy: 0.3404 - val_stops_1_accuracy: 0.2470\n",
      "[INFO]  =============== Validation for FOLD# 2 ===============\n",
      "[INFO] 60.30 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 58.35 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 42.47 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 44.88 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 51.36 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 50.06 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 39.00 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 41.55 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 53.40 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 49.15 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 46.07 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 41.02 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 54.63 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 54.67 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 37.74 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 49.00 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V29_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V29_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V29_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V29_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (665, 56) (665, 56)\n",
      "[INFO] Prediction shape for validation data:  (332, 56) (332, 56)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  521 out of 665\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  274 out of 332\n",
      "[INFO] Training Jaccard Score:  0.5398849611649257\n",
      "[INFO] Validation Jaccard Score:  0.5241742699638663\n",
      "[INFO] Training for fold: 2 finished at Thu Jun  4 21:02:40 2020\n",
      "Thu Jun  4 21:02:40 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for num, (t_index, v_index) in enumerate(span_kf.split(X_span, Y_span_stops)):\n",
    "    print(\"[INFO] ==================== FOLD#\", num, \"====================\")\n",
    "    \n",
    "    if num > 0:\n",
    "        del span_history\n",
    "        del span_detection_model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    \n",
    "    span_detection_model = span_detection_model_bkup\n",
    "    span_detection_model.load_weights(RESULTS_DIR+\"FinalSentimentModel.h5\")\n",
    "    \n",
    "    span_mcp = ModelCheckpoint(filepath=RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\", monitor='val_loss',\n",
    "                               verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "    \n",
    "    span_csvl = CSVLogger(filename=RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(num)+\".csv\",\n",
    "                          separator=\",\",\n",
    "                          append=True)\n",
    "    \n",
    "    print(\"[INFO] Training only the final layers at higher learning rates.\")\n",
    "    span_detection_model.layers[3].trainable = False\n",
    "    adam = Adam(learning_rate=MAX_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[0],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl])\n",
    "    \n",
    "    print(\"[INFO] Training only the final layers at lower learning rates.\")\n",
    "    adam = Adam(learning_rate=MID_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[1],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl])\n",
    "    \n",
    "    print(\"[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\")\n",
    "    span_detection_model.layers[3].trainable = True\n",
    "    adam = Adam(learning_rate=MIN_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[2],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl])\n",
    "    \n",
    "    # Loading best weights per fold\n",
    "    span_detection_model.load_weights(RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\")\n",
    "    \n",
    "    pred_train = span_detection_model.predict(x = {\"att_flags\":X_span_att[t_index],\n",
    "                                                   \"words\":X_span[t_index],\n",
    "                                                   \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                              batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "    pred_val = span_detection_model.predict(x = {\"att_flags\":X_span_att[v_index],\n",
    "                                                 \"words\":X_span[v_index],\n",
    "                                                 \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                            batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    pred_starts_train, pred_stops_train = (pred_train[0]/2.0 + pred_train[2]/2.0), (pred_train[1]/2.0 + pred_train[3]/2.0)\n",
    "    pred_starts_val, pred_stops_val = (pred_val[0]/2.0 + pred_val[2]/2.0), (pred_val[1]/2.0 + pred_val[3]/2.0)\n",
    "    \n",
    "    # Accumulate test results after training every fold\n",
    "    pred_test_fold = span_detection_model.predict(x = {\"att_flags\":X_span_att_test,\n",
    "                                                       \"words\":X_span_test,\n",
    "                                                       \"token_ids\":np.zeros_like(X_span_att_test)},\n",
    "                                                  batch_size=PREDICT_BATCH_SIZE)\n",
    "    if num==0:\n",
    "        pred_test = []\n",
    "        pred_test.append(pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test.append(pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    else:\n",
    "        pred_test[0] += (pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test[1] += (pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    \n",
    "    # Tabulate\n",
    "    preds = {\n",
    "        \"train\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_span_starts[t_index],\n",
    "                \"y_pred\":pred_train[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_span_stops[t_index],\n",
    "                \"y_pred\":pred_train[1].argmax(axis=1)\n",
    "            }\n",
    "        },\n",
    "        \"valid\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_span_starts[v_index],\n",
    "                \"y_pred\":pred_val[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_span_stops[v_index],\n",
    "                \"y_pred\":pred_val[1].argmax(axis=1)\n",
    "            }        \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print_metrics(pred_dict=preds)\n",
    "\n",
    "    print(\"[INFO] Prediction shape for training data: \", pred_starts_train.shape, pred_stops_train.shape)\n",
    "    print(\"[INFO] Prediction shape for validation data: \", pred_starts_val.shape, pred_stops_val.shape)\n",
    "\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for training data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_train.argmax(axis=1),\n",
    "                                  pred_stops_train.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_train.shape[0])\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for validation data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_val.argmax(axis=1),\n",
    "                                  pred_stops_val.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_val.shape[0],)\n",
    "\n",
    "    pred_words_train = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[t_index],\n",
    "                                                                                                                                   pred_starts_train.argmax(axis=1),\n",
    "                                                                                                                                   pred_stops_train.argmax(axis=1))]\n",
    "    \n",
    "    pred_words_val = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[v_index],\n",
    "                                                                                                                                 pred_starts_val.argmax(axis=1),\n",
    "                                                                                                                                 pred_stops_val.argmax(axis=1))]\n",
    "\n",
    "    print(\"[INFO] Training Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in t_index],\n",
    "                                                          pred_words_train)]))\n",
    "    print(\"[INFO] Validation Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in v_index],\n",
    "                                                          pred_words_val)]))\n",
    "    print(\"[INFO] Training for fold:\", num, \"finished at\", ctime(time()))\n",
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Prediction shape for testing data:  (1000, 56) (1000, 56)\n"
     ]
    }
   ],
   "source": [
    "pred_starts_test, pred_stops_test = pred_test[0]/NUM_FOLDS, pred_test[1]/NUM_FOLDS\n",
    "print(\"[INFO] Prediction shape for testing data: \", pred_starts_test.shape, pred_stops_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal predictions (StartIndex less than EndIndex) for testing data: 863 out of 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal predictions (StartIndex less than EndIndex) for testing data:\",\n",
    "      sum([s<e for s,e in zip(pred_starts_test.argmax(axis=1),\n",
    "                              pred_stops_test.argmax(axis=1))]), \n",
    "      \"out of\",\n",
    "      pred_starts_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words_test = [\n",
    "    post_process(tokenizer.decode(t[s:e+1])) if s<e else post_process(tokenizer.decode(t[e:])) for t,s,e in zip(X_span_test,\n",
    "                                                                                                              pred_starts_test.argmax(axis=1),\n",
    "                                                                                                              pred_stops_test.argmax(axis=1))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sun has been making me happy =] shame about work tonight  x   neutral \n",
      "1\n",
      "6\n",
      "sun has been making me happy\n"
     ]
    }
   ],
   "source": [
    "check_idx = 158\n",
    "#print([[t,i,j,k] for t,i,j,k in zip(tokenizer.decode(),X_test[check_idx],pred_starts_test[check_idx],pred_stops_test[check_idx])])\n",
    "print(tokenizer.decode(X_span_test[check_idx]))\n",
    "print(pred_starts_test.argmax(axis=1)[check_idx])\n",
    "print(pred_stops_test.argmax(axis=1)[check_idx])\n",
    "print(post_process(tokenizer.decode(X_span_test[check_idx][pred_starts_test.argmax(axis=1)[check_idx]:1+pred_stops_test.argmax(axis=1)[check_idx]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span['selected_text'] = pred_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[\"selected_text\"] = np.where(test_df_span[\"sentiment\"] == \"neutral\",\n",
    "                                         test_df_span[\"text\"],\n",
    "                                         test_df_span[\"selected_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[[\"textID\", \"selected_text\"]].to_csv(RESULTS_DIR+\"submission_\"+MODEL_PREFIX+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>you know I love you.</td>\n",
       "      <td>positive</td>\n",
       "      <td>love you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>is pretty dang tired. but chambers class is for napping.</td>\n",
       "      <td>negative</td>\n",
       "      <td>is pretty dang tired. but chambers class is for napping.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>: Awesome. Tell me if its good. I might watch it</td>\n",
       "      <td>positive</td>\n",
       "      <td>awesome. tell me if its good. i might watch it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>i see you hannah klein!. lookin good today</td>\n",
       "      <td>positive</td>\n",
       "      <td>i see you hannah klein!. lookin good today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>I feel for you, that sounds like how Kiya was last night  all I can say is get comfy on the couch and enjoy the cuddles!</td>\n",
       "      <td>positive</td>\n",
       "      <td>enjoy the cuddles!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Oh! I ate pizza last night too!  I stupidly feel closer to you somehow!</td>\n",
       "      <td>positive</td>\n",
       "      <td>oh! i ate pizza last night too!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>is awake.  Still feeling sick.     But I got the blog done at least. http://plurk.com/p/x17t9</td>\n",
       "      <td>positive</td>\n",
       "      <td>sick.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>oh ok  well im sendin lots of love ****</td>\n",
       "      <td>positive</td>\n",
       "      <td>oh ok well im sendin lots of love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>I love Google`s Mothers day theme  http://tr.im/jtl8</td>\n",
       "      <td>positive</td>\n",
       "      <td>i love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>i always have those for my Champions League parties  Tis awesome</td>\n",
       "      <td>positive</td>\n",
       "      <td>awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>Thanks mama ! I absolutely adore her</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks mama ! i absolutely adore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>I love the video for Da Funk by Daft Punk</td>\n",
       "      <td>positive</td>\n",
       "      <td>i love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>omfg. one of the worst days ever!</td>\n",
       "      <td>negative</td>\n",
       "      <td>worst days ever!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>nothing better than to have your Grandaughter smile and nothing feels better than that big hug!! Grandkids, what a hoot!</td>\n",
       "      <td>positive</td>\n",
       "      <td>what a hoot!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>What a glorious week. My best holiday ever I think. I so don`t want to go home in the morning</td>\n",
       "      <td>positive</td>\n",
       "      <td>what a glorious week. my best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>it was an awesome talk  find it very true that i am watching the ruby community closely</td>\n",
       "      <td>positive</td>\n",
       "      <td>it was an awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>Yes, I would love some more cowbell!</td>\n",
       "      <td>positive</td>\n",
       "      <td>yes, i would love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>browsin thru the videos in my multiply and i saw the video u sang with my bf. i miss you, N. we all miss you. pls come back wherever u r.</td>\n",
       "      <td>negative</td>\n",
       "      <td>miss you, n. we all miss you. pls come back wherever u r.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is SOOOOO hungry right now! Should`ve eaten before this wedding.</td>\n",
       "      <td>negative</td>\n",
       "      <td>is sooooo hungry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>i know they are delicious.miss germany but like holland as well as we used to shoppin there at weekends.lol</td>\n",
       "      <td>positive</td>\n",
       "      <td>delicious.miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>It`s already over? ****!</td>\n",
       "      <td>negative</td>\n",
       "      <td>it`s already over? ****!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>48 Laws of Power by Robert Greene. It`s a little bit much for me, but is anyone interested in a copy? DM me if you are</td>\n",
       "      <td>positive</td>\n",
       "      <td>48 laws of power by robert greene. it`s a little bit much for me,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>I talked to Kat, that is crappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>crappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>I`m feel deflated. Ugh. No more dog.</td>\n",
       "      <td>negative</td>\n",
       "      <td>ugh. no more dog.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>Happy Mother`s Day to everyone`s mothers, mothers-to-be, grandmothers, great grandmothers...All the WOMEN! I salute you all</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy mother`s day to everyone`s mothers, mothers-to-be, grandmothers, great grandmothers...all the women! i salute you all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          text  \\\n",
       "705                                                                                                                       you know I love you.   \n",
       "275                                                                                   is pretty dang tired. but chambers class is for napping.   \n",
       "354                                                                                           : Awesome. Tell me if its good. I might watch it   \n",
       "840                                                                                                 i see you hannah klein!. lookin good today   \n",
       "613                   I feel for you, that sounds like how Kiya was last night  all I can say is get comfy on the couch and enjoy the cuddles!   \n",
       "107                                                                    Oh! I ate pizza last night too!  I stupidly feel closer to you somehow!   \n",
       "524                                              is awake.  Still feeling sick.     But I got the blog done at least. http://plurk.com/p/x17t9   \n",
       "445                                                                                                    oh ok  well im sendin lots of love ****   \n",
       "406                                                                                       I love Google`s Mothers day theme  http://tr.im/jtl8   \n",
       "180                                                                           i always have those for my Champions League parties  Tis awesome   \n",
       "583                                                                                                       Thanks mama ! I absolutely adore her   \n",
       "898                                                                                                  I love the video for Da Funk by Daft Punk   \n",
       "906                                                                                                          omfg. one of the worst days ever!   \n",
       "929                   nothing better than to have your Grandaughter smile and nothing feels better than that big hug!! Grandkids, what a hoot!   \n",
       "120                                              What a glorious week. My best holiday ever I think. I so don`t want to go home in the morning   \n",
       "779                                                    it was an awesome talk  find it very true that i am watching the ruby community closely   \n",
       "851                                                                                                       Yes, I would love some more cowbell!   \n",
       "876  browsin thru the videos in my multiply and i saw the video u sang with my bf. i miss you, N. we all miss you. pls come back wherever u r.   \n",
       "0                                                                             is SOOOOO hungry right now! Should`ve eaten before this wedding.   \n",
       "966                                i know they are delicious.miss germany but like holland as well as we used to shoppin there at weekends.lol   \n",
       "683                                                                                                                   It`s already over? ****!   \n",
       "469                     48 Laws of Power by Robert Greene. It`s a little bit much for me, but is anyone interested in a copy? DM me if you are   \n",
       "572                                                                                                            I talked to Kat, that is crappy   \n",
       "822                                                                                                       I`m feel deflated. Ugh. No more dog.   \n",
       "636                Happy Mother`s Day to everyone`s mothers, mothers-to-be, grandmothers, great grandmothers...All the WOMEN! I salute you all   \n",
       "\n",
       "    sentiment  \\\n",
       "705  positive   \n",
       "275  negative   \n",
       "354  positive   \n",
       "840  positive   \n",
       "613  positive   \n",
       "107  positive   \n",
       "524  positive   \n",
       "445  positive   \n",
       "406  positive   \n",
       "180  positive   \n",
       "583  positive   \n",
       "898  positive   \n",
       "906  negative   \n",
       "929  positive   \n",
       "120  positive   \n",
       "779  positive   \n",
       "851  positive   \n",
       "876  negative   \n",
       "0    negative   \n",
       "966  positive   \n",
       "683  negative   \n",
       "469  positive   \n",
       "572  negative   \n",
       "822  negative   \n",
       "636  positive   \n",
       "\n",
       "                                                                                                                   selected_text  \n",
       "705                                                                                                                    love you.  \n",
       "275                                                                     is pretty dang tired. but chambers class is for napping.  \n",
       "354                                                                               awesome. tell me if its good. i might watch it  \n",
       "840                                                                                   i see you hannah klein!. lookin good today  \n",
       "613                                                                                                           enjoy the cuddles!  \n",
       "107                                                                                              oh! i ate pizza last night too!  \n",
       "524                                                                                                                        sick.  \n",
       "445                                                                                            oh ok well im sendin lots of love  \n",
       "406                                                                                                                       i love  \n",
       "180                                                                                                                      awesome  \n",
       "583                                                                                             thanks mama ! i absolutely adore  \n",
       "898                                                                                                                       i love  \n",
       "906                                                                                                             worst days ever!  \n",
       "929                                                                                                                 what a hoot!  \n",
       "120                                                                                                what a glorious week. my best  \n",
       "779                                                                                                            it was an awesome  \n",
       "851                                                                                                            yes, i would love  \n",
       "876                                                                    miss you, n. we all miss you. pls come back wherever u r.  \n",
       "0                                                                                                               is sooooo hungry  \n",
       "966                                                                                                               delicious.miss  \n",
       "683                                                                                                     it`s already over? ****!  \n",
       "469                                                            48 laws of power by robert greene. it`s a little bit much for me,  \n",
       "572                                                                                                                       crappy  \n",
       "822                                                                                                            ugh. no more dog.  \n",
       "636  happy mother`s day to everyone`s mothers, mothers-to-be, grandmothers, great grandmothers...all the women! i salute you all  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_span.loc[test_df_span.sentiment!=\"neutral\"][[\"text\", \"sentiment\",\"selected_text\"]].sample(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
