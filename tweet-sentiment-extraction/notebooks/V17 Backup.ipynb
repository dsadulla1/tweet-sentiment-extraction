{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V17\"\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "TRAIN_SPLIT_RATIO = 0.8\n",
    "\n",
    "DROPOUT = 0.3\n",
    "MIN_LR = 1e-6\n",
    "MAX_LR = 1e-3\n",
    "BATCH_SIZE = 1024\n",
    "PREDICT_BATCH_SIZE = 2048\n",
    "STEP_SIZE = 10\n",
    "CLR_METHOD = \"triangular\" # exp_range, triangular, triangular2\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "import pickle, os, sys, re\n",
    "from time import time, ctime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, MaxPooling1D, Layer, AdditiveAttention, Attention\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "\n",
    "import tokenizers\n",
    "from transformers import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "np.random.seed(54321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  4 22:47:41 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  1\n",
      "selected_text  object  1\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27480, 'selected_text': 22463, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count   27481        \n",
      "unique  27481        \n",
      "top     7b8a357dcf   \n",
      "freq    1            \n",
      "\n",
      "                                                                 text  \\\n",
      "count   27480                                                           \n",
      "unique  27480                                                           \n",
      "top     I think I might fall in love with jihoon in boys over flower.   \n",
      "freq    1                                                               \n",
      "\n",
      "       selected_text sentiment  \n",
      "count   27480         27481     \n",
      "unique  22463         3         \n",
      "top     good          neutral   \n",
      "freq    199           11118     \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1   I`d have responded, if I were going             \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going  neutral   \n",
       "1  Sooo SAD                             negative  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\",\n",
    "                 encoding=\"utf8\")\n",
    "\n",
    "print(pd.concat((df.dtypes, df.isna().sum()), axis=1))\n",
    "print(df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df[i].nunique() for i in df.columns})\n",
    "print(df.describe()) #.astype(int)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count   3534         \n",
      "unique  3534         \n",
      "top     2fac1b845b   \n",
      "freq    1            \n",
      "\n",
      "                                                                                                                                      text  \\\n",
      "count   3534                                                                                                                                 \n",
      "unique  3534                                                                                                                                 \n",
      "top     Is at work....  boo!  The Pin-Ups will be playing at Uncommon Ground Wednesday night at 9 p.m.! (1401 W. Devon Ave., Chicago 60660   \n",
      "freq    1                                                                                                                                    \n",
      "\n",
      "       sentiment  \n",
      "count   3534      \n",
      "unique  3         \n",
      "top     neutral   \n",
      "freq    1430      \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0  Last session of the day  http://twitpic.com/67ezh                                                         \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0  neutral   \n",
       "1  positive  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "print(pd.concat((test_df.dtypes, test_df.isna().sum()), axis=1))\n",
    "print(test_df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df[i].nunique() for i in test_df.columns})\n",
    "print(test_df.describe())\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>c77717b103</td>\n",
       "      <td>I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½</td>\n",
       "      <td>I love to!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>28dbada620</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "44   c77717b103   \n",
       "192  28dbada620   \n",
       "\n",
       "                                                                                                       text  \\\n",
       "44    I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½   \n",
       "192   *phew*  Will make a note in case anyone else runs into the same issueï¿½                                \n",
       "\n",
       "                                                                selected_text  \\\n",
       "44   I love to!                                                                 \n",
       "192  *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "    sentiment  \n",
       "44   positive  \n",
       "192  neutral   "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>7223fdccc2</td>\n",
       "      <td>tikcets are only ï¿½91...each...BUT I SO WANT TO GO</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>43ad351369</td>\n",
       "      <td>AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "145  7223fdccc2   \n",
       "618  43ad351369   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "145  tikcets are only ï¿½91...each...BUT I SO WANT TO GO                                                                                              \n",
       "618  AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s   \n",
       "\n",
       "    sentiment  \n",
       "145  positive  \n",
       "618  positive  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[test_df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment count in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>7781</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>11117</td>\n",
       "      <td>1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>8582</td>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text  text\n",
       "sentiment             \n",
       "negative   7781   1001\n",
       "neutral    11117  1430\n",
       "positive   8582   1103"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df.groupby(\"sentiment\")[[\"text\"]].count(), test_df.groupby(\"sentiment\")[[\"text\"]].count()], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For traceability\n",
    "df[\"original_index\"] = df.index\n",
    "test_df[\"original_index\"] = test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_idxs = [18, 27, 32, 39, 48, 49, 64, 66, 84, 92, 102, 116, 129, 132, 134, 149, 160, 166, 168, 189, 196, 197, 210, 223, 247, 251, 260, 285, 295, 296, 297, 309, 349, 362, 366, 382, 396, 398, 406, 410, 425, 458, 460, 482, 492, 500, 504, 517, 533, 537, 543, 569, 573, 581, 604, 605, 608, 634, 636, 637, 639, 642, 665, 670, 674, 678, 685, 702, 707, 709, 710, 727, 737, 746, 751, 757, 768, 778, 787, 794, 804, 809, 853, 854, 856, 863, 871, 872, 912, 931, 937, 942, 950, 956, 957, 963, 992, 993, 999, 1001, 1011, 1012, 1036, 1039, 1057, 1062, 1076, 1077, 1083, 1096, 1105, 1116, 1117, 1122, 1134, 1137, 1150, 1159, 1199, 1200, 1209, 1217, 1264, 1271, 1280, 1283, 1298, 1300, 1303, 1319, 1327, 1342, 1360, 1363, 1365, 1372, 1374, 1376, 1382, 1393, 1416, 1417, 1420, 1447, 1454, 1468, 1472, 1513, 1515, 1518, 1531, 1548, 1558, 1567, 1580, 1588, 1605, 1623, 1648, 1649, 1657, 1659, 1661, 1665, 1672, 1674, 1678, 1683, 1693, 1696, 1713, 1723, 1728, 1739, 1747, 1754, 1798, 1806, 1808, 1814, 1822, 1828, 1830, 1835, 1850, 1857, 1860, 1866, 1888, 1901, 1902, 1929, 1938, 1940, 1987, 1994, 2003, 2004, 2008, 2011, 2023, 2054, 2056, 2065, 2068, 2083, 2086, 2094, 2106, 2113, 2124, 2125, 2136, 2139, 2145, 2168, 2180, 2181, 2187, 2213, 2225, 2227, 2230, 2239, 2273, 2279, 2286, 2288, 2298, 2323, 2324, 2335, 2343, 2359, 2364, 2371, 2385, 2388, 2392, 2398, 2401, 2404, 2410, 2415, 2421, 2430, 2438, 2439, 2440, 2444, 2445, 2448, 2470, 2478, 2489, 2499, 2505, 2515, 2520, 2559, 2574, 2576, 2589, 2598, 2600, 2640, 2643, 2650, 2671, 2679, 2696, 2699, 2706, 2716, 2718, 2731, 2743, 2749, 2750, 2785, 2787, 2789, 2805, 2809, 2825, 2838, 2856, 2888, 2898, 2903, 2923, 2925, 2931, 2933, 2942, 2957, 2961, 2986, 2995, 3034, 3068, 3075, 3077, 3080, 3088, 3089, 3096, 3097, 3101, 3121, 3147, 3160, 3161, 3176, 3182, 3187, 3211, 3225, 3234, 3257, 3263, 3267, 3281, 3290, 3295, 3317, 3319, 3325, 3333, 3359, 3364, 3369, 3374, 3390, 3399, 3400, 3411, 3424, 3473, 3475, 3476, 3479, 3492, 3499, 3504, 3509, 3517, 3519, 3532, 3553, 3578, 3588, 3590, 3593, 3598, 3605, 3606, 3607, 3622, 3634, 3669, 3672, 3678, 3682, 3685, 3694, 3708, 3720, 3729, 3750, 3755, 3784, 3794, 3798, 3801, 3806, 3807, 3813, 3826, 3834, 3872, 3886, 3895, 3903, 3925, 3937, 3946, 3962, 3969, 3998, 4031, 4047, 4053, 4074, 4091, 4093, 4100, 4108, 4113, 4119, 4143, 4148, 4163, 4164, 4183, 4196, 4254, 4260, 4265, 4307, 4308, 4309, 4312, 4318, 4325, 4327, 4328, 4329, 4344, 4347, 4369, 4371, 4377, 4384, 4402, 4404, 4405, 4407, 4412, 4421, 4422, 4437, 4455, 4457, 4471, 4473, 4479, 4483, 4486, 4514, 4532, 4537, 4542, 4543, 4545, 4550, 4554, 4563, 4575, 4576, 4591, 4595, 4631, 4636, 4637, 4639, 4654, 4655, 4704, 4706, 4714, 4718, 4724, 4729, 4735, 4747, 4749, 4750, 4764, 4800, 4814, 4841, 4842, 4844, 4864, 4866, 4869, 4875, 4891, 4908, 4918, 4924, 4928, 4933, 4943, 4946, 4947, 4956, 4968, 4980, 4994, 5009, 5011, 5038, 5056, 5079, 5083, 5101, 5133, 5167, 5173, 5174, 5188, 5189, 5196, 5197, 5213, 5218, 5229, 5241, 5245, 5258, 5279, 5290, 5294, 5297, 5308, 5317, 5331, 5342, 5343, 5353, 5355, 5356, 5358, 5379, 5385, 5386, 5399, 5418, 5433, 5436, 5450, 5456, 5466, 5489, 5504, 5509, 5510, 5530, 5542, 5560, 5583, 5603, 5607, 5618, 5620, 5624, 5626, 5678, 5697, 5712, 5725, 5751, 5757, 5775, 5785, 5799, 5822, 5832, 5836, 5844, 5895, 5902, 5904, 5913, 5918, 5938, 5939, 5961, 5984, 5990, 6025, 6031, 6033, 6060, 6077, 6084, 6090, 6113, 6131, 6134, 6151, 6152, 6176, 6203, 6229, 6230, 6248, 6261, 6277, 6289, 6296, 6304, 6308, 6317, 6319, 6321, 6335, 6338, 6343, 6353, 6356, 6360, 6377, 6382, 6395, 6404, 6425, 6433, 6468, 6474, 6475, 6476, 6477, 6510, 6528, 6540, 6574, 6580, 6582, 6593, 6609, 6627, 6628, 6633, 6635, 6650, 6656, 6657, 6670, 6673, 6676, 6677, 6679, 6686, 6702, 6724, 6740, 6745, 6759, 6801, 6804, 6830, 6833, 6834, 6842, 6859, 6871, 6875, 6878, 6880, 6885, 6896, 6934, 6939, 6948, 6950, 6958, 6970, 6979, 6987, 7010, 7017, 7024, 7040, 7099, 7107, 7112, 7136, 7147, 7149, 7158, 7184, 7225, 7227, 7251, 7274, 7288, 7305, 7306, 7308, 7331, 7344, 7355, 7373, 7407, 7408, 7409, 7410, 7423, 7436, 7438, 7442, 7444, 7450, 7469, 7473, 7479, 7487, 7492, 7506, 7513, 7521, 7528, 7530, 7546, 7567, 7581, 7582, 7597, 7599, 7602, 7632, 7635, 7636, 7642, 7662, 7663, 7701, 7752, 7758, 7765, 7769, 7778, 7780, 7788, 7801, 7816, 7818, 7831, 7836, 7837, 7849, 7893, 7895, 7896, 7922, 7927, 7945, 7948, 7968, 7997, 8003, 8005, 8010, 8030, 8049, 8051, 8052, 8059, 8060, 8085, 8094, 8142, 8143, 8151, 8153, 8161, 8169, 8170, 8196, 8208, 8212, 8215, 8224, 8235, 8241, 8249, 8250, 8276, 8282, 8285, 8286, 8288, 8289, 8299, 8314, 8338, 8343, 8345, 8349, 8373, 8379, 8390, 8397, 8399, 8407, 8427, 8428, 8432, 8439, 8442, 8444, 8484, 8493, 8497, 8500, 8507, 8510, 8534, 8546, 8559, 8569, 8579, 8588, 8594, 8604, 8617, 8620, 8624, 8631, 8638, 8661, 8671, 8674, 8687, 8691, 8696, 8706, 8720, 8729, 8749, 8756, 8758, 8769, 8774, 8780, 8800, 8803, 8809, 8815, 8827, 8830, 8875, 8902, 8906, 8910, 8917, 8919, 8922, 8951, 8954, 8968, 8985, 8986, 8991, 8999, 9014, 9034, 9050, 9057, 9063, 9070, 9088, 9098, 9112, 9113, 9122, 9127, 9140, 9155, 9157, 9169, 9179, 9190, 9216, 9223, 9226, 9264, 9272, 9281, 9297, 9300, 9303, 9322, 9354, 9361, 9374, 9377, 9381, 9385, 9400, 9403, 9419, 9422, 9425, 9428, 9436, 9442, 9449, 9454, 9456, 9458, 9470, 9482, 9496, 9500, 9535, 9539, 9541, 9554, 9556, 9560, 9565, 9574, 9594, 9605, 9610, 9631, 9632, 9660, 9689, 9691, 9696, 9708, 9710, 9712, 9725, 9726, 9734, 9742, 9753, 9770, 9789, 9799, 9801, 9812, 9817, 9836, 9839, 9850, 9852, 9881, 9882, 9891, 9906, 9912, 9920, 9922, 9928, 9976, 9980, 9984, 10002, 10004, 10007, 10029, 10039, 10042, 10050, 10052, 10057, 10067, 10070, 10092, 10093, 10123, 10131, 10138, 10145, 10151, 10164, 10167, 10170, 10179, 10184, 10185, 10190, 10197, 10200, 10211, 10218, 10219, 10222, 10228, 10245, 10247, 10255, 10259, 10276, 10293, 10298, 10299, 10306, 10319, 10327, 10342, 10348, 10377, 10379, 10449, 10465, 10469, 10492, 10504, 10505, 10508, 10521, 10528, 10529, 10530, 10570, 10573, 10577, 10578, 10597, 10611, 10639, 10651, 10666, 10672, 10675, 10690, 10706, 10711, 10738, 10742, 10747, 10749, 10779, 10782, 10783, 10795, 10813, 10849, 10852, 10853, 10884, 10885, 10888, 10892, 10901, 10914, 10918, 10919, 10933, 10935, 10956, 10966, 10968, 10970, 10981, 10986, 10996, 10999, 11005, 11032, 11038, 11051, 11055, 11060, 11067, 11097, 11102, 11155, 11157, 11159, 11162, 11167, 11176, 11187, 11215, 11222, 11228, 11230, 11257, 11263, 11265, 11274, 11286, 11289, 11297, 11300, 11308, 11323, 11337, 11339, 11341, 11347, 11349, 11353, 11371, 11381, 11386, 11426, 11431, 11448, 11449, 11474, 11480, 11500, 11507, 11553, 11562, 11582, 11588, 11594, 11612, 11616, 11618, 11636, 11643, 11650, 11658, 11666, 11679, 11683, 11693, 11695, 11698, 11699, 11706, 11716, 11723, 11730, 11732, 11741, 11745, 11780, 11794, 11798, 11808, 11812, 11828, 11829, 11837, 11840, 11841, 11844, 11849, 11853, 11854, 11855, 11861, 11862, 11872, 11875, 11878, 11886, 11890, 11899, 11905, 11912, 11938, 11963, 11967, 11973, 11980, 11985, 11999, 12026, 12027, 12029, 12039, 12054, 12067, 12115, 12124, 12134, 12136, 12138, 12150, 12187, 12205, 12206, 12224, 12242, 12258, 12269, 12277, 12283, 12295, 12305, 12314, 12322, 12331, 12333, 12334, 12350, 12355, 12356, 12360, 12370, 12372, 12389, 12395, 12396, 12397, 12405, 12410, 12416, 12429, 12439, 12440, 12442, 12446, 12474, 12482, 12486, 12507, 12516, 12521, 12522, 12526, 12527, 12537, 12550, 12563, 12576, 12585, 12586, 12587, 12598, 12606, 12630, 12631, 12635, 12639, 12650, 12657, 12662, 12669, 12687, 12703, 12718, 12724, 12732, 12736, 12745, 12767, 12787, 12803, 12804, 12808, 12818, 12830, 12843, 12855, 12856, 12870, 12884, 12903, 12914, 12920, 12923, 12924, 12927, 12936, 12972, 12977, 12998, 13003, 13004, 13009, 13044, 13068, 13069, 13074, 13080, 13095, 13098, 13124, 13130, 13153, 13154, 13176, 13219, 13222, 13227, 13237, 13267, 13274, 13293, 13301, 13304, 13307, 13330, 13333, 13354, 13365, 13373, 13379, 13381, 13389, 13397, 13422, 13430, 13433, 13445, 13466, 13473, 13476, 13482, 13490, 13493, 13528, 13529, 13535, 13543, 13558, 13559, 13586, 13637, 13643, 13646, 13665, 13674, 13678, 13679, 13704, 13707, 13710, 13718, 13727, 13733, 13747, 13762, 13772, 13785, 13796, 13803, 13812, 13814, 13817, 13827, 13844, 13846, 13847, 13848, 13854, 13855, 13861, 13864, 13873, 13899, 13907, 13936, 13939, 13946, 13949, 13954, 13964, 13965, 13974, 13975, 13978, 13980, 14002, 14006, 14007, 14030, 14043, 14044, 14057, 14058, 14060, 14073, 14091, 14109, 14110, 14112, 14117, 14131, 14132, 14159, 14172, 14173, 14176, 14184, 14194, 14197, 14200, 14204, 14207, 14213, 14230, 14257, 14273, 14275, 14287, 14299, 14301, 14307, 14308, 14347, 14362, 14365, 14381, 14386, 14400, 14416, 14428, 14436, 14446, 14457, 14458, 14462, 14487, 14499, 14509, 14532, 14548, 14551, 14571, 14596, 14598, 14611, 14621, 14639, 14640, 14648, 14662, 14669, 14671, 14676, 14680, 14687, 14689, 14691, 14718, 14731, 14747, 14779, 14787, 14825, 14839, 14844, 14847, 14855, 14870, 14880, 14891, 14908, 14946, 14959, 14960, 14967, 14971, 14986, 15007, 15010, 15016, 15044, 15049, 15055, 15056, 15112, 15131, 15158, 15165, 15174, 15177, 15196, 15203, 15206, 15207, 15213, 15217, 15233, 15236, 15253, 15259, 15286, 15296, 15307, 15308, 15321, 15323, 15327, 15339, 15363, 15372, 15374, 15379, 15380, 15383, 15385, 15392, 15410, 15449, 15452, 15462, 15464, 15465, 15477, 15483, 15503, 15505, 15528, 15532, 15548, 15562, 15566, 15571, 15605, 15610, 15622, 15651, 15660, 15666, 15674, 15709, 15721, 15731, 15735, 15736, 15741, 15767, 15774, 15785, 15792, 15793, 15811, 15851, 15879, 15883, 15884, 15912, 15926, 15927, 15931, 15940, 15959, 15965, 15985, 15998, 16020, 16027, 16031, 16032, 16044, 16070, 16100, 16117, 16129, 16136, 16137, 16140, 16149, 16152, 16175, 16186, 16201, 16241, 16261, 16269, 16272, 16279, 16290, 16308, 16319, 16324, 16325, 16327, 16338, 16341, 16345, 16352, 16354, 16371, 16372, 16385, 16391, 16416, 16423, 16448, 16455, 16468, 16470, 16477, 16481, 16484, 16491, 16493, 16500, 16509, 16510, 16516, 16541, 16559, 16577, 16589, 16632, 16638, 16642, 16643, 16649, 16650, 16654, 16665, 16677, 16684, 16704, 16720, 16726, 16734, 16765, 16774, 16775, 16777, 16780, 16797, 16798, 16799, 16803, 16807, 16809, 16830, 16832, 16861, 16863, 16868, 16876, 16889, 16890, 16902, 16915, 16927, 16939, 16949, 16962, 16968, 17002, 17010, 17013, 17021, 17023, 17029, 17062, 17066, 17084, 17093, 17094, 17102, 17152, 17160, 17167, 17173, 17183, 17221, 17239, 17250, 17267, 17277, 17278, 17282, 17296, 17307, 17315, 17333, 17336, 17344, 17346, 17348, 17354, 17365, 17374, 17385, 17387, 17397, 17401, 17404, 17412, 17417, 17432, 17436, 17444, 17452, 17467, 17469, 17501, 17502, 17513, 17530, 17531, 17547, 17550, 17555, 17573, 17577, 17578, 17586, 17600, 17602, 17611, 17627, 17640, 17645, 17648, 17649, 17652, 17667, 17676, 17682, 17721, 17724, 17729, 17731, 17751, 17754, 17762, 17770, 17772, 17781, 17802, 17820, 17823, 17833, 17848, 17857, 17858, 17865, 17867, 17887, 17893, 17909, 17924, 17926, 17945, 17971, 17977, 17986, 17990, 18003, 18011, 18042, 18046, 18052, 18077, 18086, 18091, 18099, 18103, 18123, 18128, 18131, 18142, 18153, 18165, 18167, 18173, 18181, 18206, 18226, 18252, 18263, 18276, 18277, 18284, 18314, 18319, 18327, 18330, 18342, 18352, 18355, 18362, 18364, 18375, 18383, 18398, 18403, 18432, 18447, 18453, 18465, 18467, 18485, 18486, 18492, 18505, 18508, 18510, 18515, 18536, 18540, 18548, 18550, 18557, 18563, 18570, 18574, 18576, 18616, 18622, 18627, 18639, 18649, 18655, 18673, 18681, 18712, 18745, 18746, 18758, 18773, 18778, 18782, 18826, 18832, 18834, 18842, 18851, 18853, 18858, 18862, 18867, 18872, 18877, 18908, 18910, 18912, 18913, 18919, 18925, 18929, 18930, 18933, 18936, 18944, 18974, 18975, 18997, 19009, 19011, 19014, 19016, 19028, 19057, 19098, 19106, 19114, 19130, 19133, 19147, 19157, 19162, 19166, 19173, 19179, 19198, 19213, 19225, 19230, 19239, 19241, 19266, 19274, 19279, 19283, 19285, 19316, 19327, 19335, 19352, 19358, 19362, 19366, 19377, 19379, 19405, 19423, 19428, 19431, 19437, 19444, 19456, 19469, 19475, 19476, 19481, 19486, 19492, 19506, 19507, 19536, 19537, 19562, 19586, 19589, 19622, 19634, 19651, 19663, 19679, 19685, 19690, 19697, 19707, 19712, 19714, 19715, 19717, 19724, 19734, 19744, 19754, 19769, 19779, 19792, 19807, 19830, 19850, 19863, 19875, 19903, 19904, 19908, 19963, 19985, 19988, 20003, 20012, 20014, 20018, 20040, 20042, 20054, 20078, 20079, 20083, 20087, 20089, 20114, 20121, 20122, 20138, 20155, 20162, 20186, 20199, 20216, 20222, 20234, 20238, 20254, 20285, 20299, 20302, 20305, 20337, 20345, 20364, 20365, 20374, 20382, 20394, 20403, 20440, 20450, 20451, 20476, 20506, 20508, 20516, 20527, 20536, 20540, 20541, 20542, 20557, 20566, 20584, 20608, 20611, 20613, 20618, 20627, 20644, 20656, 20663, 20674, 20682, 20690, 20699, 20707, 20711, 20737, 20751, 20765, 20771, 20774, 20778, 20779, 20789, 20795, 20798, 20834, 20851, 20865, 20867, 20869, 20872, 20883, 20892, 20894, 20895, 20896, 20901, 20919, 20924, 20933, 20937, 20938, 20946, 20947, 20955, 20957, 20958, 20965, 20966, 20970, 20974, 20991, 21001, 21013, 21018, 21020, 21025, 21114, 21142, 21205, 21210, 21227, 21230, 21236, 21253, 21262, 21294, 21300, 21316, 21331, 21339, 21349, 21356, 21365, 21369, 21376, 21399, 21410, 21413, 21418, 21420, 21435, 21447, 21452, 21455, 21468, 21470, 21474, 21486, 21495, 21501, 21517, 21530, 21539, 21546, 21547, 21553, 21556, 21564, 21569, 21579, 21593, 21595, 21599, 21603, 21632, 21643, 21690, 21713, 21726, 21732, 21737, 21738, 21744, 21755, 21773, 21782, 21800, 21807, 21829, 21839, 21848, 21876, 21893, 21923, 21926, 21931, 21958, 21967, 21970, 21973, 21982, 22002, 22029, 22031, 22035, 22044, 22051, 22068, 22073, 22099, 22117, 22121, 22126, 22145, 22146, 22149, 22161, 22166, 22170, 22177, 22205, 22210, 22231, 22233, 22234, 22244, 22246, 22251, 22272, 22280, 22299, 22317, 22321, 22327, 22350, 22355, 22363, 22365, 22378, 22383, 22387, 22391, 22393, 22395, 22403, 22406, 22418, 22423, 22428, 22437, 22444, 22451, 22453, 22460, 22480, 22491, 22510, 22530, 22536, 22554, 22557, 22561, 22562, 22564, 22571, 22588, 22593, 22606, 22607, 22610, 22611, 22642, 22666, 22717, 22729, 22738, 22744, 22745, 22763, 22768, 22769, 22774, 22782, 22789, 22796, 22800, 22836, 22841, 22843, 22858, 22864, 22886, 22889, 22897, 22925, 22928, 22938, 22944, 22945, 22948, 22950, 22956, 22983, 22986, 22988, 22993, 22994, 22997, 23028, 23030, 23039, 23047, 23081, 23091, 23097, 23105, 23108, 23111, 23113, 23133, 23145, 23150, 23161, 23172, 23176, 23199, 23200, 23202, 23204, 23205, 23211, 23212, 23226, 23227, 23231, 23236, 23239, 23263, 23265, 23266, 23269, 23290, 23311, 23315, 23320, 23323, 23340, 23352, 23361, 23372, 23375, 23376, 23381, 23386, 23390, 23401, 23415, 23418, 23428, 23429, 23453, 23467, 23479, 23505, 23508, 23527, 23528, 23535, 23537, 23541, 23553, 23555, 23557, 23559, 23563, 23566, 23581, 23585, 23596, 23598, 23604, 23605, 23617, 23630, 23643, 23658, 23669, 23673, 23680, 23683, 23690, 23691, 23701, 23707, 23714, 23720, 23725, 23733, 23746, 23750, 23751, 23769, 23774, 23784, 23809, 23818, 23831, 23839, 23842, 23857, 23924, 23930, 23944, 23945, 23947, 23959, 23971, 23975, 23987, 23989, 24002, 24003, 24009, 24019, 24026, 24039, 24041, 24046, 24062, 24071, 24077, 24079, 24087, 24096, 24119, 24121, 24137, 24150, 24169, 24181, 24194, 24195, 24199, 24200, 24203, 24210, 24223, 24258, 24272, 24274, 24291, 24293, 24300, 24305, 24318, 24339, 24378, 24380, 24383, 24392, 24402, 24442, 24468, 24476, 24480, 24488, 24490, 24495, 24502, 24504, 24505, 24516, 24562, 24572, 24594, 24595, 24597, 24600, 24606, 24609, 24622, 24623, 24629, 24631, 24632, 24633, 24657, 24662, 24664, 24666, 24682, 24697, 24709, 24753, 24758, 24766, 24833, 24840, 24851, 24858, 24869, 24886, 24909, 24915, 24916, 24929, 24945, 24951, 24966, 24972, 24980, 24996, 25012, 25017, 25043, 25059, 25067, 25104, 25121, 25125, 25127, 25133, 25150, 25191, 25210, 25224, 25228, 25240, 25243, 25255, 25264, 25267, 25272, 25282, 25284, 25290, 25292, 25293, 25334, 25338, 25350, 25365, 25368, 25380, 25389, 25391, 25400, 25413, 25422, 25440, 25444, 25446, 25456, 25460, 25472, 25486, 25495, 25499, 25509, 25512, 25528, 25530, 25532, 25547, 25559, 25591, 25593, 25597, 25601, 25614, 25616, 25638, 25672, 25691, 25698, 25712, 25713, 25721, 25722, 25725, 25732, 25753, 25760, 25775, 25794, 25813, 25819, 25828, 25833, 25901, 25908, 25947, 25983, 25987, 25989, 25996, 26008, 26017, 26025, 26030, 26039, 26047, 26054, 26058, 26077, 26088, 26092, 26112, 26183, 26185, 26190, 26201, 26204, 26237, 26239, 26241, 26248, 26251, 26252, 26256, 26263, 26268, 26282, 26314, 26331, 26346, 26358, 26360, 26364, 26377, 26428, 26434, 26460, 26481, 26485, 26497, 26532, 26533, 26538, 26561, 26567, 26585, 26588, 26595, 26596, 26618, 26625, 26633, 26643, 26651, 26677, 26687, 26715, 26721, 26732, 26733, 26746, 26760, 26762, 26769, 26779, 26781, 26822, 26828, 26830, 26870, 26882, 26883, 26889, 26892, 26894, 26914, 26927, 26932, 26944, 26953, 26957, 26963, 26967, 26973, 26976, 26978, 26985, 27030, 27038, 27044, 27048, 27052, 27067, 27111, 27115, 27121, 27142, 27144, 27153, 27167, 27205, 27209, 27217, 27218, 27229, 27233, 27240, 27250, 27280, 27302, 27332, 27339, 27349, 27362, 27376, 27386, 27396, 27401, 27413, 27426, 27429, 27456, 27470, 27474, 27476, 27477, 27480]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27481, 5)\n",
      "(24864, 5)\n",
      "(24863, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df = df[(~df.index.isin(anomalous_idxs))]\n",
    "print(df.shape)\n",
    "df = df[(~df.text.isna())]\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.copy()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment_code\"] = df[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments = df[\"sentiment_code\"].cat.codes.values\n",
    "\n",
    "test_df[\"sentiment_code\"] = test_df[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments_test = test_df[\"sentiment_code\"].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"selected_text\"] = df[\"selected_text\"].astype(str)\n",
    "test_df[\"text\"] = test_df[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_mod\"] = \"<s> \" + df.text + \" </s> \" + df.sentiment + \" </s>\"\n",
    "test_df[\"text_mod\"] = \"<s> \" + test_df.text + \" </s> \" + test_df.sentiment + \" </s>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Shuffle and Train Val Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24863 19890 4973 24863\n"
     ]
    }
   ],
   "source": [
    "n_rows = df.shape[0]\n",
    "idx = [i for i in np.arange(n_rows)]\n",
    "np.random.shuffle(idx)\n",
    "train_idx, val_idx = idx[:round(TRAIN_SPLIT_RATIO * n_rows)], idx[round(TRAIN_SPLIT_RATIO * n_rows):]\n",
    "\n",
    "print(len(idx), len(train_idx), len(val_idx), len(train_idx) + len(val_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"set_name\"] = np.where(df.index.isin(val_idx), \"val\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set_name\n",
       "train    19890\n",
       "val      4973 \n",
       "Name: textID, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"set_name\")[\"textID\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../results/texts\"):\n",
    "    for i in os.listdir(\"../results/texts/\"):\n",
    "        os.remove(\"../results/texts/\"+i)\n",
    "else:\n",
    "    os.mkdir(\"../results/texts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip(df.textID.loc[df.set_name==\"train\"], df.text_mod.loc[df.set_name==\"train\"]):\n",
    "    with open(\"../results/texts/\"+i+\".txt\", \"w+\") as f:\n",
    "        f.write(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "paths = [str(x) for x in Path(\"../results/texts/\").glob(\"**/*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(add_prefix_space=True, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=paths, min_frequency=2, show_progress=True, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"xxxSTART\",\n",
    "    \"xxxSENTIMENT\",\n",
    "    \"xxxEND\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../results/tokenizers/V16-vocab.json',\n",
       " '../results/tokenizers/V16-merges.txt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save(\"../results/tokenizers/\", MODEL_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens = tokenizer.encode_batch(df.text_mod.tolist())\n",
    "Y_tokens = tokenizer.encode_batch(df.selected_text.tolist())\n",
    "X_tokens_test = tokenizer.encode_batch(test_df.text_mod.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i.ids for i in X_tokens]\n",
    "Y = [i.ids for i in Y_tokens]\n",
    "X_test = [i.ids for i in X_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_att = [i.attention_mask for i in X_tokens]\n",
    "Y_att = [i.attention_mask for i in Y_tokens]\n",
    "X_att_test = [i.attention_mask for i in X_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14517 108\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(i) for i in X])\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "print(VOCAB_SIZE, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremities(l_string, s_string, print_it=False):\n",
    "    len_l = len(l_string)\n",
    "    len_s = len(s_string)\n",
    "    \n",
    "    for i in range(len_l - len_s + 1):\n",
    "        if (i + len_s) <= len_l:\n",
    "            substring = l_string[i:i+len_s]\n",
    "            if substring == s_string:\n",
    "                if print_it:\n",
    "                    print(l_string)\n",
    "                    print(substring)\n",
    "                    print(i, i+len_s, substring)\n",
    "                \n",
    "                start_vector, end_vector = [0] * len_l, [0] * len_l\n",
    "                start_vector[i], end_vector[i+len_s-1] = 1, 1\n",
    "                \n",
    "                return (start_vector, end_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24863 \t: #Processed\n",
      "1 \t: # of Anomalies\n"
     ]
    }
   ],
   "source": [
    "Y_starts, Y_stops = [], []\n",
    "anomaly_idx, counter = [], 0\n",
    "for num, (i,j) in enumerate(zip(X_tokens, Y_tokens)):\n",
    "    x,y = i.ids, j.ids\n",
    "    #s,e = get_extremities(x, y)\n",
    "    try:\n",
    "        s,e = get_extremities(x, y)\n",
    "    except TypeError as t:\n",
    "        counter += 1\n",
    "        anomaly_idx.append(num)\n",
    "    Y_starts.append(s)\n",
    "    Y_stops.append(e)\n",
    "print(num + 1, \"\\t: #Processed\")\n",
    "\n",
    "print(counter,\"\\t: # of Anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5001]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>original_index</th>\n",
       "      <th>sentiment_code</th>\n",
       "      <th>text_mod</th>\n",
       "      <th>set_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5001</th>\n",
       "      <td>b507cce1a4</td>\n",
       "      <td>I`m home, Yay! Unpacked everything, now just got to wash it all</td>\n",
       "      <td>Yay! Un</td>\n",
       "      <td>positive</td>\n",
       "      <td>5543</td>\n",
       "      <td>positive</td>\n",
       "      <td>xxxSTART I`m home, Yay! Unpacked everything, now just got to wash it all xxxSENTIMENT positive xxxEND</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID  \\\n",
       "5001  b507cce1a4   \n",
       "\n",
       "                                                                 text  \\\n",
       "5001  I`m home, Yay! Unpacked everything, now just got to wash it all   \n",
       "\n",
       "     selected_text sentiment  original_index sentiment_code  \\\n",
       "5001  Yay! Un       positive  5543            positive        \n",
       "\n",
       "                                                                                                   text_mod  \\\n",
       "5001  xxxSTART I`m home, Yay! Unpacked everything, now just got to wash it all xxxSENTIMENT positive xxxEND   \n",
       "\n",
       "     set_name  \n",
       "5001  val      "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(anomaly_idx, sep=\",\")\n",
    "df.iloc[anomaly_idx].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed inspection. Did you know you can pass wo/oven, but not wo/anti-tip bracket, which is only sold w/oven? This is worse than taxes.\n",
      "This is worse than taxes.\n",
      "[['xxxSTART', 4, 0, 0], ['Ġfailed', 2659, 0, 0], ['Ġinspection', 10359, 0, 0], ['.', 20, 0, 0], ['Ġdid', 510, 0, 0], ['Ġyou', 337, 0, 0], ['Ġknow', 497, 0, 0], ['Ġyou', 337, 0, 0], ['Ġcan', 405, 0, 0], ['Ġpass', 1581, 0, 0], ['Ġwo', 3752, 0, 0], ['/', 21, 0, 0], ['o', 85, 0, 0], ['ven', 603, 0, 0], [',', 18, 0, 0], ['Ġbut', 389, 0, 0], ['Ġnot', 400, 0, 0], ['Ġwo', 3752, 0, 0], ['/', 21, 0, 0], ['anti', 11245, 0, 0], ['-', 19, 0, 0], ['ti', 292, 0, 0], ['p', 86, 0, 0], ['Ġbracket', 12162, 0, 0], [',', 18, 0, 0], ['Ġwhich', 1286, 0, 0], ['Ġis', 349, 0, 0], ['Ġonly', 649, 0, 0], ['Ġsold', 2374, 0, 0], ['Ġw', 287, 0, 0], ['/', 21, 0, 0], ['o', 85, 0, 0], ['ven', 603, 0, 0], ['?', 37, 0, 0], ['Ġthis', 425, 1, 0], ['Ġis', 349, 0, 0], ['Ġworse', 2576, 0, 0], ['Ġthan', 445, 0, 0], ['Ġtaxes', 13510, 0, 0], ['.', 20, 0, 1], ['Ġ', 227, 0, 0], ['xxxSENTIMENT', 5, 0, 0], ['Ġnegative', 328, 0, 0], ['Ġ', 227, 0, 0], ['xxxEND', 6, 0, 0]]\n",
      "[[425, 'Ġthis'], [349, 'Ġis'], [2576, 'Ġworse'], [445, 'Ġthan'], [13510, 'Ġtaxes'], [20, '.']]\n"
     ]
    }
   ],
   "source": [
    "check_idx = 201\n",
    "print(df.text[check_idx])\n",
    "print(df.selected_text[check_idx])\n",
    "print([[i,j,k,l] for i,j,k,l in zip(X_tokens[check_idx].tokens,\n",
    "                                    X_tokens[check_idx].ids,\n",
    "                                    Y_starts[check_idx],\n",
    "                                    Y_stops[check_idx])])\n",
    "print([[i,j] for i,j in zip(Y_tokens[check_idx].ids,\n",
    "                            Y_tokens[check_idx].tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Validation  split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 19890 \t : X_train \n",
      " 19890 \t : X_att_train \n",
      " 19890 \t : Y_train \n",
      " 19890 \t : Y_starts_train \n",
      " 19890 \t : Y_stops_train \n",
      " 4973 \t : X_val \n",
      " 4973 \t : X_att_val \n",
      " 4973 \t : Y_val \n",
      " 4973 \t : Y_starts_val \n",
      " 4973 \t : Y_stops_val \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val = [X[i] for i in train_idx], [X[i] for i in val_idx]\n",
    "X_att_train, X_att_val = [X_att[i] for i in train_idx], [X_att[i] for i in val_idx]\n",
    "\n",
    "Y_train, Y_val = [Y[i] for i in train_idx], [Y[i] for i in val_idx]\n",
    "Y_starts_train, Y_starts_val = [Y_starts[i] for i in train_idx], [Y_starts[i] for i in val_idx]\n",
    "Y_stops_train, Y_stops_val = [Y_stops[i] for i in train_idx], [Y_stops[i] for i in val_idx]\n",
    "\n",
    "print(\"\\n\",\n",
    "    len(X_train),\"\\t\",\": X_train\",\"\\n\",\n",
    "    len(X_att_train),\"\\t\",\": X_att_train\",\"\\n\",\n",
    "    len(Y_train),\"\\t\",\": Y_train\",\"\\n\",\n",
    "    len(Y_starts_train),\"\\t\",\": Y_starts_train\",\"\\n\",\n",
    "    len(Y_stops_train),\"\\t\",\": Y_stops_train\",\"\\n\",\n",
    "    len(X_val),\"\\t\",\": X_val\",\"\\n\",\n",
    "    len(X_att_val),\"\\t\",\": X_att_val\",\"\\n\",\n",
    "    len(Y_val),\"\\t\",\": Y_val\",\"\\n\",\n",
    "    len(Y_starts_val),\"\\t\",\": Y_starts_val\",\"\\n\",\n",
    "    len(Y_stops_val),\"\\t\",\": Y_stops_val\",\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=max_len, padding=\"post\")\n",
    "X_att_train = pad_sequences(X_att_train, maxlen=max_len, padding=\"post\")\n",
    "Y_train = pad_sequences(Y_train, maxlen=max_len, padding=\"post\")\n",
    "Y_starts_train = pad_sequences(Y_starts_train, maxlen=max_len, padding=\"post\")\n",
    "Y_stops_train = pad_sequences(Y_stops_train, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "X_val = pad_sequences(X_val, maxlen=max_len, padding=\"post\")\n",
    "X_att_val = pad_sequences(X_att_val, maxlen=max_len, padding=\"post\")\n",
    "Y_val = pad_sequences(Y_val, maxlen=max_len, padding=\"post\")\n",
    "Y_starts_val = pad_sequences(Y_starts_val, maxlen=max_len, padding=\"post\")\n",
    "Y_stops_val = pad_sequences(Y_stops_val, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding=\"post\")\n",
    "X_att_test = pad_sequences(X_att_test, maxlen=max_len, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (19890, 108) \t: X_train  \n",
      " (19890, 108) \t: X_att_train  \n",
      " (19890, 108) \t: Y_train  \n",
      " (19890, 108) \t: Y_starts_train  \n",
      " (19890, 108) \t: Y_stops_train  \n",
      " (4973, 108) \t: X_val  \n",
      " (4973, 108) \t: X_att_val  \n",
      " (4973, 108) \t: Y_val  \n",
      " (4973, 108) \t: Y_starts_val  \n",
      " (4973, 108) \t: Y_stops_val  \n",
      " (3534, 108) \t: X_test  \n",
      " (3534, 108) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_train.shape, \"\\t: X_train \", \"\\n\",\n",
    "     X_att_train.shape, \"\\t: X_att_train \", \"\\n\",\n",
    "     Y_train.shape, \"\\t: Y_train \", \"\\n\",\n",
    "     Y_starts_train.shape, \"\\t: Y_starts_train \", \"\\n\",\n",
    "     Y_stops_train.shape, \"\\t: Y_stops_train \", \"\\n\",\n",
    "\n",
    "     X_val.shape, \"\\t: X_val \", \"\\n\",\n",
    "     X_att_val.shape, \"\\t: X_att_val \", \"\\n\",\n",
    "     Y_val.shape, \"\\t: Y_val \", \"\\n\",\n",
    "     Y_starts_val.shape, \"\\t: Y_starts_val \", \"\\n\",\n",
    "     Y_stops_val.shape, \"\\t: Y_stops_val \", \"\\n\",\n",
    "\n",
    "     X_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for zero input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1302\n",
      "0 1562\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax([X_train.sum(axis=1)==0]), np.min([X_train.sum(axis=1)]))\n",
    "print(np.argmax([X_val.sum(axis=1)==0]), np.min([X_val.sum(axis=1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 1, 1066, 0, 0],\n",
       " [227, 1, 18, 0, 0],\n",
       " [1066, 1, 312, 1, 0],\n",
       " [18, 1, 1161, 0, 0],\n",
       " [312, 1, 598, 0, 0],\n",
       " [1161, 1, 642, 0, 0],\n",
       " [598, 1, 1219, 0, 0],\n",
       " [642, 1, 1960, 0, 0],\n",
       " [1219, 1, 595, 0, 0],\n",
       " [1960, 1, 20, 0, 0],\n",
       " [595, 1, 0, 0, 0],\n",
       " [20, 1, 0, 0, 1],\n",
       " [339, 1, 0, 0, 0],\n",
       " [2181, 1, 0, 0, 0],\n",
       " [436, 1, 0, 0, 0],\n",
       " [339, 1, 0, 0, 0],\n",
       " [403, 1, 0, 0, 0],\n",
       " [724, 1, 0, 0, 0],\n",
       " [793, 1, 0, 0, 0],\n",
       " [463, 1, 0, 0, 0],\n",
       " [389, 1, 0, 0, 0],\n",
       " [353, 1, 0, 0, 0],\n",
       " [1936, 1, 0, 0, 0],\n",
       " [339, 1, 0, 0, 0],\n",
       " [673, 1, 0, 0, 0],\n",
       " [70, 1, 0, 0, 0],\n",
       " [90, 1, 0, 0, 0],\n",
       " [18, 1, 0, 0, 0],\n",
       " [2947, 1, 0, 0, 0],\n",
       " [706, 1, 0, 0, 0],\n",
       " [18, 1, 0, 0, 0],\n",
       " [227, 1, 0, 0, 0],\n",
       " [5, 1, 0, 0, 0],\n",
       " [328, 1, 0, 0, 0],\n",
       " [227, 1, 0, 0, 0],\n",
       " [6, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train\n",
    "num = 100\n",
    "[[i,j,k,l,m] for i,j,k,l,m in zip(X_train[num],\n",
    "                                  X_att_train[num],\n",
    "                                  Y_train[num],\n",
    "                                  Y_starts_train[num],\n",
    "                                  Y_stops_train[num])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 1, 347, 0, 0],\n",
       " [227, 1, 482, 0, 0],\n",
       " [347, 1, 311, 1, 0],\n",
       " [482, 1, 0, 0, 0],\n",
       " [311, 1, 0, 0, 1],\n",
       " [227, 1, 0, 0, 0],\n",
       " [5, 1, 0, 0, 0],\n",
       " [309, 1, 0, 0, 0],\n",
       " [227, 1, 0, 0, 0],\n",
       " [6, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Val\n",
    "num = 100\n",
    "[[i,j,k,l,m] for i,j,k,l,m in zip(X_val[num],\n",
    "                                  X_att_val[num],\n",
    "                                  Y_val[num],\n",
    "                                  Y_starts_val[num],\n",
    "                                  Y_stops_val[num])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 4, 1],\n",
       " [3876, 3876, 1],\n",
       " [18, 18, 1],\n",
       " [2105, 2105, 1],\n",
       " [7, 7, 1],\n",
       " [282, 282, 1],\n",
       " [8781, 8781, 1],\n",
       " [353, 353, 1],\n",
       " [2105, 2105, 1],\n",
       " [7, 7, 1],\n",
       " [342, 342, 1],\n",
       " [339, 339, 1],\n",
       " [512, 512, 1],\n",
       " [350, 350, 1],\n",
       " [1113, 1113, 1],\n",
       " [647, 647, 1],\n",
       " [312, 312, 1],\n",
       " [495, 495, 1],\n",
       " [271, 271, 1],\n",
       " [1176, 1176, 1],\n",
       " [423, 423, 1],\n",
       " [342, 342, 1],\n",
       " [1882, 1882, 1],\n",
       " [7829, 7829, 1],\n",
       " [394, 394, 1],\n",
       " [793, 793, 1],\n",
       " [20, 20, 1],\n",
       " [227, 227, 1],\n",
       " [1945, 1945, 1],\n",
       " [1380, 1380, 1],\n",
       " [227, 227, 1],\n",
       " [5, 5, 1],\n",
       " [309, 309, 1],\n",
       " [227, 227, 1],\n",
       " [6, 6, 1]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "num = 100\n",
    "[[i,j,k] for i,j,k in zip(X_tokens_test[num].ids,\n",
    "                          X_test[num],\n",
    "                          X_tokens_test[num].attention_mask)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_att_flags = Input((max_len), name=\"att_flags\")\n",
    "input_sequences = Input((max_len), name=\"words\")\n",
    "\n",
    "emb_sequences = Embedding(input_dim=VOCAB_SIZE, input_length=max_len, output_dim=64, mask_zero=True)(input_sequences)\n",
    "\n",
    "seq = Bidirectional(LSTM(32, activation=None, return_sequences=True))(emb_sequences)\n",
    "seq = BatchNormalization()(seq)\n",
    "seq = Activation(\"relu\")(seq)\n",
    "seq = Dropout(DROPOUT)(seq)\n",
    "\n",
    "seq = Bidirectional(LSTM(32, activation=None, return_sequences=True))(seq)\n",
    "seq = BatchNormalization()(seq)\n",
    "seq = Activation(\"relu\")(seq)\n",
    "seq = Dropout(DROPOUT)(seq)\n",
    "\n",
    "seq = Bidirectional(LSTM(32, activation=None, return_sequences=True))(seq)\n",
    "seq = Attention(max_len)(seq)\n",
    "seq = BatchNormalization()(seq)\n",
    "seq = Activation(\"relu\")(seq)\n",
    "seq = Dropout(DROPOUT)(seq)\n",
    "\n",
    "seq = Dense(max_len, activation=\"relu\")(seq)\n",
    "seq = BatchNormalization()(seq)\n",
    "seq = Dropout(DROPOUT)(seq)\n",
    "\n",
    "att = Dense(max_len, activation=\"relu\")(input_att_flags)\n",
    "att = BatchNormalization()(att)\n",
    "att = Dropout(DROPOUT)(att)\n",
    "\n",
    "convs = Conv1D(filters=32, kernel_size=8, padding=\"same\", activation=None)(emb_sequences)\n",
    "convs = BatchNormalization()(convs)\n",
    "convs = Activation(\"relu\")(convs)\n",
    "convs = MaxPooling1D(pool_size=2, strides=1)(convs)\n",
    "convs = Dropout(DROPOUT)(convs)\n",
    "\n",
    "convs = Conv1D(filters=32, kernel_size=8, padding=\"same\", activation=None)(convs)\n",
    "convs = BatchNormalization()(convs)\n",
    "convs = Activation(\"relu\")(convs)\n",
    "convs = MaxPooling1D(pool_size=2, strides=1)(convs)\n",
    "convs = Dropout(DROPOUT)(convs)\n",
    "\n",
    "convs = Conv1D(filters=32, kernel_size=8, padding=\"same\", activation=None)(convs)\n",
    "convs = BatchNormalization()(convs)\n",
    "convs = Activation(\"relu\")(convs)\n",
    "convs = MaxPooling1D(pool_size=2, strides=1)(convs)\n",
    "convs = Dropout(DROPOUT)(convs)\n",
    "\n",
    "convs = Flatten()(convs)\n",
    "convs = Dense(max_len, activation=None)(convs)\n",
    "convs = BatchNormalization()(convs)\n",
    "convs = Activation(\"relu\")(convs)\n",
    "convs = Dropout(DROPOUT)(convs)\n",
    "\n",
    "conv = Multiply()([att, convs])\n",
    "seq = Multiply()([att, seq])\n",
    "concat_layer = concatenate([conv, seq])\n",
    "\n",
    "output_starts = Dense(max_len, activation=None)(concat_layer)\n",
    "output_starts = BatchNormalization()(output_starts)\n",
    "output_starts = Activation(\"relu\")(output_starts)\n",
    "output_starts = Dropout(DROPOUT)(output_starts)\n",
    "\n",
    "output_starts = Dense(max_len, activation=None)(output_starts)\n",
    "output_starts = Activation(\"relu\")(output_starts)\n",
    "\n",
    "output_stops = Dense(max_len, activation=None)(concat_layer)\n",
    "output_stops = BatchNormalization()(output_stops)\n",
    "output_stops = Activation(\"relu\")(output_stops)\n",
    "output_stops = Dropout(DROPOUT)(output_stops)\n",
    "\n",
    "output_stops = Dense(max_len, activation=None)(output_stops)\n",
    "output_stops = Activation(\"relu\")(output_stops)\n",
    "\n",
    "output_starts = Dense(max_len, activation='softmax', name=\"starts\")(output_starts)\n",
    "output_stops = Dense(max_len, activation='softmax', name=\"stops\")(output_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 108)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 108, 64)      929088      words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 108, 32)      16416       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 108, 32)      128         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 108, 32)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 107, 32)      0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 107, 32)      0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 108, 64)      24832       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 107, 32)      8224        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 108, 64)      256         bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 107, 32)      128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 108, 64)      0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 107, 32)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 108, 64)      0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 106, 32)      0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 108, 64)      24832       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 106, 32)      0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 108, 64)      256         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 106, 32)      8224        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 108, 64)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 106, 32)      128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 108, 64)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 106, 32)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 108, 64)      24832       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 105, 32)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 64)           172         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 105, 32)      0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64)           256         attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3360)         0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 108)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 108)          362988      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 108)          11772       att_flags[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 108)          432         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 108)          7020        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 108)          432         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 108)          0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 108)          432         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 108)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 108)          0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 108)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 108)          0           dropout_4[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 108)          0           dropout_4[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 216)          0           multiply[0][0]                   \n",
      "                                                                 multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 108)          23436       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 108)          23436       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 108)          432         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 108)          432         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 108)          0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 108)          0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 108)          0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 108)          0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 108)          11772       dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 108)          11772       dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 108)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 108)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "starts (Dense)                  (None, 108)          11772       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "stops (Dense)                   (None, 108)          11772       activation_10[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,515,672\n",
      "Trainable params: 1,514,016\n",
      "Non-trainable params: 1,656\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([input_att_flags, input_sequences], [output_starts, output_stops])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=MIN_LR)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=adam , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks\n",
    "mcp = ModelCheckpoint(filepath=\"../results/\"+MODEL_PREFIX+\"BestCheckpoint.h5\",\n",
    "                      monitor='val_loss',\n",
    "                      mode=\"auto\",\n",
    "                      save_weights_only=False,\n",
    "                      save_best_only=True)\n",
    "\n",
    "clr = CyclicLR(mode=CLR_METHOD,\n",
    "               base_lr=MIN_LR,\n",
    "               max_lr=MAX_LR,\n",
    "               step_size= STEP_SIZE * (X_train.shape[0] // BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19890 samples, validate on 4973 samples\n",
      "Epoch 1/100\n",
      "19890/19890 [==============================] - 33s 2ms/sample - loss: 9.5906 - starts_loss: 4.7787 - stops_loss: 4.8087 - starts_accuracy: 0.0397 - stops_accuracy: 0.0076 - val_loss: 9.3580 - val_starts_loss: 4.6773 - val_stops_loss: 4.6806 - val_starts_accuracy: 0.0390 - val_stops_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "19890/19890 [==============================] - 18s 890us/sample - loss: 9.2372 - starts_loss: 4.5465 - stops_loss: 4.6795 - starts_accuracy: 0.0929 - stops_accuracy: 0.0206 - val_loss: 9.3098 - val_starts_loss: 4.6468 - val_stops_loss: 4.6630 - val_starts_accuracy: 0.1100 - val_stops_accuracy: 0.0241\n",
      "Epoch 3/100\n",
      "19890/19890 [==============================] - 17s 866us/sample - loss: 8.4551 - starts_loss: 4.0256 - stops_loss: 4.4139 - starts_accuracy: 0.2058 - stops_accuracy: 0.0549 - val_loss: 9.1629 - val_starts_loss: 4.5554 - val_stops_loss: 4.6075 - val_starts_accuracy: 0.2606 - val_stops_accuracy: 0.0615\n",
      "Epoch 4/100\n",
      "19890/19890 [==============================] - 17s 865us/sample - loss: 7.1693 - starts_loss: 3.2138 - stops_loss: 3.9371 - starts_accuracy: 0.2854 - stops_accuracy: 0.0962 - val_loss: 8.7595 - val_starts_loss: 4.2900 - val_stops_loss: 4.4694 - val_starts_accuracy: 0.2737 - val_stops_accuracy: 0.0428\n",
      "Epoch 5/100\n",
      "19890/19890 [==============================] - 17s 865us/sample - loss: 5.8791 - starts_loss: 2.5372 - stops_loss: 3.3296 - starts_accuracy: 0.3311 - stops_accuracy: 0.1680 - val_loss: 8.1001 - val_starts_loss: 3.8622 - val_stops_loss: 4.2377 - val_starts_accuracy: 0.3048 - val_stops_accuracy: 0.1645\n",
      "Epoch 6/100\n",
      "19890/19890 [==============================] - 17s 855us/sample - loss: 5.1338 - starts_loss: 2.2770 - stops_loss: 2.8498 - starts_accuracy: 0.3362 - stops_accuracy: 0.2794 - val_loss: 7.5399 - val_starts_loss: 3.5513 - val_stops_loss: 3.9881 - val_starts_accuracy: 0.3376 - val_stops_accuracy: 0.2562\n",
      "Epoch 7/100\n",
      "19890/19890 [==============================] - 17s 851us/sample - loss: 4.6657 - starts_loss: 2.1574 - stops_loss: 2.5014 - starts_accuracy: 0.3432 - stops_accuracy: 0.3620 - val_loss: 7.0944 - val_starts_loss: 3.3187 - val_stops_loss: 3.7749 - val_starts_accuracy: 0.3346 - val_stops_accuracy: 0.2753\n",
      "Epoch 8/100\n",
      "19890/19890 [==============================] - 17s 853us/sample - loss: 4.3488 - starts_loss: 2.0859 - stops_loss: 2.2536 - starts_accuracy: 0.3461 - stops_accuracy: 0.4161 - val_loss: 6.6787 - val_starts_loss: 3.1399 - val_stops_loss: 3.5376 - val_starts_accuracy: 0.3535 - val_stops_accuracy: 0.2640\n",
      "Epoch 9/100\n",
      "19890/19890 [==============================] - 17s 861us/sample - loss: 4.1454 - starts_loss: 2.0377 - stops_loss: 2.1070 - starts_accuracy: 0.3545 - stops_accuracy: 0.4482 - val_loss: 6.2820 - val_starts_loss: 2.9692 - val_stops_loss: 3.3115 - val_starts_accuracy: 0.3648 - val_stops_accuracy: 0.3022\n",
      "Epoch 10/100\n",
      "19890/19890 [==============================] - 17s 848us/sample - loss: 3.9939 - starts_loss: 1.9991 - stops_loss: 1.9944 - starts_accuracy: 0.3591 - stops_accuracy: 0.4755 - val_loss: 5.9912 - val_starts_loss: 2.8340 - val_stops_loss: 3.1557 - val_starts_accuracy: 0.3571 - val_stops_accuracy: 0.2232\n",
      "Epoch 11/100\n",
      "19890/19890 [==============================] - 17s 853us/sample - loss: 3.8729 - starts_loss: 1.9689 - stops_loss: 1.9025 - starts_accuracy: 0.3600 - stops_accuracy: 0.5033 - val_loss: 5.8314 - val_starts_loss: 2.7713 - val_stops_loss: 3.0584 - val_starts_accuracy: 0.3451 - val_stops_accuracy: 0.1874\n",
      "Epoch 12/100\n",
      "19890/19890 [==============================] - 17s 844us/sample - loss: 3.7983 - starts_loss: 1.9443 - stops_loss: 1.8480 - starts_accuracy: 0.3658 - stops_accuracy: 0.5190 - val_loss: 5.6794 - val_starts_loss: 2.6987 - val_stops_loss: 2.9789 - val_starts_accuracy: 0.3487 - val_stops_accuracy: 0.2339\n",
      "Epoch 13/100\n",
      "19890/19890 [==============================] - 17s 844us/sample - loss: 3.7470 - starts_loss: 1.9344 - stops_loss: 1.8118 - starts_accuracy: 0.3702 - stops_accuracy: 0.5287 - val_loss: 5.6063 - val_starts_loss: 2.6730 - val_stops_loss: 2.9314 - val_starts_accuracy: 0.3439 - val_stops_accuracy: 0.1953\n",
      "Epoch 14/100\n",
      "19890/19890 [==============================] - 17s 853us/sample - loss: 3.7077 - starts_loss: 1.9182 - stops_loss: 1.7927 - starts_accuracy: 0.3741 - stops_accuracy: 0.5355 - val_loss: 5.5142 - val_starts_loss: 2.6407 - val_stops_loss: 2.8715 - val_starts_accuracy: 0.3443 - val_stops_accuracy: 0.2377\n",
      "Epoch 15/100\n",
      "19890/19890 [==============================] - 17s 845us/sample - loss: 3.6737 - starts_loss: 1.9065 - stops_loss: 1.7692 - starts_accuracy: 0.3794 - stops_accuracy: 0.5412 - val_loss: 5.4182 - val_starts_loss: 2.6018 - val_stops_loss: 2.8144 - val_starts_accuracy: 0.3473 - val_stops_accuracy: 0.2475\n",
      "Epoch 16/100\n",
      "19890/19890 [==============================] - 17s 863us/sample - loss: 3.6501 - starts_loss: 1.8936 - stops_loss: 1.7559 - starts_accuracy: 0.3858 - stops_accuracy: 0.5471 - val_loss: 5.3302 - val_starts_loss: 2.5867 - val_stops_loss: 2.7414 - val_starts_accuracy: 0.3364 - val_stops_accuracy: 0.3085\n",
      "Epoch 17/100\n",
      "19890/19890 [==============================] - 17s 863us/sample - loss: 3.6272 - starts_loss: 1.8959 - stops_loss: 1.7382 - starts_accuracy: 0.3887 - stops_accuracy: 0.5521 - val_loss: 5.2372 - val_starts_loss: 2.5694 - val_stops_loss: 2.6657 - val_starts_accuracy: 0.3390 - val_stops_accuracy: 0.3764\n",
      "Epoch 18/100\n",
      "19890/19890 [==============================] - 17s 855us/sample - loss: 3.6087 - starts_loss: 1.8824 - stops_loss: 1.7237 - starts_accuracy: 0.3892 - stops_accuracy: 0.5541 - val_loss: 5.1575 - val_starts_loss: 2.5524 - val_stops_loss: 2.6030 - val_starts_accuracy: 0.3439 - val_stops_accuracy: 0.3871\n",
      "Epoch 19/100\n",
      "19890/19890 [==============================] - 17s 856us/sample - loss: 3.6032 - starts_loss: 1.8779 - stops_loss: 1.7196 - starts_accuracy: 0.3924 - stops_accuracy: 0.5562 - val_loss: 5.0428 - val_starts_loss: 2.5304 - val_stops_loss: 2.5103 - val_starts_accuracy: 0.3400 - val_stops_accuracy: 0.4249\n",
      "Epoch 20/100\n",
      "19890/19890 [==============================] - 17s 866us/sample - loss: 3.5963 - starts_loss: 1.8765 - stops_loss: 1.7131 - starts_accuracy: 0.3931 - stops_accuracy: 0.5587 - val_loss: 4.9270 - val_starts_loss: 2.5054 - val_stops_loss: 2.4195 - val_starts_accuracy: 0.3420 - val_stops_accuracy: 0.4569\n",
      "Epoch 21/100\n",
      "19890/19890 [==============================] - 17s 854us/sample - loss: 3.5971 - starts_loss: 1.8801 - stops_loss: 1.7149 - starts_accuracy: 0.3912 - stops_accuracy: 0.5594 - val_loss: 4.8247 - val_starts_loss: 2.4839 - val_stops_loss: 2.3386 - val_starts_accuracy: 0.3455 - val_stops_accuracy: 0.4669\n",
      "Epoch 22/100\n",
      "19890/19890 [==============================] - 17s 873us/sample - loss: 3.5799 - starts_loss: 1.8743 - stops_loss: 1.7084 - starts_accuracy: 0.3939 - stops_accuracy: 0.5612 - val_loss: 4.7338 - val_starts_loss: 2.4657 - val_stops_loss: 2.2659 - val_starts_accuracy: 0.3479 - val_stops_accuracy: 0.4898\n",
      "Epoch 23/100\n",
      "19890/19890 [==============================] - 17s 858us/sample - loss: 3.5760 - starts_loss: 1.8689 - stops_loss: 1.7026 - starts_accuracy: 0.3964 - stops_accuracy: 0.5600 - val_loss: 4.5642 - val_starts_loss: 2.4267 - val_stops_loss: 2.1353 - val_starts_accuracy: 0.3509 - val_stops_accuracy: 0.5317\n",
      "Epoch 24/100\n",
      "19890/19890 [==============================] - 18s 921us/sample - loss: 3.5446 - starts_loss: 1.8594 - stops_loss: 1.6798 - starts_accuracy: 0.4019 - stops_accuracy: 0.5653 - val_loss: 4.4788 - val_starts_loss: 2.4021 - val_stops_loss: 2.0744 - val_starts_accuracy: 0.3597 - val_stops_accuracy: 0.5327\n",
      "Epoch 25/100\n",
      "19890/19890 [==============================] - 17s 857us/sample - loss: 3.5386 - starts_loss: 1.8562 - stops_loss: 1.6831 - starts_accuracy: 0.4041 - stops_accuracy: 0.5668 - val_loss: 4.3876 - val_starts_loss: 2.3809 - val_stops_loss: 2.0044 - val_starts_accuracy: 0.3732 - val_stops_accuracy: 0.5550\n",
      "Epoch 26/100\n",
      "19890/19890 [==============================] - 17s 857us/sample - loss: 3.4989 - starts_loss: 1.8479 - stops_loss: 1.6510 - starts_accuracy: 0.4118 - stops_accuracy: 0.5713 - val_loss: 4.3127 - val_starts_loss: 2.3808 - val_stops_loss: 1.9298 - val_starts_accuracy: 0.3825 - val_stops_accuracy: 0.5578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "19890/19890 [==============================] - 18s 904us/sample - loss: 3.4605 - starts_loss: 1.8333 - stops_loss: 1.6258 - starts_accuracy: 0.4195 - stops_accuracy: 0.5754 - val_loss: 4.2339 - val_starts_loss: 2.3628 - val_stops_loss: 1.8689 - val_starts_accuracy: 0.3821 - val_stops_accuracy: 0.5687\n",
      "Epoch 28/100\n",
      "19890/19890 [==============================] - 17s 846us/sample - loss: 3.4217 - starts_loss: 1.8107 - stops_loss: 1.6089 - starts_accuracy: 0.4323 - stops_accuracy: 0.5810 - val_loss: 4.1532 - val_starts_loss: 2.3360 - val_stops_loss: 1.8152 - val_starts_accuracy: 0.3887 - val_stops_accuracy: 0.5632\n",
      "Epoch 29/100\n",
      "19890/19890 [==============================] - 17s 836us/sample - loss: 3.3785 - starts_loss: 1.7998 - stops_loss: 1.5799 - starts_accuracy: 0.4397 - stops_accuracy: 0.5847 - val_loss: 4.0584 - val_starts_loss: 2.2960 - val_stops_loss: 1.7598 - val_starts_accuracy: 0.3998 - val_stops_accuracy: 0.5665\n",
      "Epoch 30/100\n",
      "19890/19890 [==============================] - 17s 861us/sample - loss: 3.3041 - starts_loss: 1.7661 - stops_loss: 1.5389 - starts_accuracy: 0.4543 - stops_accuracy: 0.5917 - val_loss: 4.0099 - val_starts_loss: 2.2664 - val_stops_loss: 1.7413 - val_starts_accuracy: 0.4076 - val_stops_accuracy: 0.5640\n",
      "Epoch 31/100\n",
      "19890/19890 [==============================] - 17s 832us/sample - loss: 3.2661 - starts_loss: 1.7495 - stops_loss: 1.5164 - starts_accuracy: 0.4566 - stops_accuracy: 0.5918 - val_loss: 4.0697 - val_starts_loss: 2.3095 - val_stops_loss: 1.7581 - val_starts_accuracy: 0.3947 - val_stops_accuracy: 0.5526\n",
      "Epoch 32/100\n",
      "19890/19890 [==============================] - 17s 857us/sample - loss: 3.1953 - starts_loss: 1.7184 - stops_loss: 1.4732 - starts_accuracy: 0.4709 - stops_accuracy: 0.5995 - val_loss: 3.9845 - val_starts_loss: 2.2828 - val_stops_loss: 1.6995 - val_starts_accuracy: 0.4012 - val_stops_accuracy: 0.5675\n",
      "Epoch 33/100\n",
      "19890/19890 [==============================] - 17s 854us/sample - loss: 3.1375 - starts_loss: 1.7002 - stops_loss: 1.4382 - starts_accuracy: 0.4754 - stops_accuracy: 0.6093 - val_loss: 3.9244 - val_starts_loss: 2.2254 - val_stops_loss: 1.6970 - val_starts_accuracy: 0.4064 - val_stops_accuracy: 0.5594\n",
      "Epoch 34/100\n",
      "16384/19890 [=======================>......] - ETA: 2s - loss: 3.1029 - starts_loss: 1.6793 - stops_loss: 1.4236 - starts_accuracy: 0.4812 - stops_accuracy: 0.6124"
     ]
    }
   ],
   "source": [
    "history = model.fit(x={\"att_flags\":X_att_train,\n",
    "                       \"words\":X_train},\n",
    "                    y={\"starts\":Y_starts_train.argmax(axis=1),\n",
    "                       \"stops\":Y_stops_train.argmax(axis=1)},\n",
    "                    shuffle=True,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=({\"att_flags\":X_att_val, \"words\":X_val},\n",
    "                                     {\"starts\":Y_starts_val.argmax(axis=1), \"stops\":Y_stops_val.argmax(axis=1)}),\n",
    "                    verbose=1,\n",
    "                    callbacks=[clr, mcp]) #es, rlrop, tb, mcp,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric_names = ['loss' ,'accuracy']\n",
    "metric_names = [\"loss\", \"starts_loss\", \"stops_loss\", \"starts_accuracy\", \"stops_accuracy\"]\n",
    "\n",
    "for i, j in zip(metric_names, ['val_'+i for i in metric_names]):\n",
    "    plt.plot(history.history[i])\n",
    "    plt.plot(history.history[j])\n",
    "    plt.title('Model '+i)\n",
    "    plt.ylabel(i.upper())\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../results/\"+MODEL_PREFIX+\"EndCheckpoint.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = tf.keras.models.load_model(\n",
    "    filepath=\"../results/\"+MODEL_PREFIX+\"BestCheckpoint.h5\",\n",
    "    compile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x = {\"att_flags\":X_att_val, \"words\":X_val},\n",
    "               y={\"starts\":Y_starts_val.argmax(axis=1), \"stops\":Y_stops_val.argmax(axis=1)},\n",
    "               batch_size=PREDICT_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(x = {\"att_flags\":X_att_train, \"words\":X_train},\n",
    "                           batch_size=PREDICT_BATCH_SIZE)\n",
    "pred_val = model.predict(x = {\"att_flags\":X_att_val, \"words\":X_val},\n",
    "                         batch_size=PREDICT_BATCH_SIZE)\n",
    "pred_test = model.predict(x = {\"att_flags\":X_att_test, \"words\":X_test},\n",
    "                          batch_size=PREDICT_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_starts_train, pred_stops_train = pred_train[0], pred_train[1]\n",
    "pred_starts_val, pred_stops_val = pred_val[0], pred_val[1]\n",
    "pred_starts_test, pred_stops_test = pred_test[0], pred_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_starts_train.shape, pred_stops_train.shape)\n",
    "print(pred_starts_val.shape, pred_stops_val.shape)\n",
    "print(pred_starts_test.shape, pred_stops_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {\n",
    "    \"train\":{\n",
    "        \"starts\":{\n",
    "            \"y_true\":Y_starts_train.argmax(axis=1),\n",
    "            \"y_pred\":pred_starts_train.argmax(axis=1)\n",
    "        },\n",
    "        \"stops\":{\n",
    "            \"y_true\":Y_stops_train.argmax(axis=1),\n",
    "            \"y_pred\":pred_stops_train.argmax(axis=1)\n",
    "        }\n",
    "    },\n",
    "    \"valid\":{\n",
    "        \"starts\":{\n",
    "            \"y_true\":Y_starts_val.argmax(axis=1),\n",
    "            \"y_pred\":pred_starts_val.argmax(axis=1)\n",
    "        },\n",
    "        \"stops\":{\n",
    "            \"y_true\":Y_stops_train.argmax(axis=1),\n",
    "            \"y_pred\":pred_stops_train.argmax(axis=1)\n",
    "        }        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics():\n",
    "    funcs = [accuracy_score, f1_score, precision_score, recall_score, confusion_matrix]\n",
    "    for f in funcs:\n",
    "        for data_set in [\"train\",\"valid\"]:\n",
    "            for var in [\"starts\", \"stops\"]:\n",
    "                if f in [accuracy_score]:\n",
    "                    res = f(**preds[data_set][var])\n",
    "                    print(\"{:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "                elif f in [confusion_matrix]:\n",
    "                    res = f(**preds[data_set][var], labels = np.arange(max_len))\n",
    "                    np.savetxt(X=res, fmt='%i', delimiter=\",\",\n",
    "                               fname=\"../results/ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                    print(\"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__, \"\\t||\", \n",
    "                          \"../results/ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                else:\n",
    "                    res = f(**preds[data_set][var], average=\"macro\")\n",
    "                    print(\"{:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "        print(\"================================================================\")\n",
    "\n",
    "print_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_words_train = [tokenizer.decode(i) for i in Y_train]\n",
    "Y_words_val = [tokenizer.decode(i) for i in Y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_starts_train.shape[0],\n",
    "      sum([s<e for s,e in zip(pred_starts_train.argmax(axis=1),\n",
    "                              pred_stops_train.argmax(axis=1))]))\n",
    "print(pred_starts_val.shape[0],\n",
    "      sum([s<e for s,e in zip(pred_starts_val.argmax(axis=1),\n",
    "                              pred_stops_val.argmax(axis=1))]))\n",
    "print(pred_starts_test.shape[0],\n",
    "      sum([s<e for s,e in zip(pred_starts_test.argmax(axis=1),\n",
    "                              pred_stops_test.argmax(axis=1))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(string):\n",
    "    string = re.sub(pattern=\" (negative|positive|neutral)[ ]+$\", repl=\"\", string=string)\n",
    "    string = re.sub(pattern=\" +\", repl=\" \", string=string.strip())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words_train = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_train, pred_starts_train.argmax(axis=1), pred_stops_train.argmax(axis=1))]\n",
    "pred_words_val = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_val, pred_starts_val.argmax(axis=1), pred_stops_val.argmax(axis=1))]\n",
    "pred_words_test = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_test, pred_starts_test.argmax(axis=1), pred_stops_test.argmax(axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    a = set(str1)\n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Jaccard Score\", np.mean([jaccard(str1=i, str2=j) for i,j in zip(Y_words_train, pred_words_train)]))\n",
    "print(\"Validation Jaccard Score\", np.mean([jaccard(str1=i, str2=j) for i,j in zip(Y_words_val, pred_words_val)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_labels = {i:j for i,j in enumerate(df.sentiment_code.cat.categories)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['selected_text'] = pred_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[[\"text\", \"sentiment\",\"selected_text\"]].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[[\"textID\", \"selected_text\"]].to_csv(\"../results/submission_\"+MODEL_PREFIX+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ctime(time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
