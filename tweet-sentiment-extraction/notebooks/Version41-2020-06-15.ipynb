{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V41\" # OnlySpanModelKF1 2X2Tasks LabelSmoothed with NeutralSamples and Better Preprocessing\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 16\n",
    "PREDICT_BATCH_SIZE = 512\n",
    "DROPOUT = 0.3\n",
    "LABEL_SMOOTHING_PARAM = 0.2\n",
    "\n",
    "RUN_ON_SAMPLE = False\n",
    "EXCLUDE_NEUTRAL_CLASS = True\n",
    "NUM_EPOCHS = [10, 10, 3]\n",
    "NUM_FOLDS = 3\n",
    "#LRs = [5e-3, 1e-4, 1e-6]\n",
    "MAX_LR = 5e-3 #5e-4 #5e-3 #3e-5\n",
    "MID_LR = 5e-4 #5e-5 #1e-4 #3e-5\n",
    "MIN_LR = 5e-5 #5e-6 #1e-6 #3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"../results/\"\n",
    "DATA_DIR = \"../data/\"\n",
    "MODEL_DIR = \"../data/models/roberta-base/\"\n",
    "EXT_MODEL_DIR = \"../data/models/roberta-tokenizer/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, Layer, LeakyReLU, Subtract\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    a = set(str1)\n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeded_value = 12345\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(seeded_value)\n",
    "tf.random.set_seed(seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 15 18:51:10 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0', '2.8.0', '0.5.2']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "if not os.path.exists(MODEL_DIR+\"tokenizers\"):\n",
    "    os.mkdir(MODEL_DIR+\"tokenizers\")\n",
    "\n",
    "if not os.path.exists(MODEL_DIR+\"tokenizers/roberta_tokenizer\"):\n",
    "    os.mkdir(MODEL_DIR+\"tokenizers/roberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LR = 1e-4\n",
    "MAX_LR = 1e-3\n",
    "STEP_SIZE = 10\n",
    "CLR_METHOD = \"triangular\" # exp_range, triangular, triangular2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import *\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\"  target=\"_blank\"><h2 id=\"limiting_gpu_memory_growth\" data-text=\"Limiting GPU memory growth\" tabindex=\"0\">Limiting GPU memory growth</h2></a>\n",
    "<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\"><code translate=\"no\" dir=\"ltr\">CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the <code translate=\"no\" dir=\"ltr\">tf.config.experimental.set_visible_devices</code> method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file=EXT_MODEL_DIR+'/vocab.json',\n",
    "                                             merges_file=EXT_MODEL_DIR+'/merges.txt',                                         \n",
    "                                             add_prefix_space=True,\n",
    "                                             lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(EXT_MODEL_DIR+\"/special_tokens_map.json\") as f:\n",
    "    special_tokens = json.load(f)\n",
    "\n",
    "tokenizer.add_special_tokens([i for i in special_tokens.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.get_vocab_size(); VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  0\n",
      "selected_text  object  0\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27481, 'selected_text': 22464, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count        27481   \n",
      "unique       27481   \n",
      "top     65da7a4cc4   \n",
      "freq             1   \n",
      "\n",
      "                                                                                          text  \\\n",
      "count                                                                                    27481   \n",
      "unique                                                                                   27481   \n",
      "top      a couple of YEARS? I may die without my Kateage.  *keeps fingers crossed for you tho*   \n",
      "freq                                                                                         1   \n",
      "\n",
      "       selected_text sentiment  \n",
      "count          27481     27481  \n",
      "unique         22464         3  \n",
      "top             good   neutral  \n",
      "freq             199     11118  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\").fillna('')\n",
    "\n",
    "print(pd.concat((df_span.dtypes, df_span.isna().sum()), axis=1))\n",
    "print(df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df_span[i].nunique() for i in df_span.columns})\n",
    "print(df_span.describe())\n",
    "df_span.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID                                                text  \\\n",
      "count         3534                                                3534   \n",
      "unique        3534                                                3534   \n",
      "top     4ad37ac5b7  sad that will have to leave my beautiful apartment   \n",
      "freq             1                                                   1   \n",
      "\n",
      "       sentiment  \n",
      "count       3534  \n",
      "unique         3  \n",
      "top      neutral  \n",
      "freq        1430  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_span = pd.read_csv(DATA_DIR+\"test.csv\", encoding=\"utf8\").fillna('')\n",
    "print(pd.concat((test_df_span.dtypes, test_df_span.isna().sum()), axis=1))\n",
    "print(test_df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df_span[i].nunique() for i in test_df_span.columns})\n",
    "print(test_df_span.describe())\n",
    "test_df_span.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text\"] = df_span[\"text\"].astype(str)\n",
    "df_span[\"selected_text\"] = df_span[\"selected_text\"].astype(str)\n",
    "test_df_span[\"text\"] = test_df_span[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_addspace(text:str) -> str:\n",
    "    text = text.lower()\n",
    "    text = \" \" + text.strip(\" \") + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(text:str, selected_text:str) -> (str, str, int, int):\n",
    "    \n",
    "    text, selected_text = text.lower(), selected_text.lower()\n",
    "    \n",
    "    text = trim_addspace(text)\n",
    "    \n",
    "    substring_ = re.findall(pattern=\"\\\\s[^\\s]*?\"+re.escape(selected_text)+\"[^\\s]*?\\\\s\", string=text)[0]\n",
    "    \n",
    "    return pd.Series([text, \" \"+substring_.strip(\" \"), text.find(substring_), len(substring_) + text.find(substring_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[[\"text_mod\", \"selected_text_mod\", \"start\", \"stop\"]] = df_span[['text','selected_text']].apply(lambda x: find_indices(x.text, x.selected_text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'textID': '4eac33d1c0',\n",
       " 'text': ' wish we could come see u on Denver  husband lost his job and can`t afford it',\n",
       " 'selected_text': 'd lost',\n",
       " 'sentiment': 'negative',\n",
       " 'text_mod': ' wish we could come see u on denver  husband lost his job and can`t afford it ',\n",
       " 'selected_text_mod': ' husband lost',\n",
       " 'start': 36,\n",
       " 'stop': 50}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span.iloc[27476].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span['text_mod'] = test_df_span['text'].apply(trim_addspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'textID': {12154: 'adfbcc6806'},\n",
       " 'text': {12154: 'i wanna see `up` tonight, but no one will go with me. whhhyyy'},\n",
       " 'selected_text': {12154: 'but no one will go with me.'},\n",
       " 'sentiment': {12154: 'negative'},\n",
       " 'text_mod': {12154: ' i wanna see `up` tonight, but no one will go with me. whhhyyy '},\n",
       " 'selected_text_mod': {12154: ' but no one will go with me.'},\n",
       " 'start': {12154: 26},\n",
       " 'stop': {12154: 55}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span.loc[df_span.text_mod.str.contains(\"tonight, but no one will go\")].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning for span detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neutral': [7974], 'negative': [2430], 'positive': [1313]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{t:tokenizer.encode(\" \"+t).ids for t in df_span.sentiment.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text_mod\"] = \"<s>\" + df_span['text_mod'] + \"</s></s> \" + df_span.sentiment + \" </s>\"\n",
    "test_df_span[\"text_mod\"] = \"<s>\" + test_df_span['text_mod'] + \"</s></s> \" + test_df_span.sentiment + \" </s>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclusions for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCLUDE_NEUTRAL_CLASS: (16363, 8)\n"
     ]
    }
   ],
   "source": [
    "if EXCLUDE_NEUTRAL_CLASS:\n",
    "    df_span = df_span.loc[df_span.sentiment!=\"neutral\"].copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"EXCLUDE_NEUTRAL_CLASS:\", df_span.shape)\n",
    "\n",
    "if RUN_ON_SAMPLE:\n",
    "    df_span = df_span.sample(2000).copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"Train RUN_ON_SAMPLE\", df_span.shape)\n",
    "    test_df_span = test_df_span.sample(2000).copy()\n",
    "    test_df_span = test_df_span.reset_index(drop=True)\n",
    "    print(\"Test  RUN_ON_SAMPLE\", test_df_span.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_tokens = tokenizer.encode_batch(df_span.text_mod.tolist())\n",
    "Y_span_tokens = tokenizer.encode_batch(df_span.selected_text_mod.tolist())\n",
    "X_span_tokens_test = tokenizer.encode_batch(test_df_span.text_mod.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = [i.ids for i in X_span_tokens]\n",
    "Y_span = [i.ids for i in Y_span_tokens]\n",
    "X_span_test = [i.ids for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_att = [i.attention_mask for i in X_span_tokens]\n",
    "Y_span_att = [i.attention_mask for i in Y_span_tokens] # Useless\n",
    "X_span_att_test = [i.attention_mask for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN_SPAN = max([len(i) for i in X_span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremities(l_string, s_string, print_it=False):\n",
    "    len_l = len(l_string)\n",
    "    len_s = len(s_string)\n",
    "    \n",
    "    for i in range(len_l - len_s + 1):\n",
    "        if (i + len_s) <= len_l:\n",
    "            substring = l_string[i:i+len_s]\n",
    "            if substring == s_string:\n",
    "                if print_it:\n",
    "                    print(l_string)\n",
    "                    print(substring)\n",
    "                    print(i, i+len_s, substring)\n",
    "                \n",
    "                start_vector, end_vector = [0] * len_l, [0] * len_l\n",
    "                start_vector[i], end_vector[i+len_s-1] = 1, 1\n",
    "                \n",
    "                return (start_vector, end_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16363 \t: #Processed\n",
      "2 \t: # of Anomalies\n"
     ]
    }
   ],
   "source": [
    "Y_span_starts, Y_span_stops = [], []\n",
    "anomaly_idx, counter = [], 0\n",
    "for num, (i,j) in enumerate(zip(X_span_tokens, Y_span_tokens)):\n",
    "    x,y = i.ids, j.ids\n",
    "    try:\n",
    "        s,e = get_extremities(x, y)\n",
    "        Y_span_starts.append(s)\n",
    "        Y_span_stops.append(e)\n",
    "    except TypeError as t:\n",
    "        counter += 1\n",
    "        anomaly_idx.append(num)\n",
    "        Y_span_starts.append([0]*15)\n",
    "        Y_span_stops.append([0]*15)\n",
    "print(num + 1, \"\\t: #Processed\")\n",
    "\n",
    "print(counter,\"\\t: # of Anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " If you`re flying to NZ www.hot.co.nz is a great way to get results in aggregate.\n",
      "----------\n",
      "great\n",
      "----------\n",
      "[['<s>', 0, 0, 0], ['Ġif', 114, 0, 0], ['Ġyou', 47, 0, 0], ['`', 12905, 0, 0], ['re', 241, 0, 0], ['Ġflying', 4731, 0, 0], ['Ġto', 7, 0, 0], ['Ġn', 295, 0, 0], ['z', 329, 0, 0], ['Ġwww', 1662, 0, 0], ['.', 4, 0, 0], ['hot', 10120, 0, 0], ['.', 4, 0, 0], ['co', 876, 0, 0], ['.', 4, 0, 0], ['nz', 22973, 0, 0], ['Ġis', 16, 0, 0], ['Ġa', 10, 0, 0], ['Ġgreat', 372, 1, 1], ['Ġway', 169, 0, 0], ['Ġto', 7, 0, 0], ['Ġget', 120, 0, 0], ['Ġresults', 775, 0, 0], ['Ġin', 11, 0, 0], ['Ġaggregate', 13884, 0, 0], ['.', 4, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0], ['</s>', 2, 0, 0], ['Ġpositive', 1313, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0]]\n",
      "----------\n",
      "[[372, 'Ġgreat']]\n"
     ]
    }
   ],
   "source": [
    "check_idx = 1333\n",
    "print(df_span.text[check_idx])\n",
    "print(\"----------\")\n",
    "print(df_span.selected_text[check_idx])\n",
    "print(\"----------\")\n",
    "print([[i,j,k,l] for i,j,k,l in zip(X_span_tokens[check_idx].tokens,\n",
    "                                    X_span_tokens[check_idx].ids,\n",
    "                                    Y_span_starts[check_idx],\n",
    "                                    Y_span_stops[check_idx])])\n",
    "print(\"----------\")\n",
    "print([[i,j] for i,j in zip(Y_span_tokens[check_idx].ids,\n",
    "                            Y_span_tokens[check_idx].tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "\n",
    "Y_span_starts = pad_sequences(Y_span_starts, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")#.argmax(axis=1)\n",
    "Y_span_stops = pad_sequences(Y_span_stops, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")#.argmax(axis=1)\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAX_SEQ_LEN_SPAN': 77,\n",
      " 'VOCAB_SIZE': 50265,\n",
      " 'X_span': (16363, 77),\n",
      " 'X_span_att': (16363, 77),\n",
      " 'X_span_att_test': (3534, 77),\n",
      " 'X_span_test': (3534, 77),\n",
      " 'Y_span': (16363, 77),\n",
      " 'Y_span_starts': (16363, 77),\n",
      " 'Y_span_stops': (16363, 77)}\n"
     ]
    }
   ],
   "source": [
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \"Y_span_starts\" : Y_span_starts.shape,\n",
    "    \"Y_span_stops\" : Y_span_stops.shape,\n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "    \"VOCAB_SIZE\":VOCAB_SIZE,\n",
    "    \"MAX_SEQ_LEN_SPAN\":MAX_SEQ_LEN_SPAN\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_flag = np.isin(Y_span_stops.argmax(axis=1),\n",
    "                    np.unique(Y_span_stops.argmax(axis=1),\n",
    "                              return_counts=True)[0][np.unique(Y_span_stops.argmax(axis=1),\n",
    "                                                               return_counts=True)[1]>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16359, 16363, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(keep_flag), df_span.shape[0], df_span.shape[0] - sum(keep_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (16363, 77) \t: X  \n",
      " (16363, 77) \t: X_att  \n",
      " (16363, 77) \t: Y  \n",
      " (16363, 77) \t: Y_starts  \n",
      " (16363, 77) \t: Y_stops  \n",
      " (3534, 77) \t: X_test  \n",
      " (3534, 77) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = X_span[keep_flag]\n",
    "X_span_att = X_span_att[keep_flag]\n",
    "Y_span = Y_span[keep_flag]\n",
    "Y_span_starts = Y_span_starts[keep_flag]\n",
    "Y_span_stops = Y_span_stops[keep_flag]\n",
    "X_span_test = X_span_test\n",
    "X_span_att_test = X_span_att_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (16359, 77) \t: X  \n",
      " (16359, 77) \t: X_att  \n",
      " (16359, 77) \t: Y  \n",
      " (16359, 77) \t: Y_starts  \n",
      " (16359, 77) \t: Y_stops  \n",
      " (3534, 77) \t: X_test  \n",
      " (3534, 77) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_span_words = [tokenizer.decode(i) for i in Y_span]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 77)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LEN =  MAX_SEQ_LEN_SPAN\n",
    "MAX_SEQ_LEN, MAX_SEQ_LEN_SPAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X_span': (16359, 77),\n",
      " 'X_span_att': (16359, 77),\n",
      " 'X_span_att_test': (3534, 77),\n",
      " 'X_span_test': (3534, 77),\n",
      " 'Y_span': (16359, 77)}\n"
     ]
    }
   ],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span_starts = pad_sequences(Y_span_starts, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span_stops = pad_sequences(Y_span_stops, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_sequences = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"words\")\n",
    "    input_att_flags = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"att_flags\")\n",
    "    input_token_ids = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"token_ids\")\n",
    "    \n",
    "    config = RobertaConfig.from_pretrained(MODEL_DIR+'config.json')\n",
    "    roberta_model = TFRobertaModel.from_pretrained(MODEL_DIR+'tf_model.h5', config=config)\n",
    "    x = roberta_model(inputs=input_sequences, attention_mask=input_att_flags, token_type_ids=input_token_ids)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(DROPOUT)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    output_starts_0 = tf.keras.layers.Activation('softmax', name=\"starts_0\")(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(DROPOUT)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    output_stops_0 = tf.keras.layers.Activation('softmax', name=\"stops_0\")(x2)\n",
    "    \n",
    "    output_subtract = tf.keras.layers.Subtract()([output_starts_0, output_stops_0])\n",
    "    output_flat = concatenate([output_starts_0, output_stops_0, output_subtract])\n",
    "    output_starts_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"starts_1\")(output_flat)\n",
    "    output_stops_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"stops_1\")(output_flat)\n",
    "    \n",
    "    span_detection_model = Model([input_att_flags, input_sequences, input_token_ids],\n",
    "                                 [output_starts_0, output_stops_0, output_starts_1, output_stops_1])\n",
    "    \n",
    "    return span_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_detection_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 77)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 77)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 77)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 77, 768), (N 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 77, 768)      0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 77, 768)      0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 77, 768)      1180416     dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 77, 768)      1180416     dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 77, 768)      0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 77, 768)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 77, 1)        769         leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 77, 1)        769         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 77)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 77)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "starts_0 (Activation)           (None, 77)           0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "stops_0 (Activation)            (None, 77)           0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 77)           0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 231)          0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "                                                                 subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "starts_1 (Dense)                (None, 77)           17864       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "stops_1 (Dense)                 (None, 77)           17864       concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 127,043,730\n",
      "Trainable params: 127,043,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "span_detection_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span Detection Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_FOLDS):\n",
    "    if os.path.exists(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\"):\n",
    "        os.remove(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(pred_dict):\n",
    "    print(\"[INFO] \",\"=\"*15,\"Validation for FOLD#\", num, \"=\"*15)\n",
    "    funcs = [accuracy_score, f1_score, precision_score, recall_score, confusion_matrix]\n",
    "    for f in funcs:\n",
    "        for data_set in [\"train\",\"valid\"]:\n",
    "            for var in [\"starts\", \"stops\"]:\n",
    "                if f in [accuracy_score]:\n",
    "                    res = f(**pred_dict[data_set][var])\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "                elif f in [confusion_matrix]:\n",
    "                    res = f(**pred_dict[data_set][var], labels=np.arange(MAX_SEQ_LEN))\n",
    "                    np.savetxt(X=res, fmt='%i', delimiter=\",\",\n",
    "                               fname=RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                    print(\"[INFO] \\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__, \"\\t||\", \n",
    "                          RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                else:\n",
    "                    res = f(**pred_dict[data_set][var], average=\"macro\")\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "        print(\"=======================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(string):\n",
    "    string = re.sub(pattern=\" (negative|positive|neutral)[ ]+$\", repl=\"\", string=string)\n",
    "    string = re.sub(pattern=\" +\", repl=\" \", string=string.strip())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><h2 id=\"finetuning\">Fine-tuning</h2></a>\n",
    "<p>Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.</p>\n",
    " <p>This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.</p>\n",
    " <p>It is critical to only do this step <em>after</em> the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.</p>\n",
    " <p>It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>\n",
    "\n",
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><p><strong>Important note about <code>compile()</code> and <code>trainable</code></strong></p></a>\n",
    "<p>Calling <code>compile()</code> on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the <code>trainable</code>\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until <code>compile</code> is called again. Hence, if you change any <code>trainable</code> value, make sure\n",
    " to call <code>compile()</code> again on your\n",
    "model for your changes to be taken into account.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossWeightAdjust(Callback):\n",
    "    def __init__(self, alpha, beta, gamma, delta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "    \n",
    "    # customize your behavior\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        losses = np.array([v for k,v in logs.items() if k in ['val_starts_0_loss', 'val_stops_0_loss', 'val_starts_1_loss', 'val_stops_1_loss']], dtype=np.float64)\n",
    "        \n",
    "        total_loss = np.sum(losses)\n",
    "        \n",
    "        losses = (losses - 0.5*losses.min()) / (losses.max() - 0.5*losses.min())\n",
    "        losses = losses/np.sum(losses)\n",
    "\n",
    "        K.set_value(self.alpha, losses[0])\n",
    "        K.set_value(self.beta, losses[1])\n",
    "        K.set_value(self.gamma, losses[2])\n",
    "        K.set_value(self.delta, losses[3])\n",
    "        \n",
    "        print(\"\\n Loss weights recalibrated to alpha = %s, beta = %s, gamma = %s, delta = %s \" % (np.round(losses[0],2),\n",
    "                                                                                                  np.round(losses[1],2),\n",
    "                                                                                                  np.round(losses[2],2),\n",
    "                                                                                                  np.round(losses[3],2)))\n",
    "        print(\"Total Val Loss\", np.round(total_loss,3))\n",
    "        logger.info(\"Loss weights recalibrated to alpha = %s, beta = %s, gamma = %s, delta = %s \" % (K.get_value(self.alpha),\n",
    "                                                                                                     K.get_value(self.beta),\n",
    "                                                                                                     K.get_value(self.gamma),\n",
    "                                                                                                     K.get_value(self.delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What does the Loss Weight Adjust Callback do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17266986, 0.1622123 , 0.33177851, 0.33333934])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = np.array([2.7892, 2.7021, 4.1144, 4.1274])\n",
    "losses = (losses - 0.5*losses.min()) / (losses.max() - 0.5*losses.min())\n",
    "losses = losses/np.sum(losses)\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=seeded_value)\n",
    "t_index, v_index = train_test_split(np.arange(X_span.shape[0]), shuffle=True, random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr = CyclicLR(mode=CLR_METHOD,\n",
    "               base_lr=MIN_LR,\n",
    "               max_lr=MAX_LR,\n",
    "               step_size= STEP_SIZE * (X_cat.shape[0] // BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 12269 samples, validate on 4090 samples\n",
      "Epoch 1/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 4.8416 - starts_0_loss: 3.0039 - stops_0_loss: 3.0332 - starts_1_loss: 3.2192 - stops_1_loss: 3.3700 - starts_0_accuracy: 0.3606 - stops_0_accuracy: 0.4022 - starts_1_accuracy: 0.3282 - stops_1_accuracy: 0.3246\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 11.288\n",
      "12269/12269 [==============================] - 145s 12ms/sample - loss: 4.8412 - starts_0_loss: 3.0047 - stops_0_loss: 3.0330 - starts_1_loss: 3.2189 - stops_1_loss: 3.3694 - starts_0_accuracy: 0.3604 - stops_0_accuracy: 0.4022 - starts_1_accuracy: 0.3282 - stops_1_accuracy: 0.3248 - val_loss: 4.2685 - val_starts_0_loss: 2.7949 - val_stops_0_loss: 2.7175 - val_starts_1_loss: 2.8821 - val_stops_1_loss: 2.8933 - val_starts_0_accuracy: 0.4066 - val_stops_0_accuracy: 0.4589 - val_starts_1_accuracy: 0.3499 - val_stops_1_accuracy: 0.4756\n",
      "Epoch 2/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.8922 - starts_0_loss: 2.9112 - stops_0_loss: 2.7910 - starts_1_loss: 2.8951 - stops_1_loss: 2.9612 - starts_0_accuracy: 0.3754 - stops_0_accuracy: 0.4289 - starts_1_accuracy: 0.3664 - stops_1_accuracy: 0.4431\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.22, gamma = 0.27, delta = 0.27 \n",
      "Total Val Loss 10.924\n",
      "12269/12269 [==============================] - 127s 10ms/sample - loss: 2.8924 - starts_0_loss: 2.9116 - stops_0_loss: 2.7909 - starts_1_loss: 2.8953 - stops_1_loss: 2.9615 - starts_0_accuracy: 0.3753 - stops_0_accuracy: 0.4288 - starts_1_accuracy: 0.3663 - stops_1_accuracy: 0.4430 - val_loss: 2.7367 - val_starts_0_loss: 2.6926 - val_stops_0_loss: 2.5627 - val_starts_1_loss: 2.8429 - val_stops_1_loss: 2.8262 - val_starts_0_accuracy: 0.4191 - val_stops_0_accuracy: 0.4555 - val_starts_1_accuracy: 0.3650 - val_stops_1_accuracy: 0.4924\n",
      "Epoch 3/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.8631 - starts_0_loss: 2.9217 - stops_0_loss: 2.8092 - starts_1_loss: 2.8241 - stops_1_loss: 2.8936 - starts_0_accuracy: 0.3791 - stops_0_accuracy: 0.4249 - starts_1_accuracy: 0.3859 - stops_1_accuracy: 0.4467\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.24, gamma = 0.25, delta = 0.26 \n",
      "Total Val Loss 10.697\n",
      "12269/12269 [==============================] - 128s 10ms/sample - loss: 2.8633 - starts_0_loss: 2.9219 - stops_0_loss: 2.8093 - starts_1_loss: 2.8245 - stops_1_loss: 2.8938 - starts_0_accuracy: 0.3790 - stops_0_accuracy: 0.4249 - starts_1_accuracy: 0.3859 - stops_1_accuracy: 0.4467 - val_loss: 2.6767 - val_starts_0_loss: 2.6447 - val_stops_0_loss: 2.6373 - val_starts_1_loss: 2.6961 - val_stops_1_loss: 2.7188 - val_starts_0_accuracy: 0.4455 - val_stops_0_accuracy: 0.4604 - val_starts_1_accuracy: 0.4347 - val_stops_1_accuracy: 0.4912\n",
      "Epoch 4/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.8343 - starts_0_loss: 2.8930 - stops_0_loss: 2.8032 - starts_1_loss: 2.7865 - stops_1_loss: 2.8551 - starts_0_accuracy: 0.3899 - stops_0_accuracy: 0.4224 - starts_1_accuracy: 0.4028 - stops_1_accuracy: 0.4496\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.787\n",
      "12269/12269 [==============================] - 128s 10ms/sample - loss: 2.8345 - starts_0_loss: 2.8934 - stops_0_loss: 2.8034 - starts_1_loss: 2.7866 - stops_1_loss: 2.8554 - starts_0_accuracy: 0.3898 - stops_0_accuracy: 0.4224 - starts_1_accuracy: 0.4029 - stops_1_accuracy: 0.4497 - val_loss: 2.6987 - val_starts_0_loss: 2.6414 - val_stops_0_loss: 2.6110 - val_starts_1_loss: 2.7722 - val_stops_1_loss: 2.7627 - val_starts_0_accuracy: 0.4313 - val_stops_0_accuracy: 0.4389 - val_starts_1_accuracy: 0.4051 - val_stops_1_accuracy: 0.4756\n",
      "Epoch 5/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.8288 - starts_0_loss: 2.9020 - stops_0_loss: 2.8100 - starts_1_loss: 2.7775 - stops_1_loss: 2.8300 - starts_0_accuracy: 0.3918 - stops_0_accuracy: 0.4254 - starts_1_accuracy: 0.4064 - stops_1_accuracy: 0.4580\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.25, gamma = 0.25, delta = 0.27 \n",
      "Total Val Loss 10.728\n",
      "12269/12269 [==============================] - 129s 10ms/sample - loss: 2.8293 - starts_0_loss: 2.9030 - stops_0_loss: 2.8107 - starts_1_loss: 2.7777 - stops_1_loss: 2.8306 - starts_0_accuracy: 0.3916 - stops_0_accuracy: 0.4254 - starts_1_accuracy: 0.4065 - stops_1_accuracy: 0.4578 - val_loss: 2.6839 - val_starts_0_loss: 2.6164 - val_stops_0_loss: 2.6601 - val_starts_1_loss: 2.6694 - val_stops_1_loss: 2.7817 - val_starts_0_accuracy: 0.4472 - val_stops_0_accuracy: 0.4506 - val_starts_1_accuracy: 0.4340 - val_stops_1_accuracy: 0.4822\n",
      "Epoch 6/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.8176 - starts_0_loss: 2.8938 - stops_0_loss: 2.7944 - starts_1_loss: 2.7698 - stops_1_loss: 2.8152 - starts_0_accuracy: 0.3905 - stops_0_accuracy: 0.4337 - starts_1_accuracy: 0.4051 - stops_1_accuracy: 0.4599\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.24, gamma = 0.26, delta = 0.25 \n",
      "Total Val Loss 10.782\n",
      "12269/12269 [==============================] - 128s 10ms/sample - loss: 2.8173 - starts_0_loss: 2.8934 - stops_0_loss: 2.7941 - starts_1_loss: 2.7697 - stops_1_loss: 2.8149 - starts_0_accuracy: 0.3907 - stops_0_accuracy: 0.4337 - starts_1_accuracy: 0.4051 - stops_1_accuracy: 0.4601 - val_loss: 2.6960 - val_starts_0_loss: 2.6777 - val_stops_0_loss: 2.6547 - val_starts_1_loss: 2.7402 - val_stops_1_loss: 2.7091 - val_starts_0_accuracy: 0.4249 - val_stops_0_accuracy: 0.4381 - val_starts_1_accuracy: 0.4144 - val_stops_1_accuracy: 0.4844\n",
      "Epoch 7/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.8053 - starts_0_loss: 2.8826 - stops_0_loss: 2.7788 - starts_1_loss: 2.7619 - stops_1_loss: 2.7995 - starts_0_accuracy: 0.3923 - stops_0_accuracy: 0.4288 - starts_1_accuracy: 0.4102 - stops_1_accuracy: 0.4642\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.24, gamma = 0.25, delta = 0.26 \n",
      "Total Val Loss 10.577\n",
      "12269/12269 [==============================] - 127s 10ms/sample - loss: 2.8052 - starts_0_loss: 2.8828 - stops_0_loss: 2.7783 - starts_1_loss: 2.7621 - stops_1_loss: 2.7991 - starts_0_accuracy: 0.3924 - stops_0_accuracy: 0.4290 - starts_1_accuracy: 0.4102 - stops_1_accuracy: 0.4644 - val_loss: 2.6451 - val_starts_0_loss: 2.6364 - val_stops_0_loss: 2.5973 - val_starts_1_loss: 2.6650 - val_stops_1_loss: 2.6782 - val_starts_0_accuracy: 0.4452 - val_stops_0_accuracy: 0.4394 - val_starts_1_accuracy: 0.4364 - val_stops_1_accuracy: 0.4895\n",
      "Epoch 8/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.8025 - starts_0_loss: 2.8894 - stops_0_loss: 2.7736 - starts_1_loss: 2.7553 - stops_1_loss: 2.7923 - starts_0_accuracy: 0.3962 - stops_0_accuracy: 0.4261 - starts_1_accuracy: 0.4130 - stops_1_accuracy: 0.4676\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.27, delta = 0.26 \n",
      "Total Val Loss 10.488\n",
      "12269/12269 [==============================] - 129s 10ms/sample - loss: 2.8028 - starts_0_loss: 2.8895 - stops_0_loss: 2.7741 - starts_1_loss: 2.7553 - stops_1_loss: 2.7931 - starts_0_accuracy: 0.3963 - stops_0_accuracy: 0.4258 - starts_1_accuracy: 0.4131 - stops_1_accuracy: 0.4672 - val_loss: 2.6238 - val_starts_0_loss: 2.5954 - val_stops_0_loss: 2.5245 - val_starts_1_loss: 2.7039 - val_stops_1_loss: 2.6646 - val_starts_0_accuracy: 0.4462 - val_stops_0_accuracy: 0.4592 - val_starts_1_accuracy: 0.4232 - val_stops_1_accuracy: 0.5020\n",
      "Epoch 9/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.8041 - starts_0_loss: 2.8969 - stops_0_loss: 2.7768 - starts_1_loss: 2.7577 - stops_1_loss: 2.7879 - starts_0_accuracy: 0.3900 - stops_0_accuracy: 0.4235 - starts_1_accuracy: 0.4107 - stops_1_accuracy: 0.4640\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.25, gamma = 0.25, delta = 0.25 \n",
      "Total Val Loss 10.987\n",
      "12269/12269 [==============================] - 128s 10ms/sample - loss: 2.8038 - starts_0_loss: 2.8965 - stops_0_loss: 2.7765 - starts_1_loss: 2.7576 - stops_1_loss: 2.7876 - starts_0_accuracy: 0.3901 - stops_0_accuracy: 0.4235 - starts_1_accuracy: 0.4107 - stops_1_accuracy: 0.4640 - val_loss: 2.7465 - val_starts_0_loss: 2.7434 - val_stops_0_loss: 2.7735 - val_starts_1_loss: 2.7442 - val_stops_1_loss: 2.7258 - val_starts_0_accuracy: 0.4296 - val_stops_0_accuracy: 0.4279 - val_starts_1_accuracy: 0.4403 - val_stops_1_accuracy: 0.4692\n",
      "Epoch 10/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.7949 - starts_0_loss: 2.8772 - stops_0_loss: 2.7659 - starts_1_loss: 2.7533 - stops_1_loss: 2.7835 - starts_0_accuracy: 0.3952 - stops_0_accuracy: 0.4249 - starts_1_accuracy: 0.4098 - stops_1_accuracy: 0.4610\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.24, gamma = 0.25, delta = 0.25 \n",
      "Total Val Loss 10.669\n",
      "12269/12269 [==============================] - 128s 10ms/sample - loss: 2.7948 - starts_0_loss: 2.8773 - stops_0_loss: 2.7657 - starts_1_loss: 2.7533 - stops_1_loss: 2.7833 - starts_0_accuracy: 0.3952 - stops_0_accuracy: 0.4251 - starts_1_accuracy: 0.4097 - stops_1_accuracy: 0.4610 - val_loss: 2.6673 - val_starts_0_loss: 2.6812 - val_stops_0_loss: 2.6108 - val_starts_1_loss: 2.6869 - val_stops_1_loss: 2.6903 - val_starts_0_accuracy: 0.4553 - val_stops_0_accuracy: 0.4570 - val_starts_1_accuracy: 0.4469 - val_stops_1_accuracy: 0.5059\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 12269 samples, validate on 4090 samples\n",
      "Epoch 1/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.6146 - starts_0_loss: 2.6503 - stops_0_loss: 2.5568 - starts_1_loss: 2.6325 - stops_1_loss: 2.6158 - starts_0_accuracy: 0.4487 - stops_0_accuracy: 0.4665 - starts_1_accuracy: 0.4502 - stops_1_accuracy: 0.5237\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.188\n",
      "12269/12269 [==============================] - 145s 12ms/sample - loss: 2.6146 - starts_0_loss: 2.6505 - stops_0_loss: 2.5566 - starts_1_loss: 2.6327 - stops_1_loss: 2.6157 - starts_0_accuracy: 0.4484 - stops_0_accuracy: 0.4665 - starts_1_accuracy: 0.4501 - stops_1_accuracy: 0.5237 - val_loss: 2.5484 - val_starts_0_loss: 2.5251 - val_stops_0_loss: 2.4581 - val_starts_1_loss: 2.6182 - val_stops_1_loss: 2.5864 - val_starts_0_accuracy: 0.4768 - val_stops_0_accuracy: 0.4770 - val_starts_1_accuracy: 0.4601 - val_stops_1_accuracy: 0.5267\n",
      "Epoch 2/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5986 - starts_0_loss: 2.6453 - stops_0_loss: 2.5281 - starts_1_loss: 2.6206 - stops_1_loss: 2.5953 - starts_0_accuracy: 0.4436 - stops_0_accuracy: 0.4701 - starts_1_accuracy: 0.4483 - stops_1_accuracy: 0.5292\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.27, delta = 0.26 \n",
      "Total Val Loss 10.168\n",
      "12269/12269 [==============================] - 128s 10ms/sample - loss: 2.5989 - starts_0_loss: 2.6456 - stops_0_loss: 2.5283 - starts_1_loss: 2.6209 - stops_1_loss: 2.5959 - starts_0_accuracy: 0.4435 - stops_0_accuracy: 0.4701 - starts_1_accuracy: 0.4481 - stops_1_accuracy: 0.5292 - val_loss: 2.5452 - val_starts_0_loss: 2.5197 - val_stops_0_loss: 2.4405 - val_starts_1_loss: 2.6256 - val_stops_1_loss: 2.5821 - val_starts_0_accuracy: 0.4741 - val_stops_0_accuracy: 0.4834 - val_starts_1_accuracy: 0.4477 - val_stops_1_accuracy: 0.5281\n",
      "Epoch 3/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5892 - starts_0_loss: 2.6303 - stops_0_loss: 2.5256 - starts_1_loss: 2.6101 - stops_1_loss: 2.5853 - starts_0_accuracy: 0.4453 - stops_0_accuracy: 0.4716 - starts_1_accuracy: 0.4562 - stops_1_accuracy: 0.5329\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.157\n",
      "12269/12269 [==============================] - 128s 10ms/sample - loss: 2.5892 - starts_0_loss: 2.6304 - stops_0_loss: 2.5254 - starts_1_loss: 2.6104 - stops_1_loss: 2.5854 - starts_0_accuracy: 0.4453 - stops_0_accuracy: 0.4716 - starts_1_accuracy: 0.4562 - stops_1_accuracy: 0.5329 - val_loss: 2.5424 - val_starts_0_loss: 2.5181 - val_stops_0_loss: 2.4522 - val_starts_1_loss: 2.6147 - val_stops_1_loss: 2.5716 - val_starts_0_accuracy: 0.4724 - val_stops_0_accuracy: 0.4782 - val_starts_1_accuracy: 0.4609 - val_stops_1_accuracy: 0.5330\n",
      "Epoch 4/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5787 - starts_0_loss: 2.6258 - stops_0_loss: 2.5166 - starts_1_loss: 2.5964 - stops_1_loss: 2.5716 - starts_0_accuracy: 0.4541 - stops_0_accuracy: 0.4773 - starts_1_accuracy: 0.4590 - stops_1_accuracy: 0.5354\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.142\n",
      "12269/12269 [==============================] - 129s 10ms/sample - loss: 2.5786 - starts_0_loss: 2.6258 - stops_0_loss: 2.5165 - starts_1_loss: 2.5962 - stops_1_loss: 2.5716 - starts_0_accuracy: 0.4541 - stops_0_accuracy: 0.4773 - starts_1_accuracy: 0.4590 - stops_1_accuracy: 0.5354 - val_loss: 2.5382 - val_starts_0_loss: 2.5145 - val_stops_0_loss: 2.4493 - val_starts_1_loss: 2.5997 - val_stops_1_loss: 2.5781 - val_starts_0_accuracy: 0.4760 - val_stops_0_accuracy: 0.4804 - val_starts_1_accuracy: 0.4648 - val_stops_1_accuracy: 0.5345\n",
      "Epoch 5/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5768 - starts_0_loss: 2.6168 - stops_0_loss: 2.5238 - starts_1_loss: 2.5910 - stops_1_loss: 2.5720 - starts_0_accuracy: 0.4614 - stops_0_accuracy: 0.4715 - starts_1_accuracy: 0.4639 - stops_1_accuracy: 0.5346\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.154\n",
      "12269/12269 [==============================] - 128s 10ms/sample - loss: 2.5770 - starts_0_loss: 2.6174 - stops_0_loss: 2.5240 - starts_1_loss: 2.5911 - stops_1_loss: 2.5722 - starts_0_accuracy: 0.4613 - stops_0_accuracy: 0.4716 - starts_1_accuracy: 0.4640 - stops_1_accuracy: 0.5345 - val_loss: 2.5410 - val_starts_0_loss: 2.5443 - val_stops_0_loss: 2.4435 - val_starts_1_loss: 2.6007 - val_stops_1_loss: 2.5653 - val_starts_0_accuracy: 0.4746 - val_stops_0_accuracy: 0.4785 - val_starts_1_accuracy: 0.4677 - val_stops_1_accuracy: 0.5269\n",
      "Epoch 6/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5658 - starts_0_loss: 2.6143 - stops_0_loss: 2.5060 - starts_1_loss: 2.5875 - stops_1_loss: 2.5502 - starts_0_accuracy: 0.4575 - stops_0_accuracy: 0.4842 - starts_1_accuracy: 0.4668 - stops_1_accuracy: 0.5438\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.24, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.181\n",
      "12269/12269 [==============================] - 128s 10ms/sample - loss: 2.5656 - starts_0_loss: 2.6143 - stops_0_loss: 2.5055 - starts_1_loss: 2.5875 - stops_1_loss: 2.5497 - starts_0_accuracy: 0.4575 - stops_0_accuracy: 0.4845 - starts_1_accuracy: 0.4666 - stops_1_accuracy: 0.5440 - val_loss: 2.5475 - val_starts_0_loss: 2.5148 - val_stops_0_loss: 2.4743 - val_starts_1_loss: 2.6189 - val_stops_1_loss: 2.5729 - val_starts_0_accuracy: 0.4709 - val_stops_0_accuracy: 0.4655 - val_starts_1_accuracy: 0.4606 - val_stops_1_accuracy: 0.5196\n",
      "Epoch 7/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5583 - starts_0_loss: 2.6054 - stops_0_loss: 2.5039 - starts_1_loss: 2.5724 - stops_1_loss: 2.5491 - starts_0_accuracy: 0.4657 - stops_0_accuracy: 0.4834 - starts_1_accuracy: 0.4761 - stops_1_accuracy: 0.5406\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.24, gamma = 0.26, delta = 0.25 \n",
      "Total Val Loss 10.164\n",
      "12269/12269 [==============================] - 129s 11ms/sample - loss: 2.5583 - starts_0_loss: 2.6057 - stops_0_loss: 2.5036 - starts_1_loss: 2.5726 - stops_1_loss: 2.5487 - starts_0_accuracy: 0.4656 - stops_0_accuracy: 0.4836 - starts_1_accuracy: 0.4761 - stops_1_accuracy: 0.5408 - val_loss: 2.5431 - val_starts_0_loss: 2.5315 - val_stops_0_loss: 2.4667 - val_starts_1_loss: 2.6090 - val_stops_1_loss: 2.5566 - val_starts_0_accuracy: 0.4731 - val_stops_0_accuracy: 0.4846 - val_starts_1_accuracy: 0.4675 - val_stops_1_accuracy: 0.5333\n",
      "Epoch 8/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5483 - starts_0_loss: 2.5990 - stops_0_loss: 2.4965 - starts_1_loss: 2.5648 - stops_1_loss: 2.5296 - starts_0_accuracy: 0.4691 - stops_0_accuracy: 0.4858 - starts_1_accuracy: 0.4823 - stops_1_accuracy: 0.5463\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.189\n",
      "12269/12269 [==============================] - 146s 12ms/sample - loss: 2.5485 - starts_0_loss: 2.5991 - stops_0_loss: 2.4969 - starts_1_loss: 2.5649 - stops_1_loss: 2.5301 - starts_0_accuracy: 0.4691 - stops_0_accuracy: 0.4855 - starts_1_accuracy: 0.4823 - stops_1_accuracy: 0.5461 - val_loss: 2.5498 - val_starts_0_loss: 2.5407 - val_stops_0_loss: 2.4511 - val_starts_1_loss: 2.6217 - val_stops_1_loss: 2.5756 - val_starts_0_accuracy: 0.4650 - val_stops_0_accuracy: 0.4711 - val_starts_1_accuracy: 0.4501 - val_stops_1_accuracy: 0.5249\n",
      "Epoch 9/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5415 - starts_0_loss: 2.5926 - stops_0_loss: 2.4972 - starts_1_loss: 2.5537 - stops_1_loss: 2.5191 - starts_0_accuracy: 0.4719 - stops_0_accuracy: 0.4966 - starts_1_accuracy: 0.4829 - stops_1_accuracy: 0.5545\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.18\n",
      "12269/12269 [==============================] - 130s 11ms/sample - loss: 2.5412 - starts_0_loss: 2.5923 - stops_0_loss: 2.4970 - starts_1_loss: 2.5534 - stops_1_loss: 2.5189 - starts_0_accuracy: 0.4719 - stops_0_accuracy: 0.4965 - starts_1_accuracy: 0.4829 - stops_1_accuracy: 0.5544 - val_loss: 2.5478 - val_starts_0_loss: 2.5382 - val_stops_0_loss: 2.4560 - val_starts_1_loss: 2.6088 - val_stops_1_loss: 2.5770 - val_starts_0_accuracy: 0.4677 - val_stops_0_accuracy: 0.4848 - val_starts_1_accuracy: 0.4628 - val_stops_1_accuracy: 0.5279\n",
      "Epoch 10/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5319 - starts_0_loss: 2.5825 - stops_0_loss: 2.4906 - starts_1_loss: 2.5394 - stops_1_loss: 2.5127 - starts_0_accuracy: 0.4820 - stops_0_accuracy: 0.4969 - starts_1_accuracy: 0.4920 - stops_1_accuracy: 0.5552\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.188\n",
      "12269/12269 [==============================] - 129s 10ms/sample - loss: 2.5321 - starts_0_loss: 2.5828 - stops_0_loss: 2.4907 - starts_1_loss: 2.5399 - stops_1_loss: 2.5130 - starts_0_accuracy: 0.4820 - stops_0_accuracy: 0.4967 - starts_1_accuracy: 0.4921 - stops_1_accuracy: 0.5551 - val_loss: 2.5497 - val_starts_0_loss: 2.5462 - val_stops_0_loss: 2.4471 - val_starts_1_loss: 2.6162 - val_stops_1_loss: 2.5781 - val_starts_0_accuracy: 0.4711 - val_stops_0_accuracy: 0.4844 - val_starts_1_accuracy: 0.4599 - val_stops_1_accuracy: 0.5291\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 12269 samples, validate on 4090 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.4465 - starts_0_loss: 2.4720 - stops_0_loss: 2.3367 - starts_1_loss: 2.5004 - stops_1_loss: 2.4654 - starts_0_accuracy: 0.4844 - stops_0_accuracy: 0.5262 - starts_1_accuracy: 0.4932 - stops_1_accuracy: 0.5631\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.23, gamma = 0.27, delta = 0.27 \n",
      "Total Val Loss 9.429\n",
      "12269/12269 [==============================] - 298s 24ms/sample - loss: 2.4467 - starts_0_loss: 2.4720 - stops_0_loss: 2.3369 - starts_1_loss: 2.5007 - stops_1_loss: 2.4658 - starts_0_accuracy: 0.4842 - stops_0_accuracy: 0.5260 - starts_1_accuracy: 0.4932 - stops_1_accuracy: 0.5630 - val_loss: 2.3609 - val_starts_0_loss: 2.3032 - val_stops_0_loss: 2.2428 - val_starts_1_loss: 2.4386 - val_stops_1_loss: 2.4442 - val_starts_0_accuracy: 0.5357 - val_stops_0_accuracy: 0.5795 - val_starts_1_accuracy: 0.5181 - val_stops_1_accuracy: 0.5924\n",
      "Epoch 2/3\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.2911 - starts_0_loss: 2.2989 - stops_0_loss: 2.1785 - starts_1_loss: 2.3629 - stops_1_loss: 2.3082 - starts_0_accuracy: 0.5459 - stops_0_accuracy: 0.5887 - starts_1_accuracy: 0.5558 - stops_1_accuracy: 0.6177\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.22, gamma = 0.28, delta = 0.26 \n",
      "Total Val Loss 9.241\n",
      "12269/12269 [==============================] - 278s 23ms/sample - loss: 2.2913 - starts_0_loss: 2.2991 - stops_0_loss: 2.1785 - starts_1_loss: 2.3632 - stops_1_loss: 2.3085 - starts_0_accuracy: 0.5458 - stops_0_accuracy: 0.5886 - starts_1_accuracy: 0.5558 - stops_1_accuracy: 0.6177 - val_loss: 2.3170 - val_starts_0_loss: 2.2798 - val_stops_0_loss: 2.1665 - val_starts_1_loss: 2.4330 - val_stops_1_loss: 2.3618 - val_starts_0_accuracy: 0.5443 - val_stops_0_accuracy: 0.5780 - val_starts_1_accuracy: 0.5130 - val_stops_1_accuracy: 0.5995\n",
      "Epoch 3/3\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.1980 - starts_0_loss: 2.1996 - stops_0_loss: 2.0934 - starts_1_loss: 2.2672 - stops_1_loss: 2.2122 - starts_0_accuracy: 0.5860 - stops_0_accuracy: 0.6328 - starts_1_accuracy: 0.5960 - stops_1_accuracy: 0.6554\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.22, gamma = 0.27, delta = 0.26 \n",
      "Total Val Loss 9.504\n",
      "12269/12269 [==============================] - 284s 23ms/sample - loss: 2.1981 - starts_0_loss: 2.1997 - stops_0_loss: 2.0934 - starts_1_loss: 2.2675 - stops_1_loss: 2.2123 - starts_0_accuracy: 0.5862 - stops_0_accuracy: 0.6328 - starts_1_accuracy: 0.5961 - stops_1_accuracy: 0.6552 - val_loss: 2.3832 - val_starts_0_loss: 2.3628 - val_stops_0_loss: 2.2407 - val_starts_1_loss: 2.4812 - val_stops_1_loss: 2.4188 - val_starts_0_accuracy: 0.5279 - val_stops_0_accuracy: 0.5531 - val_starts_1_accuracy: 0.5232 - val_stops_1_accuracy: 0.5758\n",
      "[INFO]  =============== Validation for FOLD# 0 ===============\n",
      "[INFO] 66.18 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 69.17 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 52.79 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 55.31 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 56.25 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 66.19 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 41.84 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 51.36 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 56.69 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 64.47 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 44.44 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 49.64 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 57.32 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 68.91 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 40.36 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 55.26 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V39_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V39_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V39_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V39_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (12269, 77) (12269, 77)\n",
      "[INFO] Prediction shape for validation data:  (4090, 77) (4090, 77)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  9508 out of 12269\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  3193 out of 4090\n",
      "[INFO] Training Jaccard Score:  0.28393638311280756\n",
      "[INFO] Validation Jaccard Score:  0.2871192341494632\n",
      "[INFO] Training for fold: 0 finished at Mon Jun 15 19:51:26 2020\n",
      "Mon Jun 15 19:51:26 2020\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "alpha = K.variable(0.25)\n",
    "beta = K.variable(0.25)\n",
    "gamma = K.variable(0.25)\n",
    "delta = K.variable(0.75)\n",
    "\n",
    "span_mcp = ModelCheckpoint(filepath=RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\", monitor='val_loss',\n",
    "                           verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "span_csvl = CSVLogger(filename=RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(num)+\".csv\",\n",
    "                      separator=\",\",\n",
    "                      append=True)\n",
    "\n",
    "print(\"[INFO] Training only the final layers at higher learning rates.\")\n",
    "span_detection_model.layers[3].trainable = False\n",
    "adam = Adam(learning_rate=MAX_LR)\n",
    "span_detection_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=LABEL_SMOOTHING_PARAM),\n",
    "                             optimizer=adam,\n",
    "                             metrics=['accuracy'],\n",
    "                             loss_weights={\"starts_0\":alpha,\"stops_0\":beta,\"starts_1\":gamma,\"stops_1\":delta})\n",
    "span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                           \"words\":X_span[t_index],\n",
    "                                           \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                        y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                           \"stops_0\":Y_span_stops[t_index], \n",
    "                                           \"starts_1\":Y_span_starts[t_index],\n",
    "                                           \"stops_1\":Y_span_stops[t_index]},\n",
    "                                        shuffle=True,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        epochs=NUM_EPOCHS[0],\n",
    "                                        validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                          \"words\":X_span[v_index],\n",
    "                                                          \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                         {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                          \"stops_0\":Y_span_stops[v_index], \n",
    "                                                          \"starts_1\":Y_span_starts[v_index],\n",
    "                                                          \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                        verbose=1,\n",
    "                                        callbacks=[span_mcp, span_csvl, LossWeightAdjust(alpha=alpha, beta=beta, gamma=gamma, delta=delta)])\n",
    "\n",
    "print(\"[INFO] Training only the final layers at lower learning rates.\")\n",
    "adam = Adam(learning_rate=MID_LR)\n",
    "span_detection_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=LABEL_SMOOTHING_PARAM),\n",
    "                             optimizer=adam,\n",
    "                             metrics=['accuracy'],\n",
    "                             loss_weights={\"starts_0\":alpha,\"stops_0\":beta,\"starts_1\":gamma,\"stops_1\":delta})\n",
    "span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                           \"words\":X_span[t_index],\n",
    "                                           \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                        y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                           \"stops_0\":Y_span_stops[t_index],\n",
    "                                           \"starts_1\":Y_span_starts[t_index],\n",
    "                                           \"stops_1\":Y_span_stops[t_index]},\n",
    "                                        shuffle=True,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        epochs=NUM_EPOCHS[1],\n",
    "                                        validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                          \"words\":X_span[v_index],\n",
    "                                                          \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                         {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                          \"stops_0\":Y_span_stops[v_index], \n",
    "                                                          \"starts_1\":Y_span_starts[v_index],\n",
    "                                                          \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                        verbose=1,\n",
    "                                        callbacks=[span_mcp, span_csvl, LossWeightAdjust(alpha=alpha, beta=beta, gamma=gamma, delta=delta)])\n",
    "\n",
    "print(\"[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\")\n",
    "span_detection_model.layers[3].trainable = True\n",
    "adam = Adam(learning_rate=MIN_LR)\n",
    "span_detection_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=LABEL_SMOOTHING_PARAM),\n",
    "                             optimizer=adam,\n",
    "                             metrics=['accuracy'],\n",
    "                             loss_weights={\"starts_0\":alpha,\"stops_0\":beta,\"starts_1\":gamma,\"stops_1\":delta})\n",
    "span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                           \"words\":X_span[t_index],\n",
    "                                           \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                        y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                           \"stops_0\":Y_span_stops[t_index],\n",
    "                                           \"starts_1\":Y_span_starts[t_index],\n",
    "                                           \"stops_1\":Y_span_stops[t_index]},\n",
    "                                        shuffle=True,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        epochs=NUM_EPOCHS[2],\n",
    "                                        validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                          \"words\":X_span[v_index],\n",
    "                                                          \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                         {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                          \"stops_0\":Y_span_stops[v_index], \n",
    "                                                          \"starts_1\":Y_span_starts[v_index],\n",
    "                                                          \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                        verbose=1,\n",
    "                                        callbacks=[span_mcp, span_csvl, LossWeightAdjust(alpha=alpha, beta=beta, gamma=gamma, delta=delta)])\n",
    "\n",
    "# Loading best weights per fold\n",
    "span_detection_model.load_weights(RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\")\n",
    "\n",
    "pred_train = span_detection_model.predict(x = {\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                          batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "pred_val = span_detection_model.predict(x = {\"att_flags\":X_span_att[v_index],\n",
    "                                             \"words\":X_span[v_index],\n",
    "                                             \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                        batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "pred_starts_train, pred_stops_train = (pred_train[0]/2.0 + pred_train[2]/2.0), (pred_train[1]/2.0 + pred_train[3]/2.0)\n",
    "pred_starts_val, pred_stops_val = (pred_val[0]/2.0 + pred_val[2]/2.0), (pred_val[1]/2.0 + pred_val[3]/2.0)\n",
    "\n",
    "# Accumulate test results after training every fold\n",
    "pred_test_fold = span_detection_model.predict(x = {\"att_flags\":X_span_att_test,\n",
    "                                                   \"words\":X_span_test,\n",
    "                                                   \"token_ids\":np.zeros_like(X_span_att_test)},\n",
    "                                              batch_size=PREDICT_BATCH_SIZE)\n",
    "if num==0:\n",
    "    pred_test = []\n",
    "    pred_test.append(pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "    pred_test.append(pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "\n",
    "# Tabulate\n",
    "preds = {\n",
    "    \"train\":{\n",
    "        \"starts\":{\n",
    "            \"y_true\":Y_span_starts[t_index].argmax(axis=1),\n",
    "            \"y_pred\":pred_train[0].argmax(axis=1)\n",
    "        },\n",
    "        \"stops\":{\n",
    "            \"y_true\":Y_span_stops[t_index].argmax(axis=1),\n",
    "            \"y_pred\":pred_train[1].argmax(axis=1)\n",
    "        }\n",
    "    },\n",
    "    \"valid\":{\n",
    "        \"starts\":{\n",
    "            \"y_true\":Y_span_starts[v_index].argmax(axis=1),\n",
    "            \"y_pred\":pred_val[0].argmax(axis=1)\n",
    "        },\n",
    "        \"stops\":{\n",
    "            \"y_true\":Y_span_stops[v_index].argmax(axis=1),\n",
    "            \"y_pred\":pred_val[1].argmax(axis=1)\n",
    "        }        \n",
    "    }\n",
    "}\n",
    "\n",
    "print_metrics(pred_dict=preds)\n",
    "\n",
    "print(\"[INFO] Prediction shape for training data: \", pred_starts_train.shape, pred_stops_train.shape)\n",
    "print(\"[INFO] Prediction shape for validation data: \", pred_starts_val.shape, pred_stops_val.shape)\n",
    "\n",
    "print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for training data: \",\n",
    "      sum([s<e for s,e in zip(pred_starts_train.argmax(axis=1),\n",
    "                              pred_stops_train.argmax(axis=1))]),\n",
    "      \"out of\", pred_starts_train.shape[0])\n",
    "print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for validation data: \",\n",
    "      sum([s<e for s,e in zip(pred_starts_val.argmax(axis=1),\n",
    "                              pred_stops_val.argmax(axis=1))]),\n",
    "      \"out of\", pred_starts_val.shape[0],)\n",
    "\n",
    "pred_words_train = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[t_index],\n",
    "                                                                                                                               pred_starts_train.argmax(axis=1),\n",
    "                                                                                                                               pred_stops_train.argmax(axis=1))]\n",
    "\n",
    "pred_words_val = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[v_index],\n",
    "                                                                                                                             pred_starts_val.argmax(axis=1),\n",
    "                                                                                                                             pred_stops_val.argmax(axis=1))]\n",
    "\n",
    "print(\"[INFO] Training Jaccard Score: \",\n",
    "      np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in t_index],\n",
    "                                                      pred_words_train)]))\n",
    "print(\"[INFO] Validation Jaccard Score: \",\n",
    "      np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in v_index],\n",
    "                                                      pred_words_val)]))\n",
    "print(\"[INFO] Training for fold:\", num, \"finished at\", ctime(time()))\n",
    "\n",
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Prediction shape for testing data:  (3534, 77) (3534, 77)\n"
     ]
    }
   ],
   "source": [
    "pred_starts_test, pred_stops_test = pred_test[0]/NUM_FOLDS, pred_test[1]/NUM_FOLDS\n",
    "print(\"[INFO] Prediction shape for testing data: \", pred_starts_test.shape, pred_stops_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal predictions (StartIndex less than EndIndex) for testing data: 2978 out of 3534\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal predictions (StartIndex less than EndIndex) for testing data:\",\n",
    "      sum([s<e for s,e in zip(pred_starts_test.argmax(axis=1),\n",
    "                              pred_stops_test.argmax(axis=1))]), \n",
    "      \"out of\",\n",
    "      pred_starts_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words_test = [\n",
    "    post_process(tokenizer.decode(t[s:e+1])) if s<e else post_process(tokenizer.decode(t[e:])) for t,s,e in zip(X_span_test,\n",
    "                                                                                                              pred_starts_test.argmax(axis=1),\n",
    "                                                                                                              pred_stops_test.argmax(axis=1))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sad i missed  going away party due to too much work, but i really am gonna miss that kid  negative \n",
      "1\n",
      "1\n",
      "sad\n"
     ]
    }
   ],
   "source": [
    "check_idx = 1111\n",
    "#print([[t,i,j,k] for t,i,j,k in zip(tokenizer.decode(),X_test[check_idx],pred_starts_test[check_idx],pred_stops_test[check_idx])])\n",
    "print(tokenizer.decode(X_span_test[check_idx]))\n",
    "print(pred_starts_test.argmax(axis=1)[check_idx])\n",
    "print(pred_stops_test.argmax(axis=1)[check_idx])\n",
    "print(post_process(tokenizer.decode(X_span_test[check_idx][pred_starts_test.argmax(axis=1)[check_idx]:1+pred_stops_test.argmax(axis=1)[check_idx]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span['selected_text'] = pred_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[\"selected_text\"] = np.where(test_df_span[\"sentiment\"] == \"neutral\",\n",
    "                                         test_df_span[\"text\"],\n",
    "                                         test_df_span[\"selected_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[[\"textID\", \"selected_text\"]].to_csv(RESULTS_DIR+\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>Ahh... I love Chinese music. Haha.  Not gonna see my luff til Thursday...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>_Wright THANK YOU! 'Return of the Mack' is THE jam!</td>\n",
       "      <td>positive</td>\n",
       "      <td>thank you!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>try #fireflight first... female fronted metal is awesome..  ..  Unbreakable album would be a good start!</td>\n",
       "      <td>positive</td>\n",
       "      <td>awesome..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>The sun is shining  Have a great day</td>\n",
       "      <td>positive</td>\n",
       "      <td>have a great day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>she lookd soo freaked out!!poor wee thing</td>\n",
       "      <td>negative</td>\n",
       "      <td>freaked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>lmao you witty wacko...loves it</td>\n",
       "      <td>positive</td>\n",
       "      <td>wacko...loves it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3493</th>\n",
       "      <td>airsoft is so much fun! i play with my brothers and it`s a great bonding experience.</td>\n",
       "      <td>positive</td>\n",
       "      <td>fun!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>I hate cottage cheese.  I even got some fancy stuff last weekend with lemon and berries...I could only eat half of it.</td>\n",
       "      <td>negative</td>\n",
       "      <td>i hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>burning cd`s,,,,,,,,, **** outa blank disc`s</td>\n",
       "      <td>positive</td>\n",
       "      <td>**** outa blank disc`s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>Aren`t we though? lol He`s at work now and I miss him  blah Wut cha up to?</td>\n",
       "      <td>negative</td>\n",
       "      <td>i miss him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>wowie wooie someone updated their twitter without me having to remind you. I`m not even mad,I`m impressed.  how was the lasagna ?</td>\n",
       "      <td>positive</td>\n",
       "      <td>mad,i`m impressed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>bye LA... I already miss you</td>\n",
       "      <td>negative</td>\n",
       "      <td>i already miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>Good morning tweeps. Busy this a.m. but not in a working way</td>\n",
       "      <td>positive</td>\n",
       "      <td>good morning tweeps. busy this a.m. but not in a working way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>sick to my stomach.. and i have a headache.. i wish someone could come rub my temples.</td>\n",
       "      <td>negative</td>\n",
       "      <td>sick to my stomach.. and i have a headache..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>no internet for a week or longer</td>\n",
       "      <td>negative</td>\n",
       "      <td>no internet for a week or longer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3459</th>\n",
       "      <td>OMG - Madness Just Came On The Radio</td>\n",
       "      <td>negative</td>\n",
       "      <td>madness just came on the radio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3453</th>\n",
       "      <td>LOL! Glad you like it!</td>\n",
       "      <td>positive</td>\n",
       "      <td>glad you like it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>sucks that i don`t have a drawing tablet!</td>\n",
       "      <td>negative</td>\n",
       "      <td>sucks that i don`t have a drawing tablet!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>no other reason? I wonder if she loves you</td>\n",
       "      <td>positive</td>\n",
       "      <td>i wonder if she loves you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>_e thanks for following me. Nice to meet you</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks for following me. nice to meet you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2678</th>\n",
       "      <td>I don`t wanna work tonight</td>\n",
       "      <td>negative</td>\n",
       "      <td>i don`t wanna work tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2637</th>\n",
       "      <td>tried to swat a fly with my Buddhist magazine... bad karma</td>\n",
       "      <td>negative</td>\n",
       "      <td>bad karma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>Mommas day is may 10th! Don`t forget to do something nice for your mommyyy</td>\n",
       "      <td>positive</td>\n",
       "      <td>don`t forget to do something nice for your mommyyy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>just got home from a nice party, just not tired yet</td>\n",
       "      <td>positive</td>\n",
       "      <td>nice party,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>lol that was a great movie</td>\n",
       "      <td>positive</td>\n",
       "      <td>great movie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    text  \\\n",
       "1234                                                           Ahh... I love Chinese music. Haha.  Not gonna see my luff til Thursday...   \n",
       "227                                                                                  _Wright THANK YOU! 'Return of the Mack' is THE jam!   \n",
       "2101                            try #fireflight first... female fronted metal is awesome..  ..  Unbreakable album would be a good start!   \n",
       "2933                                                                                                The sun is shining  Have a great day   \n",
       "1769                                                                                           she lookd soo freaked out!!poor wee thing   \n",
       "1472                                                                                                     lmao you witty wacko...loves it   \n",
       "3493                                                airsoft is so much fun! i play with my brothers and it`s a great bonding experience.   \n",
       "1086              I hate cottage cheese.  I even got some fancy stuff last weekend with lemon and berries...I could only eat half of it.   \n",
       "1030                                                                                        burning cd`s,,,,,,,,, **** outa blank disc`s   \n",
       "1934                                                          Aren`t we though? lol He`s at work now and I miss him  blah Wut cha up to?   \n",
       "502    wowie wooie someone updated their twitter without me having to remind you. I`m not even mad,I`m impressed.  how was the lasagna ?   \n",
       "2893                                                                                                        bye LA... I already miss you   \n",
       "466                                                                         Good morning tweeps. Busy this a.m. but not in a working way   \n",
       "204                                               sick to my stomach.. and i have a headache.. i wish someone could come rub my temples.   \n",
       "430                                                                                                     no internet for a week or longer   \n",
       "3459                                                                                                OMG - Madness Just Came On The Radio   \n",
       "3453                                                                                                              LOL! Glad you like it!   \n",
       "2256                                                                                           sucks that i don`t have a drawing tablet!   \n",
       "486                                                                                           no other reason? I wonder if she loves you   \n",
       "1131                                                                                        _e thanks for following me. Nice to meet you   \n",
       "2678                                                                                                          I don`t wanna work tonight   \n",
       "2637                                                                          tried to swat a fly with my Buddhist magazine... bad karma   \n",
       "1922                                                          Mommas day is may 10th! Don`t forget to do something nice for your mommyyy   \n",
       "134                                                                                  just got home from a nice party, just not tired yet   \n",
       "673                                                                                                           lol that was a great movie   \n",
       "\n",
       "     sentiment                                                 selected_text  \n",
       "1234  positive                                                        i love  \n",
       "227   positive                                                    thank you!  \n",
       "2101  positive                                                     awesome..  \n",
       "2933  positive                                              have a great day  \n",
       "1769  negative                                                       freaked  \n",
       "1472  positive                                              wacko...loves it  \n",
       "3493  positive                                                          fun!  \n",
       "1086  negative                                                        i hate  \n",
       "1030  positive                                        **** outa blank disc`s  \n",
       "1934  negative                                                    i miss him  \n",
       "502   positive                                            mad,i`m impressed.  \n",
       "2893  negative                                            i already miss you  \n",
       "466   positive  good morning tweeps. busy this a.m. but not in a working way  \n",
       "204   negative                  sick to my stomach.. and i have a headache..  \n",
       "430   negative                              no internet for a week or longer  \n",
       "3459  negative                                madness just came on the radio  \n",
       "3453  positive                                             glad you like it!  \n",
       "2256  negative                     sucks that i don`t have a drawing tablet!  \n",
       "486   positive                                     i wonder if she loves you  \n",
       "1131  positive                     thanks for following me. nice to meet you  \n",
       "2678  negative                                    i don`t wanna work tonight  \n",
       "2637  negative                                                     bad karma  \n",
       "1922  positive            don`t forget to do something nice for your mommyyy  \n",
       "134   positive                                                   nice party,  \n",
       "673   positive                                                   great movie  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_span.loc[test_df_span.sentiment!=\"neutral\"][[\"text\", \"sentiment\",\"selected_text\"]].sample(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
