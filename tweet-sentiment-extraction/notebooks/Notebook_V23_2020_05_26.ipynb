{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V23\"\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 16\n",
    "PREDICT_BATCH_SIZE = 512\n",
    "DROPOUT = 0.3\n",
    "\n",
    "RUN_ON_SAMPLE = False\n",
    "EXCLUDE_NEUTRAL_CLASS = True\n",
    "SENTIMENT_MAX_LR = 5e-3\n",
    "SENTIMENT_MIN_LR = 5e-5\n",
    "SENTIMENT_NUM_EPOCHS = [3, 3]\n",
    "MAX_LR = 5e-3\n",
    "MID_LR = 1e-4\n",
    "MIN_LR = 1e-6\n",
    "NUM_EPOCHS = [3, 3, 3]\n",
    "NUM_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"../results/\"\n",
    "DATA_DIR = \"../data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, MaxPooling1D, Layer, AdditiveAttention, Attention\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    a = set(str1)\n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeded_value = 654123\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 31 14:57:24 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0', '2.8.0', '0.5.2']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\"  target=\"_blank\"><h2 id=\"limiting_gpu_memory_growth\" data-text=\"Limiting GPU memory growth\" tabindex=\"0\">Limiting GPU memory growth</h2></a>\n",
    "<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\"><code translate=\"no\" dir=\"ltr\">CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the <code translate=\"no\" dir=\"ltr\">tf.config.experimental.set_visible_devices</code> method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it okay to exclude neutral text from models and force it later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVERAGE: 0.5334592552617378\n",
      "MEAN JACCARD: 0.9808316180492903\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10212</th>\n",
       "      <td>4c28a5cc39</td>\n",
       "      <td>tee we beefin....what was u supposed to do before leavin????</td>\n",
       "      <td>tee we beefin....what was u supposed to do before leavin????</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>942f5f4cc6</td>\n",
       "      <td>we`ll see Craig might just hog u guys! haha I work till 10 tonight anyways</td>\n",
       "      <td>we`ll see Craig might just hog u guys! haha I work till 10 tonight anyways</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9462</th>\n",
       "      <td>8de55e944a</td>\n",
       "      <td>i feel like a nerd saying it, but the new star trek looks kinda cool, i might see it haha. Let me know how it goes!</td>\n",
       "      <td>i feel like a nerd saying it, but the new star trek looks kinda cool, i might see it haha. Let me know how it goes!</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>bf10294a83</td>\n",
       "      <td>u just got me real excited cause i thought teniece was on twitter...</td>\n",
       "      <td>u just got me real excited cause i thought teniece was on twitter...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2870</th>\n",
       "      <td>86418f2ca6</td>\n",
       "      <td>I was sad to hear they shaved too  I really liked the beards. I feel like I`m in the minority.</td>\n",
       "      <td>I was sad to hear they shaved too  I really liked the beards. I feel like I`m in the minority.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID  \\\n",
       "10212  4c28a5cc39   \n",
       "2419   942f5f4cc6   \n",
       "9462   8de55e944a   \n",
       "4083   bf10294a83   \n",
       "2870   86418f2ca6   \n",
       "\n",
       "                                                                                                                       text  \\\n",
       "10212                                                          tee we beefin....what was u supposed to do before leavin????   \n",
       "2419                                             we`ll see Craig might just hog u guys! haha I work till 10 tonight anyways   \n",
       "9462    i feel like a nerd saying it, but the new star trek looks kinda cool, i might see it haha. Let me know how it goes!   \n",
       "4083                                                   u just got me real excited cause i thought teniece was on twitter...   \n",
       "2870                         I was sad to hear they shaved too  I really liked the beards. I feel like I`m in the minority.   \n",
       "\n",
       "                                                                                                             selected_text  \\\n",
       "10212                                                         tee we beefin....what was u supposed to do before leavin????   \n",
       "2419                                            we`ll see Craig might just hog u guys! haha I work till 10 tonight anyways   \n",
       "9462   i feel like a nerd saying it, but the new star trek looks kinda cool, i might see it haha. Let me know how it goes!   \n",
       "4083                                                  u just got me real excited cause i thought teniece was on twitter...   \n",
       "2870                        I was sad to hear they shaved too  I really liked the beards. I feel like I`m in the minority.   \n",
       "\n",
       "      sentiment  \n",
       "10212   neutral  \n",
       "2419    neutral  \n",
       "9462    neutral  \n",
       "4083    neutral  \n",
       "2870    neutral  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\")\n",
    "# cases where neutral's text and selected text columns are the same\n",
    "neutrals = df2.loc[df2.sentiment==\"neutral\"].copy()\n",
    "neutrals = neutrals.reset_index(drop=True)\n",
    "neutrals[\"text\"] = neutrals[\"text\"].astype(str)\n",
    "neutrals[\"selected_text\"] = neutrals[\"selected_text\"].astype(str)\n",
    "print(\"COVERAGE:\", np.sum(np.where((neutrals.text == neutrals.selected_text), 1, 0))/neutrals.shape[0])\n",
    "print(\"MEAN JACCARD:\", np.mean([jaccard(str1=i, str2=j) for i,j in zip(neutrals.selected_text,neutrals.text)]))\n",
    "\n",
    "neutrals.loc[neutrals.text != neutrals.selected_text].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  1\n",
      "selected_text  object  1\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27480, 'selected_text': 22463, 'sentiment': 3}\n",
      "            textID                           text selected_text sentiment\n",
      "count        27481                          27480         27480     27481\n",
      "unique       27481                          27480         22463         3\n",
      "top     8f8ac1bc28   I hope to see you soon again          good   neutral\n",
      "freq             1                              1           199     11118\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\")\n",
    "\n",
    "print(pd.concat((df.dtypes, df.isna().sum()), axis=1))\n",
    "print(df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df[i].nunique() for i in df.columns})\n",
    "print(df.describe())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID                                   text sentiment\n",
      "count         3534                                   3534      3534\n",
      "unique        3534                                   3534         3\n",
      "top     09554aef59  Ill catch you at the very last second   neutral\n",
      "freq             1                                      1      1430\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(DATA_DIR+\"test.csv\", encoding=\"utf8\")\n",
    "print(pd.concat((test_df.dtypes, test_df.isna().sum()), axis=1))\n",
    "print(test_df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df[i].nunique() for i in test_df.columns})\n",
    "print(test_df.describe())\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>c77717b103</td>\n",
       "      <td>I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½</td>\n",
       "      <td>I love to!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>28dbada620</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "44   c77717b103   \n",
       "192  28dbada620   \n",
       "\n",
       "                                                                                                       text  \\\n",
       "44    I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½   \n",
       "192                                *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "                                                                selected_text  \\\n",
       "44                                                                 I love to!   \n",
       "192  *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "    sentiment  \n",
       "44   positive  \n",
       "192   neutral  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>7223fdccc2</td>\n",
       "      <td>tikcets are only ï¿½91...each...BUT I SO WANT TO GO</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>43ad351369</td>\n",
       "      <td>AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "145  7223fdccc2   \n",
       "618  43ad351369   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "145                                                                                             tikcets are only ï¿½91...each...BUT I SO WANT TO GO   \n",
       "618  AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s   \n",
       "\n",
       "    sentiment  \n",
       "145  positive  \n",
       "618  positive  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[test_df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"set\"], test_df[\"set\"] = \"train\", \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create smaller sample for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_ON_SAMPLE:\n",
    "    df = df.sample(500).reset_index(drop=True)\n",
    "    test_df = test_df.sample(500).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine datasets for pretraining using sentiment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31015, 3)\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat((df[[\"text\",\"set\",\"sentiment\"]],\n",
    "                  test_df[[\"text\",\"set\",\"sentiment\"]]), axis=0)\n",
    "data[\"text\"] = data[\"text\"].astype(str)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment count in combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">test</th>\n",
       "      <th>negative</th>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">train</th>\n",
       "      <th>negative</th>\n",
       "      <td>7781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>11118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>8582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  text\n",
       "set   sentiment       \n",
       "test  negative    1001\n",
       "      neutral     1430\n",
       "      positive    1103\n",
       "train negative    7781\n",
       "      neutral    11118\n",
       "      positive    8582"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"set\",\"sentiment\"])[[\"text\"]].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../results/tokenizers/roberta_tokenizer/vocab.json',\n",
       " '../results/tokenizers/roberta_tokenizer/merges.txt',\n",
       " '../results/tokenizers/roberta_tokenizer/special_tokens_map.json',\n",
       " '../results/tokenizers/roberta_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer.save_pretrained(RESULTS_DIR+\"tokenizers/roberta_tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=RESULTS_DIR+\"tokenizers/roberta_tokenizer/vocab.json\",\n",
    "    merges_file=RESULTS_DIR+\"tokenizers/roberta_tokenizer/merges.txt\",\n",
    "    add_prefix_space=True,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(RESULTS_DIR+\"tokenizers/roberta_tokenizer/special_tokens_map.json\") as f:\n",
    "    special_tokens = json.load(f)\n",
    "\n",
    "tokenizer.add_special_tokens([i for i in special_tokens.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for sentiment detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lookup = {\"positive\":2,\"neutral\":1,\"negative\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': (31015, 100), 'X_att': (31015, 100), 'Y': (31015,), 'VOCAB_SIZE': 50265, 'MAX_SEQ_LEN': 100}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentiment(text_series=data.text.tolist(), sentiment_series=data.sentiment):\n",
    "\n",
    "    X_tokens = tokenizer.encode_batch(text_series)\n",
    "\n",
    "    X = [i.ids for i in X_tokens]\n",
    "    MAX_SEQ_LEN = max([len(i) for i in X])\n",
    "    X = pad_sequences(X, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "    X_att = [i.attention_mask for i in X_tokens]\n",
    "    X_att = pad_sequences(X_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "    Y = sentiment_series.apply(lambda x: sentiment_lookup[x]).values\n",
    "\n",
    "    VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "\n",
    "    print({\n",
    "        \"X\":X.shape,\n",
    "        \"X_att\":X_att.shape,\n",
    "        \"Y\":Y.shape,\n",
    "        \"VOCAB_SIZE\":VOCAB_SIZE,\n",
    "        \"MAX_SEQ_LEN\":MAX_SEQ_LEN\n",
    "    })\n",
    "    \n",
    "    return X_tokens, X, X_att, Y, VOCAB_SIZE, MAX_SEQ_LEN\n",
    "\n",
    "X_sent_tokens, X_sent, X_sent_att, Y_sent, VOCAB_SIZE, MAX_SEQ_LEN_SENT = preprocess_sentiment(**{\n",
    "    \"text_series\" : data.text.tolist(),\n",
    "    \"sentiment_series\" : data.sentiment\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  1\n",
      "selected_text  object  1\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27480, 'selected_text': 22463, 'sentiment': 3}\n",
      "            textID                           text selected_text sentiment\n",
      "count        27481                          27480         27480     27481\n",
      "unique       27481                          27480         22463         3\n",
      "top     8f8ac1bc28   I hope to see you soon again          good   neutral\n",
      "freq             1                              1           199     11118\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\")\n",
    "\n",
    "print(pd.concat((df_span.dtypes, df_span.isna().sum()), axis=1))\n",
    "print(df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df_span[i].nunique() for i in df_span.columns})\n",
    "print(df_span.describe())\n",
    "df_span.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID                                   text sentiment\n",
      "count         3534                                   3534      3534\n",
      "unique        3534                                   3534         3\n",
      "top     09554aef59  Ill catch you at the very last second   neutral\n",
      "freq             1                                      1      1430\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_span = pd.read_csv(DATA_DIR+\"test.csv\")\n",
    "print(pd.concat((test_df_span.dtypes, test_df_span.isna().sum()), axis=1))\n",
    "print(test_df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df_span[i].nunique() for i in test_df_span.columns})\n",
    "print(test_df_span.describe())\n",
    "test_df_span.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for span detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For traceability\n",
    "df_span[\"original_index\"] = df_span.index\n",
    "test_df_span[\"original_index\"] = test_df_span.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_idxs = [513, 1060, 5543, 8410, 13605, 24778, 25258] + [18, 27, 32, 39, 48, 49, 64, 66, 84, 92, 102, 116, 129, 132, 134, 149, 160, 166, 168, 189, 196, 197, 210, 223, 247, 251, 260, 285, 295, 296, 297, 309, 349, 362, 366, 382, 396, 398, 406, 410, 425, 458, 460, 482, 492, 500, 504, 517, 533, 537, 543, 569, 573, 581, 604, 605, 608, 634, 636, 637, 639, 642, 665, 670, 674, 678, 685, 702, 707, 709, 710, 727, 737, 746, 751, 757, 768, 778, 787, 794, 804, 809, 853, 854, 856, 863, 871, 872, 912, 931, 937, 942, 950, 956, 957, 963, 992, 993, 999, 1001, 1011, 1012, 1036, 1039, 1057, 1062, 1076, 1077, 1083, 1096, 1105, 1116, 1117, 1122, 1134, 1137, 1150, 1159, 1199, 1200, 1209, 1217, 1264, 1271, 1280, 1283, 1298, 1300, 1303, 1319, 1327, 1342, 1360, 1363, 1365, 1372, 1374, 1376, 1382, 1393, 1416, 1417, 1420, 1447, 1454, 1468, 1472, 1513, 1515, 1518, 1531, 1548, 1558, 1567, 1580, 1588, 1605, 1623, 1648, 1649, 1657, 1659, 1661, 1665, 1672, 1674, 1678, 1683, 1693, 1696, 1713, 1723, 1728, 1739, 1747, 1754, 1798, 1806, 1808, 1814, 1822, 1828, 1830, 1835, 1850, 1857, 1860, 1866, 1888, 1901, 1902, 1929, 1938, 1940, 1987, 1994, 2003, 2004, 2008, 2011, 2023, 2054, 2056, 2065, 2068, 2083, 2086, 2094, 2106, 2113, 2124, 2125, 2136, 2139, 2145, 2168, 2180, 2181, 2187, 2213, 2225, 2227, 2230, 2239, 2273, 2279, 2286, 2288, 2298, 2323, 2324, 2335, 2343, 2359, 2364, 2371, 2385, 2388, 2392, 2398, 2401, 2404, 2410, 2415, 2421, 2430, 2438, 2439, 2440, 2444, 2445, 2448, 2470, 2478, 2489, 2499, 2505, 2515, 2520, 2559, 2574, 2576, 2589, 2598, 2600, 2640, 2643, 2650, 2671, 2679, 2696, 2699, 2706, 2716, 2718, 2731, 2743, 2749, 2750, 2785, 2787, 2789, 2805, 2809, 2825, 2838, 2856, 2888, 2898, 2903, 2923, 2925, 2931, 2933, 2942, 2957, 2961, 2986, 2995, 3034, 3068, 3075, 3077, 3080, 3088, 3089, 3096, 3097, 3101, 3121, 3147, 3160, 3161, 3176, 3182, 3187, 3211, 3225, 3234, 3257, 3263, 3267, 3281, 3290, 3295, 3317, 3319, 3325, 3333, 3359, 3364, 3369, 3374, 3390, 3399, 3400, 3411, 3424, 3473, 3475, 3476, 3479, 3492, 3499, 3504, 3509, 3517, 3519, 3532, 3553, 3578, 3588, 3590, 3593, 3598, 3605, 3606, 3607, 3622, 3634, 3669, 3672, 3678, 3682, 3685, 3694, 3708, 3720, 3729, 3750, 3755, 3784, 3794, 3798, 3801, 3806, 3807, 3813, 3826, 3834, 3872, 3886, 3895, 3903, 3925, 3937, 3946, 3962, 3969, 3998, 4031, 4047, 4053, 4074, 4091, 4093, 4100, 4108, 4113, 4119, 4143, 4148, 4163, 4164, 4183, 4196, 4254, 4260, 4265, 4307, 4308, 4309, 4312, 4318, 4325, 4327, 4328, 4329, 4344, 4347, 4369, 4371, 4377, 4384, 4402, 4404, 4405, 4407, 4412, 4421, 4422, 4437, 4455, 4457, 4471, 4473, 4479, 4483, 4486, 4514, 4532, 4537, 4542, 4543, 4545, 4550, 4554, 4563, 4575, 4576, 4591, 4595, 4631, 4636, 4637, 4639, 4654, 4655, 4704, 4706, 4714, 4718, 4724, 4729, 4735, 4747, 4749, 4750, 4764, 4800, 4814, 4841, 4842, 4844, 4864, 4866, 4869, 4875, 4891, 4908, 4918, 4924, 4928, 4933, 4943, 4946, 4947, 4956, 4968, 4980, 4994, 5009, 5011, 5038, 5056, 5079, 5083, 5101, 5133, 5167, 5173, 5174, 5188, 5189, 5196, 5197, 5213, 5218, 5229, 5241, 5245, 5258, 5279, 5290, 5294, 5297, 5308, 5317, 5331, 5342, 5343, 5353, 5355, 5356, 5358, 5379, 5385, 5386, 5399, 5418, 5433, 5436, 5450, 5456, 5466, 5489, 5504, 5509, 5510, 5530, 5542, 5560, 5583, 5603, 5607, 5618, 5620, 5624, 5626, 5678, 5697, 5712, 5725, 5751, 5757, 5775, 5785, 5799, 5822, 5832, 5836, 5844, 5895, 5902, 5904, 5913, 5918, 5938, 5939, 5961, 5984, 5990, 6025, 6031, 6033, 6060, 6077, 6084, 6090, 6113, 6131, 6134, 6151, 6152, 6176, 6203, 6229, 6230, 6248, 6261, 6277, 6289, 6296, 6304, 6308, 6317, 6319, 6321, 6335, 6338, 6343, 6353, 6356, 6360, 6377, 6382, 6395, 6404, 6425, 6433, 6468, 6474, 6475, 6476, 6477, 6510, 6528, 6540, 6574, 6580, 6582, 6593, 6609, 6627, 6628, 6633, 6635, 6650, 6656, 6657, 6670, 6673, 6676, 6677, 6679, 6686, 6702, 6724, 6740, 6745, 6759, 6801, 6804, 6830, 6833, 6834, 6842, 6859, 6871, 6875, 6878, 6880, 6885, 6896, 6934, 6939, 6948, 6950, 6958, 6970, 6979, 6987, 7010, 7017, 7024, 7040, 7099, 7107, 7112, 7136, 7147, 7149, 7158, 7184, 7225, 7227, 7251, 7274, 7288, 7305, 7306, 7308, 7331, 7344, 7355, 7373, 7407, 7408, 7409, 7410, 7423, 7436, 7438, 7442, 7444, 7450, 7469, 7473, 7479, 7487, 7492, 7506, 7513, 7521, 7528, 7530, 7546, 7567, 7581, 7582, 7597, 7599, 7602, 7632, 7635, 7636, 7642, 7662, 7663, 7701, 7752, 7758, 7765, 7769, 7778, 7780, 7788, 7801, 7816, 7818, 7831, 7836, 7837, 7849, 7893, 7895, 7896, 7922, 7927, 7945, 7948, 7968, 7997, 8003, 8005, 8010, 8030, 8049, 8051, 8052, 8059, 8060, 8085, 8094, 8142, 8143, 8151, 8153, 8161, 8169, 8170, 8196, 8208, 8212, 8215, 8224, 8235, 8241, 8249, 8250, 8276, 8282, 8285, 8286, 8288, 8289, 8299, 8314, 8338, 8343, 8345, 8349, 8373, 8379, 8390, 8397, 8399, 8407, 8427, 8428, 8432, 8439, 8442, 8444, 8484, 8493, 8497, 8500, 8507, 8510, 8534, 8546, 8559, 8569, 8579, 8588, 8594, 8604, 8617, 8620, 8624, 8631, 8638, 8661, 8671, 8674, 8687, 8691, 8696, 8706, 8720, 8729, 8749, 8756, 8758, 8769, 8774, 8780, 8800, 8803, 8809, 8815, 8827, 8830, 8875, 8902, 8906, 8910, 8917, 8919, 8922, 8951, 8954, 8968, 8985, 8986, 8991, 8999, 9014, 9034, 9050, 9057, 9063, 9070, 9088, 9098, 9112, 9113, 9122, 9127, 9140, 9155, 9157, 9169, 9179, 9190, 9216, 9223, 9226, 9264, 9272, 9281, 9297, 9300, 9303, 9322, 9354, 9361, 9374, 9377, 9381, 9385, 9400, 9403, 9419, 9422, 9425, 9428, 9436, 9442, 9449, 9454, 9456, 9458, 9470, 9482, 9496, 9500, 9535, 9539, 9541, 9554, 9556, 9560, 9565, 9574, 9594, 9605, 9610, 9631, 9632, 9660, 9689, 9691, 9696, 9708, 9710, 9712, 9725, 9726, 9734, 9742, 9753, 9770, 9789, 9799, 9801, 9812, 9817, 9836, 9839, 9850, 9852, 9881, 9882, 9891, 9906, 9912, 9920, 9922, 9928, 9976, 9980, 9984, 10002, 10004, 10007, 10029, 10039, 10042, 10050, 10052, 10057, 10067, 10070, 10092, 10093, 10123, 10131, 10138, 10145, 10151, 10164, 10167, 10170, 10179, 10184, 10185, 10190, 10197, 10200, 10211, 10218, 10219, 10222, 10228, 10245, 10247, 10255, 10259, 10276, 10293, 10298, 10299, 10306, 10319, 10327, 10342, 10348, 10377, 10379, 10449, 10465, 10469, 10492, 10504, 10505, 10508, 10521, 10528, 10529, 10530, 10570, 10573, 10577, 10578, 10597, 10611, 10639, 10651, 10666, 10672, 10675, 10690, 10706, 10711, 10738, 10742, 10747, 10749, 10779, 10782, 10783, 10795, 10813, 10849, 10852, 10853, 10884, 10885, 10888, 10892, 10901, 10914, 10918, 10919, 10933, 10935, 10956, 10966, 10968, 10970, 10981, 10986, 10996, 10999, 11005, 11032, 11038, 11051, 11055, 11060, 11067, 11097, 11102, 11155, 11157, 11159, 11162, 11167, 11176, 11187, 11215, 11222, 11228, 11230, 11257, 11263, 11265, 11274, 11286, 11289, 11297, 11300, 11308, 11323, 11337, 11339, 11341, 11347, 11349, 11353, 11371, 11381, 11386, 11426, 11431, 11448, 11449, 11474, 11480, 11500, 11507, 11553, 11562, 11582, 11588, 11594, 11612, 11616, 11618, 11636, 11643, 11650, 11658, 11666, 11679, 11683, 11693, 11695, 11698, 11699, 11706, 11716, 11723, 11730, 11732, 11741, 11745, 11780, 11794, 11798, 11808, 11812, 11828, 11829, 11837, 11840, 11841, 11844, 11849, 11853, 11854, 11855, 11861, 11862, 11872, 11875, 11878, 11886, 11890, 11899, 11905, 11912, 11938, 11963, 11967, 11973, 11980, 11985, 11999, 12026, 12027, 12029, 12039, 12054, 12067, 12115, 12124, 12134, 12136, 12138, 12150, 12187, 12205, 12206, 12224, 12242, 12258, 12269, 12277, 12283, 12295, 12305, 12314, 12322, 12331, 12333, 12334, 12350, 12355, 12356, 12360, 12370, 12372, 12389, 12395, 12396, 12397, 12405, 12410, 12416, 12429, 12439, 12440, 12442, 12446, 12474, 12482, 12486, 12507, 12516, 12521, 12522, 12526, 12527, 12537, 12550, 12563, 12576, 12585, 12586, 12587, 12598, 12606, 12630, 12631, 12635, 12639, 12650, 12657, 12662, 12669, 12687, 12703, 12718, 12724, 12732, 12736, 12745, 12767, 12787, 12803, 12804, 12808, 12818, 12830, 12843, 12855, 12856, 12870, 12884, 12903, 12914, 12920, 12923, 12924, 12927, 12936, 12972, 12977, 12998, 13003, 13004, 13009, 13044, 13068, 13069, 13074, 13080, 13095, 13098, 13124, 13130, 13153, 13154, 13176, 13219, 13222, 13227, 13237, 13267, 13274, 13293, 13301, 13304, 13307, 13330, 13333, 13354, 13365, 13373, 13379, 13381, 13389, 13397, 13422, 13430, 13433, 13445, 13466, 13473, 13476, 13482, 13490, 13493, 13528, 13529, 13535, 13543, 13558, 13559, 13586, 13637, 13643, 13646, 13665, 13674, 13678, 13679, 13704, 13707, 13710, 13718, 13727, 13733, 13747, 13762, 13772, 13785, 13796, 13803, 13812, 13814, 13817, 13827, 13844, 13846, 13847, 13848, 13854, 13855, 13861, 13864, 13873, 13899, 13907, 13936, 13939, 13946, 13949, 13954, 13964, 13965, 13974, 13975, 13978, 13980, 14002, 14006, 14007, 14030, 14043, 14044, 14057, 14058, 14060, 14073, 14091, 14109, 14110, 14112, 14117, 14131, 14132, 14159, 14172, 14173, 14176, 14184, 14194, 14197, 14200, 14204, 14207, 14213, 14230, 14257, 14273, 14275, 14287, 14299, 14301, 14307, 14308, 14347, 14362, 14365, 14381, 14386, 14400, 14416, 14428, 14436, 14446, 14457, 14458, 14462, 14487, 14499, 14509, 14532, 14548, 14551, 14571, 14596, 14598, 14611, 14621, 14639, 14640, 14648, 14662, 14669, 14671, 14676, 14680, 14687, 14689, 14691, 14718, 14731, 14747, 14779, 14787, 14825, 14839, 14844, 14847, 14855, 14870, 14880, 14891, 14908, 14946, 14959, 14960, 14967, 14971, 14986, 15007, 15010, 15016, 15044, 15049, 15055, 15056, 15112, 15131, 15158, 15165, 15174, 15177, 15196, 15203, 15206, 15207, 15213, 15217, 15233, 15236, 15253, 15259, 15286, 15296, 15307, 15308, 15321, 15323, 15327, 15339, 15363, 15372, 15374, 15379, 15380, 15383, 15385, 15392, 15410, 15449, 15452, 15462, 15464, 15465, 15477, 15483, 15503, 15505, 15528, 15532, 15548, 15562, 15566, 15571, 15605, 15610, 15622, 15651, 15660, 15666, 15674, 15709, 15721, 15731, 15735, 15736, 15741, 15767, 15774, 15785, 15792, 15793, 15811, 15851, 15879, 15883, 15884, 15912, 15926, 15927, 15931, 15940, 15959, 15965, 15985, 15998, 16020, 16027, 16031, 16032, 16044, 16070, 16100, 16117, 16129, 16136, 16137, 16140, 16149, 16152, 16175, 16186, 16201, 16241, 16261, 16269, 16272, 16279, 16290, 16308, 16319, 16324, 16325, 16327, 16338, 16341, 16345, 16352, 16354, 16371, 16372, 16385, 16391, 16416, 16423, 16448, 16455, 16468, 16470, 16477, 16481, 16484, 16491, 16493, 16500, 16509, 16510, 16516, 16541, 16559, 16577, 16589, 16632, 16638, 16642, 16643, 16649, 16650, 16654, 16665, 16677, 16684, 16704, 16720, 16726, 16734, 16765, 16774, 16775, 16777, 16780, 16797, 16798, 16799, 16803, 16807, 16809, 16830, 16832, 16861, 16863, 16868, 16876, 16889, 16890, 16902, 16915, 16927, 16939, 16949, 16962, 16968, 17002, 17010, 17013, 17021, 17023, 17029, 17062, 17066, 17084, 17093, 17094, 17102, 17152, 17160, 17167, 17173, 17183, 17221, 17239, 17250, 17267, 17277, 17278, 17282, 17296, 17307, 17315, 17333, 17336, 17344, 17346, 17348, 17354, 17365, 17374, 17385, 17387, 17397, 17401, 17404, 17412, 17417, 17432, 17436, 17444, 17452, 17467, 17469, 17501, 17502, 17513, 17530, 17531, 17547, 17550, 17555, 17573, 17577, 17578, 17586, 17600, 17602, 17611, 17627, 17640, 17645, 17648, 17649, 17652, 17667, 17676, 17682, 17721, 17724, 17729, 17731, 17751, 17754, 17762, 17770, 17772, 17781, 17802, 17820, 17823, 17833, 17848, 17857, 17858, 17865, 17867, 17887, 17893, 17909, 17924, 17926, 17945, 17971, 17977, 17986, 17990, 18003, 18011, 18042, 18046, 18052, 18077, 18086, 18091, 18099, 18103, 18123, 18128, 18131, 18142, 18153, 18165, 18167, 18173, 18181, 18206, 18226, 18252, 18263, 18276, 18277, 18284, 18314, 18319, 18327, 18330, 18342, 18352, 18355, 18362, 18364, 18375, 18383, 18398, 18403, 18432, 18447, 18453, 18465, 18467, 18485, 18486, 18492, 18505, 18508, 18510, 18515, 18536, 18540, 18548, 18550, 18557, 18563, 18570, 18574, 18576, 18616, 18622, 18627, 18639, 18649, 18655, 18673, 18681, 18712, 18745, 18746, 18758, 18773, 18778, 18782, 18826, 18832, 18834, 18842, 18851, 18853, 18858, 18862, 18867, 18872, 18877, 18908, 18910, 18912, 18913, 18919, 18925, 18929, 18930, 18933, 18936, 18944, 18974, 18975, 18997, 19009, 19011, 19014, 19016, 19028, 19057, 19098, 19106, 19114, 19130, 19133, 19147, 19157, 19162, 19166, 19173, 19179, 19198, 19213, 19225, 19230, 19239, 19241, 19266, 19274, 19279, 19283, 19285, 19316, 19327, 19335, 19352, 19358, 19362, 19366, 19377, 19379, 19405, 19423, 19428, 19431, 19437, 19444, 19456, 19469, 19475, 19476, 19481, 19486, 19492, 19506, 19507, 19536, 19537, 19562, 19586, 19589, 19622, 19634, 19651, 19663, 19679, 19685, 19690, 19697, 19707, 19712, 19714, 19715, 19717, 19724, 19734, 19744, 19754, 19769, 19779, 19792, 19807, 19830, 19850, 19863, 19875, 19903, 19904, 19908, 19963, 19985, 19988, 20003, 20012, 20014, 20018, 20040, 20042, 20054, 20078, 20079, 20083, 20087, 20089, 20114, 20121, 20122, 20138, 20155, 20162, 20186, 20199, 20216, 20222, 20234, 20238, 20254, 20285, 20299, 20302, 20305, 20337, 20345, 20364, 20365, 20374, 20382, 20394, 20403, 20440, 20450, 20451, 20476, 20506, 20508, 20516, 20527, 20536, 20540, 20541, 20542, 20557, 20566, 20584, 20608, 20611, 20613, 20618, 20627, 20644, 20656, 20663, 20674, 20682, 20690, 20699, 20707, 20711, 20737, 20751, 20765, 20771, 20774, 20778, 20779, 20789, 20795, 20798, 20834, 20851, 20865, 20867, 20869, 20872, 20883, 20892, 20894, 20895, 20896, 20901, 20919, 20924, 20933, 20937, 20938, 20946, 20947, 20955, 20957, 20958, 20965, 20966, 20970, 20974, 20991, 21001, 21013, 21018, 21020, 21025, 21114, 21142, 21205, 21210, 21227, 21230, 21236, 21253, 21262, 21294, 21300, 21316, 21331, 21339, 21349, 21356, 21365, 21369, 21376, 21399, 21410, 21413, 21418, 21420, 21435, 21447, 21452, 21455, 21468, 21470, 21474, 21486, 21495, 21501, 21517, 21530, 21539, 21546, 21547, 21553, 21556, 21564, 21569, 21579, 21593, 21595, 21599, 21603, 21632, 21643, 21690, 21713, 21726, 21732, 21737, 21738, 21744, 21755, 21773, 21782, 21800, 21807, 21829, 21839, 21848, 21876, 21893, 21923, 21926, 21931, 21958, 21967, 21970, 21973, 21982, 22002, 22029, 22031, 22035, 22044, 22051, 22068, 22073, 22099, 22117, 22121, 22126, 22145, 22146, 22149, 22161, 22166, 22170, 22177, 22205, 22210, 22231, 22233, 22234, 22244, 22246, 22251, 22272, 22280, 22299, 22317, 22321, 22327, 22350, 22355, 22363, 22365, 22378, 22383, 22387, 22391, 22393, 22395, 22403, 22406, 22418, 22423, 22428, 22437, 22444, 22451, 22453, 22460, 22480, 22491, 22510, 22530, 22536, 22554, 22557, 22561, 22562, 22564, 22571, 22588, 22593, 22606, 22607, 22610, 22611, 22642, 22666, 22717, 22729, 22738, 22744, 22745, 22763, 22768, 22769, 22774, 22782, 22789, 22796, 22800, 22836, 22841, 22843, 22858, 22864, 22886, 22889, 22897, 22925, 22928, 22938, 22944, 22945, 22948, 22950, 22956, 22983, 22986, 22988, 22993, 22994, 22997, 23028, 23030, 23039, 23047, 23081, 23091, 23097, 23105, 23108, 23111, 23113, 23133, 23145, 23150, 23161, 23172, 23176, 23199, 23200, 23202, 23204, 23205, 23211, 23212, 23226, 23227, 23231, 23236, 23239, 23263, 23265, 23266, 23269, 23290, 23311, 23315, 23320, 23323, 23340, 23352, 23361, 23372, 23375, 23376, 23381, 23386, 23390, 23401, 23415, 23418, 23428, 23429, 23453, 23467, 23479, 23505, 23508, 23527, 23528, 23535, 23537, 23541, 23553, 23555, 23557, 23559, 23563, 23566, 23581, 23585, 23596, 23598, 23604, 23605, 23617, 23630, 23643, 23658, 23669, 23673, 23680, 23683, 23690, 23691, 23701, 23707, 23714, 23720, 23725, 23733, 23746, 23750, 23751, 23769, 23774, 23784, 23809, 23818, 23831, 23839, 23842, 23857, 23924, 23930, 23944, 23945, 23947, 23959, 23971, 23975, 23987, 23989, 24002, 24003, 24009, 24019, 24026, 24039, 24041, 24046, 24062, 24071, 24077, 24079, 24087, 24096, 24119, 24121, 24137, 24150, 24169, 24181, 24194, 24195, 24199, 24200, 24203, 24210, 24223, 24258, 24272, 24274, 24291, 24293, 24300, 24305, 24318, 24339, 24378, 24380, 24383, 24392, 24402, 24442, 24468, 24476, 24480, 24488, 24490, 24495, 24502, 24504, 24505, 24516, 24562, 24572, 24594, 24595, 24597, 24600, 24606, 24609, 24622, 24623, 24629, 24631, 24632, 24633, 24657, 24662, 24664, 24666, 24682, 24697, 24709, 24753, 24758, 24766, 24833, 24840, 24851, 24858, 24869, 24886, 24909, 24915, 24916, 24929, 24945, 24951, 24966, 24972, 24980, 24996, 25012, 25017, 25043, 25059, 25067, 25104, 25121, 25125, 25127, 25133, 25150, 25191, 25210, 25224, 25228, 25240, 25243, 25255, 25264, 25267, 25272, 25282, 25284, 25290, 25292, 25293, 25334, 25338, 25350, 25365, 25368, 25380, 25389, 25391, 25400, 25413, 25422, 25440, 25444, 25446, 25456, 25460, 25472, 25486, 25495, 25499, 25509, 25512, 25528, 25530, 25532, 25547, 25559, 25591, 25593, 25597, 25601, 25614, 25616, 25638, 25672, 25691, 25698, 25712, 25713, 25721, 25722, 25725, 25732, 25753, 25760, 25775, 25794, 25813, 25819, 25828, 25833, 25901, 25908, 25947, 25983, 25987, 25989, 25996, 26008, 26017, 26025, 26030, 26039, 26047, 26054, 26058, 26077, 26088, 26092, 26112, 26183, 26185, 26190, 26201, 26204, 26237, 26239, 26241, 26248, 26251, 26252, 26256, 26263, 26268, 26282, 26314, 26331, 26346, 26358, 26360, 26364, 26377, 26428, 26434, 26460, 26481, 26485, 26497, 26532, 26533, 26538, 26561, 26567, 26585, 26588, 26595, 26596, 26618, 26625, 26633, 26643, 26651, 26677, 26687, 26715, 26721, 26732, 26733, 26746, 26760, 26762, 26769, 26779, 26781, 26822, 26828, 26830, 26870, 26882, 26883, 26889, 26892, 26894, 26914, 26927, 26932, 26944, 26953, 26957, 26963, 26967, 26973, 26976, 26978, 26985, 27030, 27038, 27044, 27048, 27052, 27067, 27111, 27115, 27121, 27142, 27144, 27153, 27167, 27205, 27209, 27217, 27218, 27229, 27233, 27240, 27250, 27280, 27302, 27332, 27339, 27349, 27362, 27376, 27386, 27396, 27401, 27413, 27426, 27429, 27456, 27470, 27474, 27476, 27477, 27480]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27481, 5)\n",
      "(24857, 5)\n",
      "(24856, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df_span.shape)\n",
    "df_span = df_span[(~df_span.index.isin(anomalous_idxs))]\n",
    "print(df_span.shape)\n",
    "df_span = df_span[(~df_span.text.isna())]\n",
    "df_span = df_span.reset_index(drop=True)\n",
    "df_span = df_span.copy()\n",
    "print(df_span.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"sentiment_code\"] = df_span[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments = df_span[\"sentiment_code\"].cat.codes.values\n",
    "\n",
    "test_df_span[\"sentiment_code\"] = test_df_span[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments_test = test_df_span[\"sentiment_code\"].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text\"] = df_span[\"text\"].astype(str)\n",
    "df_span[\"selected_text\"] = df_span[\"selected_text\"].astype(str)\n",
    "test_df_span[\"text\"] = test_df_span[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text_mod\"] = \"<s> \" + df_span.text.str.strip() + \" </s> \" + df_span.sentiment + \" </s>\"\n",
    "test_df_span[\"text_mod\"] = \"<s> \" + test_df_span.text.str.strip() + \" </s> \" + test_df_span.sentiment + \" </s>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclusions for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCLUDE_NEUTRAL_CLASS: (14017, 7)\n"
     ]
    }
   ],
   "source": [
    "if EXCLUDE_NEUTRAL_CLASS:\n",
    "    df_span = df_span.loc[df_span.sentiment!=\"neutral\"].copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"EXCLUDE_NEUTRAL_CLASS:\", df_span.shape)\n",
    "\n",
    "if RUN_ON_SAMPLE:\n",
    "    df_span = df_span.sample(1000).copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"Train RUN_ON_SAMPLE\", df_span.shape)\n",
    "    test_df_span = test_df_span.sample(1000).copy()\n",
    "    test_df_span = test_df_span.reset_index(drop=True)\n",
    "    print(\"Test  RUN_ON_SAMPLE\", test_df_span.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_tokens = tokenizer.encode_batch(df_span.text_mod.tolist())\n",
    "Y_span_tokens = tokenizer.encode_batch(df_span.selected_text.tolist())\n",
    "X_span_tokens_test = tokenizer.encode_batch(test_df_span.text_mod.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = [i.ids for i in X_span_tokens]\n",
    "Y_span = [i.ids for i in Y_span_tokens]\n",
    "X_span_test = [i.ids for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_att = [i.attention_mask for i in X_span_tokens]\n",
    "Y_span_att = [i.attention_mask for i in Y_span_tokens]\n",
    "X_span_att_test = [i.attention_mask for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN_SPAN = max([len(i) for i in X_span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremities(l_string, s_string, print_it=False):\n",
    "    len_l = len(l_string)\n",
    "    len_s = len(s_string)\n",
    "    \n",
    "    for i in range(len_l - len_s + 1):\n",
    "        if (i + len_s) <= len_l:\n",
    "            substring = l_string[i:i+len_s]\n",
    "            if substring == s_string:\n",
    "                if print_it:\n",
    "                    print(l_string)\n",
    "                    print(substring)\n",
    "                    print(i, i+len_s, substring)\n",
    "                \n",
    "                start_vector, end_vector = [0] * len_l, [0] * len_l\n",
    "                start_vector[i], end_vector[i+len_s-1] = 1, 1\n",
    "                \n",
    "                return (start_vector, end_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14017 \t: #Processed\n",
      "0 \t: # of Anomalies\n"
     ]
    }
   ],
   "source": [
    "Y_span_starts, Y_span_stops = [], []\n",
    "anomaly_idx, counter = [], 0\n",
    "for num, (i,j) in enumerate(zip(X_span_tokens, Y_span_tokens)):\n",
    "    x,y = i.ids, j.ids\n",
    "    try:\n",
    "        s,e = get_extremities(x, y)\n",
    "    except TypeError as t:\n",
    "        counter += 1\n",
    "        anomaly_idx.append(num)\n",
    "    Y_span_starts.append(s)\n",
    "    Y_span_stops.append(e)\n",
    "print(num + 1, \"\\t: #Processed\")\n",
    "\n",
    "print(counter,\"\\t: # of Anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if counter:\n",
    "    print(anomaly_idx, sep=\",\")\n",
    "    print(df_span[\"original_index\"].iloc[anomaly_idx].tolist())\n",
    "    df_span.iloc[anomaly_idx].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcfly gig last nightt omg it was amazin didnt sit down through the whole thing  mcfly did you see me and ma best mate we were in tutus\n",
      "it was amazin\n",
      "[['<s>', 0, 0, 0], ['Ġmc', 44355, 0, 0], ['fly', 19252, 0, 0], ['Ġgig', 10196, 0, 0], ['Ġlast', 94, 0, 0], ['Ġnight', 363, 0, 0], ['t', 90, 0, 0], ['Ġo', 1021, 0, 0], ['mg', 22984, 0, 0], ['Ġit', 24, 1, 0], ['Ġwas', 21, 0, 0], ['Ġamaz', 42402, 0, 0], ['in', 179, 0, 1], ['Ġdidnt', 46405, 0, 0], ['Ġsit', 2662, 0, 0], ['Ġdown', 159, 0, 0], ['Ġthrough', 149, 0, 0], ['Ġthe', 5, 0, 0], ['Ġwhole', 1086, 0, 0], ['Ġthing', 631, 0, 0], ['Ġ', 1437, 0, 0], ['Ġmc', 44355, 0, 0], ['fly', 19252, 0, 0], ['Ġdid', 222, 0, 0], ['Ġyou', 47, 0, 0], ['Ġsee', 192, 0, 0], ['Ġme', 162, 0, 0], ['Ġand', 8, 0, 0], ['Ġma', 9131, 0, 0], ['Ġbest', 275, 0, 0], ['Ġmate', 12563, 0, 0], ['Ġwe', 52, 0, 0], ['Ġwere', 58, 0, 0], ['Ġin', 11, 0, 0], ['Ġtut', 15511, 0, 0], ['us', 687, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0], ['Ġpositive', 1313, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0]]\n",
      "[[24, 'Ġit'], [21, 'Ġwas'], [42402, 'Ġamaz'], [179, 'in']]\n"
     ]
    }
   ],
   "source": [
    "check_idx = 156\n",
    "print(df_span.text[check_idx])\n",
    "print(df_span.selected_text[check_idx])\n",
    "print([[i,j,k,l] for i,j,k,l in zip(X_span_tokens[check_idx].tokens,\n",
    "                                    X_span_tokens[check_idx].ids,\n",
    "                                    Y_span_starts[check_idx],\n",
    "                                    Y_span_stops[check_idx])])\n",
    "print([[i,j] for i,j in zip(Y_span_tokens[check_idx].ids,\n",
    "                            Y_span_tokens[check_idx].tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "\n",
    "Y_span_starts = pad_sequences(Y_span_starts, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\").argmax(axis=1)\n",
    "Y_span_stops = pad_sequences(Y_span_stops, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\").argmax(axis=1)\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAX_SEQ_LEN_SPAN': 76,\n",
      " 'VOCAB_SIZE': 50265,\n",
      " 'X_span': (14017, 76),\n",
      " 'X_span_att': (14017, 76),\n",
      " 'X_span_att_test': (3534, 76),\n",
      " 'X_span_test': (3534, 76),\n",
      " 'Y_span': (14017, 76),\n",
      " 'Y_span_starts': (14017,),\n",
      " 'Y_span_stops': (14017,)}\n"
     ]
    }
   ],
   "source": [
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \"Y_span_starts\" : Y_span_starts.shape,\n",
    "    \"Y_span_stops\" : Y_span_stops.shape,\n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "    \"VOCAB_SIZE\":VOCAB_SIZE,\n",
    "    \"MAX_SEQ_LEN_SPAN\":MAX_SEQ_LEN_SPAN\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_flag = np.isin(Y_span_stops,\n",
    "                    np.unique(Y_span_stops,\n",
    "                              return_counts=True)[0][np.unique(Y_span_stops,\n",
    "                                                               return_counts=True)[1]>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (14017, 76) \t: X  \n",
      " (14017, 76) \t: X_att  \n",
      " (14017, 76) \t: Y  \n",
      " (14017,) \t: Y_starts  \n",
      " (14017,) \t: Y_stops  \n",
      " (3534, 76) \t: X_test  \n",
      " (3534, 76) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = X_span[keep_flag]\n",
    "X_span_att = X_span_att[keep_flag]\n",
    "Y_span = Y_span[keep_flag]\n",
    "Y_span_starts = Y_span_starts[keep_flag]\n",
    "Y_span_stops = Y_span_stops[keep_flag]\n",
    "X_span_test = X_span_test\n",
    "X_span_att_test = X_span_att_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (14015, 76) \t: X  \n",
      " (14015, 76) \t: X_att  \n",
      " (14015, 76) \t: Y  \n",
      " (14015,) \t: Y_starts  \n",
      " (14015,) \t: Y_stops  \n",
      " (3534, 76) \t: X_test  \n",
      " (3534, 76) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sss = StratifiedKFold(n_splits=NUM_FOLDS,shuffle=True,random_state=seeded_value)\n",
    "sss = KFold(n_splits=NUM_FOLDS,shuffle=True,random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_span_words = [tokenizer.decode(i) for i in Y_span]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = max(MAX_SEQ_LEN_SENT, MAX_SEQ_LEN_SPAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X_sent': (31015, 100),\n",
      " 'X_sent_att': (31015, 100),\n",
      " 'X_span': (14015, 100),\n",
      " 'X_span_att': (14015, 100),\n",
      " 'X_span_att_test': (3534, 100),\n",
      " 'X_span_test': (3534, 100),\n",
      " 'Y_span': (14015, 100)}\n"
     ]
    }
   ],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "X_sent = pad_sequences(X_sent, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_sent_att = pad_sequences(X_sent_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "    \n",
    "    \"X_sent\" : X_sent.shape,\n",
    "    \"X_sent_att\" : X_sent_att.shape,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_sequences = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"words\")\n",
    "    input_att_flags = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"att_flags\")\n",
    "    input_token_ids = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"token_ids\")\n",
    "    \n",
    "    config = RobertaConfig.from_pretrained('roberta-base')\n",
    "    roberta_model = TFRobertaModel.from_pretrained('roberta-base', config=config)\n",
    "    x = roberta_model(inputs=input_sequences, attention_mask=input_att_flags, token_type_ids=input_token_ids)\n",
    "    \n",
    "    x_conv = Reshape((MAX_SEQ_LEN, 768, 1))(x[0])\n",
    "    x_conv = Conv2D(filters=2, kernel_size=(1,768), data_format=\"channels_last\")(x_conv)\n",
    "    x_conv = BatchNormalization()(x_conv)\n",
    "    x_conv = Dropout(DROPOUT)(x_conv)\n",
    "    x_conv = Flatten()(x_conv)\n",
    "    \n",
    "    x_lstm = Bidirectional(LSTM(2, activation='relu', return_sequences=False))(x[0])\n",
    "    x_lstm = BatchNormalization()(x_lstm)\n",
    "    x_lstm = Dropout(DROPOUT)(x_lstm)\n",
    "    x_lstm = Flatten()(x_lstm)\n",
    "    \n",
    "    x_dense = TimeDistributed(Dense(2, activation='relu'))(x[0])\n",
    "    x_dense = BatchNormalization()(x_dense)\n",
    "    x_dense = Dropout(DROPOUT)(x_dense)\n",
    "    x_dense = Flatten()(x_dense)\n",
    "    \n",
    "    x_flat = concatenate([x_conv, x_lstm, x_dense])\n",
    "    \n",
    "    output = Dense(3, activation='softmax', name=\"output_sentiments\")(x_flat)\n",
    "    \n",
    "    sentiment_model = Model([input_att_flags, input_sequences, input_token_ids], [output])\n",
    "    \n",
    "    output_starts_0 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"starts_0\")(x_flat)\n",
    "    output_stops_0 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"stops_0\")(x_flat)\n",
    "    \n",
    "    output_subtract = tf.keras.layers.Subtract()([output_starts_0, output_stops_0])\n",
    "    output_flat = concatenate([output_starts_0, output_stops_0, output_subtract])\n",
    "    output_starts_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"starts_1\")(output_flat)\n",
    "    output_stops_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"stops_1\")(output_flat)\n",
    "    \n",
    "    span_detection_model = Model([input_att_flags, input_sequences, input_token_ids],\n",
    "                                 [output_starts_0, output_stops_0, output_starts_1, output_stops_1])\n",
    "    \n",
    "    return sentiment_model, span_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model, span_detection_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 100, 768, 1)  0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 100, 1, 2)    1538        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 4)            12336       tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 100, 2)       1538        tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 100, 1, 2)    8           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4)            16          bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 100, 2)       8           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 100, 1, 2)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 4)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 100, 2)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 200)          0           dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4)            0           dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 200)          0           dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 404)          0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_sentiments (Dense)       (None, 3)            1215        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 124,662,291\n",
      "Trainable params: 124,662,275\n",
      "Non-trainable params: 16\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 100, 768, 1)  0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 100, 1, 2)    1538        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 4)            12336       tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 100, 2)       1538        tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 100, 1, 2)    8           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4)            16          bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 100, 2)       8           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 100, 1, 2)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 4)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 100, 2)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 200)          0           dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4)            0           dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 200)          0           dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 404)          0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "starts_0 (Dense)                (None, 100)          40500       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "stops_0 (Dense)                 (None, 100)          40500       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 100)          0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300)          0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "                                                                 subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "starts_1 (Dense)                (None, 100)          30100       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "stops_1 (Dense)                 (None, 100)          30100       concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 124,802,276\n",
      "Trainable params: 124,802,260\n",
      "Non-trainable params: 16\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "span_detection_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mcp = ModelCheckpoint(filepath=RESULTS_DIR+\"Sentiment_\"+MODEL_PREFIX+\"BestCheckpoint.h5\", monitor='val_loss',\n",
    "                                verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "sentiment_csvl = CSVLogger(filename=RESULTS_DIR+\"Sentiment_\"+MODEL_PREFIX+\"_LossLogs.csv\",\n",
    "                           separator=\",\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_index, va_index, _, _ = train_test_split(np.arange(X_sent.shape[0]),\n",
    "                                            np.arange(Y_sent.shape[0]),\n",
    "                                            test_size=TRAIN_SPLIT_RATIO,\n",
    "                                            random_state=seeded_value,\n",
    "                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,c = np.unique(Y_sent, return_counts=True)\n",
    "cw = class_weight.compute_class_weight('balanced', np.unique(Y_sent), Y_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model.layers[3].trainable = False\n",
    "adam = Adam(learning_rate=SENTIMENT_MAX_LR)\n",
    "sentiment_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                        optimizer=adam,\n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 100, 768, 1)  0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 100, 1, 2)    1538        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 4)            12336       tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 100, 2)       1538        tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 100, 1, 2)    8           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4)            16          bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 100, 2)       8           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 100, 1, 2)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 4)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 100, 2)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 200)          0           dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4)            0           dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 200)          0           dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 404)          0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_sentiments (Dense)       (None, 3)            1215        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 124,662,291\n",
      "Trainable params: 16,643\n",
      "Non-trainable params: 124,645,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24812 samples, validate on 6203 samples\n",
      "Epoch 1/3\n",
      "24812/24812 [==============================] - 543s 22ms/sample - loss: 0.9078 - accuracy: 0.5825 - val_loss: 0.8033 - val_accuracy: 0.6247\n",
      "Epoch 2/3\n",
      "24812/24812 [==============================] - 517s 21ms/sample - loss: 0.8186 - accuracy: 0.6346 - val_loss: 0.8019 - val_accuracy: 0.6308\n",
      "Epoch 3/3\n",
      "24812/24812 [==============================] - 518s 21ms/sample - loss: 0.8082 - accuracy: 0.6421 - val_loss: 0.7341 - val_accuracy: 0.6782\n"
     ]
    }
   ],
   "source": [
    "sent_history = sentiment_model.fit(x={\"att_flags\":X_sent_att[tr_index],\n",
    "                                      \"words\":X_sent[tr_index],\n",
    "                                      \"token_ids\":np.zeros_like(X_sent_att[tr_index])},\n",
    "                                   y={\"output_sentiments\":Y_sent[tr_index]},\n",
    "                                   shuffle=True,\n",
    "                                   batch_size=BATCH_SIZE,\n",
    "                                   epochs=SENTIMENT_NUM_EPOCHS[0],\n",
    "                                   validation_data=({\"att_flags\":X_sent_att[va_index],\n",
    "                                                     \"words\":X_sent[va_index],\n",
    "                                                     \"token_ids\":np.zeros_like(X_sent_att[va_index])},\n",
    "                                                    {\"output_sentiments\":Y_sent[va_index]}),\n",
    "                                   verbose=1,\n",
    "                                   class_weight=cw,\n",
    "                                   callbacks=[sentiment_mcp, sentiment_csvl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model.layers[3].trainable = True\n",
    "adam = Adam(learning_rate=SENTIMENT_MIN_LR)\n",
    "sentiment_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                        optimizer=adam,\n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24812 samples, validate on 6203 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "24812/24812 [==============================] - 911s 37ms/sample - loss: 0.7155 - accuracy: 0.7006 - val_loss: 0.6126 - val_accuracy: 0.7567\n",
      "Epoch 2/3\n",
      "24812/24812 [==============================] - 888s 36ms/sample - loss: 0.5742 - accuracy: 0.7722 - val_loss: 0.5760 - val_accuracy: 0.7846\n",
      "Epoch 3/3\n",
      "24812/24812 [==============================] - 891s 36ms/sample - loss: 0.4885 - accuracy: 0.8099 - val_loss: 0.6420 - val_accuracy: 0.7529\n"
     ]
    }
   ],
   "source": [
    "sent_history_finetuned = sentiment_model.fit(x={\"att_flags\":X_sent_att[tr_index],\n",
    "                                                \"words\":X_sent[tr_index],\n",
    "                                                \"token_ids\":np.zeros_like(X_sent_att[tr_index])},\n",
    "                                             y={\"output_sentiments\":Y_sent[tr_index]},\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             epochs=SENTIMENT_NUM_EPOCHS[1],\n",
    "                                             validation_data=({\"att_flags\":X_sent_att[va_index],\n",
    "                                                               \"words\":X_sent[va_index],\n",
    "                                                               \"token_ids\":np.zeros_like(X_sent_att[va_index])},\n",
    "                                                              {\"output_sentiments\":Y_sent[va_index]}),\n",
    "                                             verbose=1,\n",
    "                                             class_weight=cw,\n",
    "                                             callbacks=[sentiment_mcp, sentiment_csvl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'predicted_sentiment': 'positive',\n",
      "     'text': 'has had a nice long nap  http://plurk.com/p/stvqp'},\n",
      " 1: {'predicted_sentiment': 'negative',\n",
      "     'text': 'Sicky sicky  sucks on such a lovely day'},\n",
      " 2: {'predicted_sentiment': 'neutral',\n",
      "     'text': 'wow i`m tired... going to bed, GOOD NIGHT'},\n",
      " 3: {'predicted_sentiment': 'negative',\n",
      "     'text': '_Obsession lol but its only gonna be 80  the water will prob be '\n",
      "             'cold as heck...'},\n",
      " 4: {'predicted_sentiment': 'neutral', 'text': ' i havent got a call'}}\n"
     ]
    }
   ],
   "source": [
    "sample_text = data.text.sample(5).tolist()\n",
    "\n",
    "encoded_repr = tokenizer.encode_batch(sample_text)\n",
    "\n",
    "sample_text_ids = pad_sequences([i.ids for i in encoded_repr],\n",
    "                                maxlen=MAX_SEQ_LEN,\n",
    "                                padding=\"post\")\n",
    "sample_text_att = pad_sequences([i.attention_mask for i in encoded_repr],\n",
    "                                maxlen=MAX_SEQ_LEN,\n",
    "                                padding=\"post\")\n",
    "pred = sentiment_model.predict({\"att_flags\":sample_text_att,\n",
    "                                \"words\":sample_text_ids,\n",
    "                                \"token_ids\":np.zeros_like(sample_text_att)})\n",
    "\n",
    "pprint({\n",
    "    num:{\n",
    "        \"text\":i,\n",
    "        \"predicted_sentiment\":[k for k,v in sentiment_lookup.items() if v==j][0]\n",
    "    } for num,(i,j) in enumerate(zip(sample_text, pred.argmax(axis=1)))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_sentiment(x):\n",
    "    encoded_repr = tokenizer.encode_batch(x.tolist())\n",
    "\n",
    "    sample_text_ids = pad_sequences([i.ids for i in encoded_repr],\n",
    "                                    maxlen=MAX_SEQ_LEN,\n",
    "                                    padding=\"post\")\n",
    "    sample_text_att = pad_sequences([i.attention_mask for i in encoded_repr],\n",
    "                                    maxlen=MAX_SEQ_LEN,\n",
    "                                    padding=\"post\")\n",
    "    pred = sentiment_model.predict({\"att_flags\":sample_text_att,\n",
    "                                    \"words\":sample_text_ids,\n",
    "                                    \"token_ids\":np.zeros_like(sample_text_att)},\n",
    "                                  batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    res = pd.DataFrame({\"predicted_sentiment\":pred.argmax(axis=1)})\n",
    "    \n",
    "    return res.predicted_sentiment.apply(lambda x:[k for k,v in sentiment_lookup.items() if v==x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data[\"predicted_sentiment\"] = infer_sentiment(x=data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      0.89      0.76      8782\n",
      "     neutral       0.79      0.68      0.73     12548\n",
      "    positive       0.87      0.75      0.80      9685\n",
      "\n",
      "    accuracy                           0.76     31015\n",
      "   macro avg       0.77      0.77      0.77     31015\n",
      "weighted avg       0.78      0.76      0.76     31015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=data.sentiment, y_pred=data.predicted_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7265, 1610,  810],\n",
       "       [ 761, 8567, 3220],\n",
       "       [ 343,  631, 7808]], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=data.sentiment,\n",
    "                 y_pred=data.predicted_sentiment,\n",
    "                 labels=['positive', 'neutral', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2342415 , 0.05191037, 0.0261164 ],\n",
       "       [0.02453651, 0.27622118, 0.10382073],\n",
       "       [0.01105916, 0.02034499, 0.25174915]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=data.sentiment,\n",
    "                 y_pred=data.predicted_sentiment,\n",
    "                 labels=['positive', 'neutral', 'negative'],\n",
    "                 normalize=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set2\n",
       "train    0.776007\n",
       "valid    0.707364\n",
       "dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"set2\"] = np.where(data.index.isin(tr_index), \"train\", \"valid\")\n",
    "data.groupby(\"set2\").apply(lambda x : accuracy_score(y_true=x.sentiment, y_pred=x.predicted_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set\n",
       "test     0.350028\n",
       "train    0.815218\n",
       "dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(\"set\").apply(lambda x : accuracy_score(y_true=x.sentiment, y_pred=x.predicted_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set    set2 \n",
       "test   train     2779\n",
       "       valid      755\n",
       "train  train    22003\n",
       "       valid     5478\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"set\", \"set2\"])[\"sentiment\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set    set2 \n",
       "test   train    0.347607\n",
       "       valid    0.358940\n",
       "train  train    0.830114\n",
       "       valid    0.755385\n",
       "dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"set\", \"set2\"]).apply(lambda x : accuracy_score(y_true=x.sentiment, y_pred=x.predicted_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 2, 'neutral': 1, 'negative': 0}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>set</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>oh Marly, I`m so sorry!!  I hope you find her soon!! &lt;3 &lt;3</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>gotta restart my computer .. I thought Win7 was supposed to put an end to the constant rebootiness</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cALLed LoSe f0LloWeRs FridAy... smH</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Car not happy, big big dent in boot! Hoping theyre not going to write it off, crossing fingers and waiting</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3487</th>\n",
       "      <td>Bored, making a mothers day card</td>\n",
       "      <td>test</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3489</th>\n",
       "      <td>I think Max (my cat) may really be gone</td>\n",
       "      <td>test</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3490</th>\n",
       "      <td>`m working on a logo on photoshop &amp; it didn`t work out..  hehe.. try again lah..</td>\n",
       "      <td>test</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3515</th>\n",
       "      <td>im riding the highs and lows of moods now.... chores blow</td>\n",
       "      <td>test</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3520</th>\n",
       "      <td>Where`d the songs go on the site, I want 'Do You' on this computer too</td>\n",
       "      <td>test</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3220 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                             text  \\\n",
       "5                    http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth   \n",
       "20                                                     oh Marly, I`m so sorry!!  I hope you find her soon!! <3 <3   \n",
       "23             gotta restart my computer .. I thought Win7 was supposed to put an end to the constant rebootiness   \n",
       "24                                SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cALLed LoSe f0LloWeRs FridAy... smH   \n",
       "40     Car not happy, big big dent in boot! Hoping theyre not going to write it off, crossing fingers and waiting   \n",
       "...                                                                                                           ...   \n",
       "3487                                                                             Bored, making a mothers day card   \n",
       "3489                                                                      I think Max (my cat) may really be gone   \n",
       "3490                             `m working on a logo on photoshop & it didn`t work out..  hehe.. try again lah..   \n",
       "3515                                                    im riding the highs and lows of moods now.... chores blow   \n",
       "3520                                       Where`d the songs go on the site, I want 'Do You' on this computer too   \n",
       "\n",
       "        set sentiment predicted_sentiment  \n",
       "5     train   neutral            negative  \n",
       "20    train   neutral            negative  \n",
       "23    train   neutral            negative  \n",
       "24    train   neutral            negative  \n",
       "40    train   neutral            negative  \n",
       "...     ...       ...                 ...  \n",
       "3487   test   neutral            negative  \n",
       "3489   test   neutral            negative  \n",
       "3490   test   neutral            negative  \n",
       "3515   test   neutral            negative  \n",
       "3520   test   neutral            negative  \n",
       "\n",
       "[3220 rows x 4 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[(data.sentiment == \"neutral\") & (data.predicted_sentiment == \"negative\")] # most incorrect in this cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span Detection Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_FOLDS):\n",
    "    if os.path.exists(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\"):\n",
    "        os.remove(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(pred_dict):\n",
    "    print(\"[INFO] \",\"=\"*15,\"Validation for FOLD#\", num, \"=\"*15)\n",
    "    funcs = [accuracy_score, f1_score, precision_score, recall_score, confusion_matrix]\n",
    "    for f in funcs:\n",
    "        for data_set in [\"train\",\"valid\"]:\n",
    "            for var in [\"starts\", \"stops\"]:\n",
    "                if f in [accuracy_score]:\n",
    "                    res = f(**pred_dict[data_set][var])\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "                elif f in [confusion_matrix]:\n",
    "                    res = f(**pred_dict[data_set][var], labels=np.arange(MAX_SEQ_LEN))\n",
    "                    np.savetxt(X=res, fmt='%i', delimiter=\",\",\n",
    "                               fname=RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                    print(\"[INFO] \\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__, \"\\t||\", \n",
    "                          RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                else:\n",
    "                    res = f(**pred_dict[data_set][var], average=\"macro\")\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "        print(\"=======================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(string):\n",
    "    string = re.sub(pattern=\" (negative|positive|neutral)[ ]+$\", repl=\"\", string=string)\n",
    "    string = re.sub(pattern=\" +\", repl=\" \", string=string.strip())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><h2 id=\"finetuning\">Fine-tuning</h2></a>\n",
    "<p>Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.</p>\n",
    " <p>This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.</p>\n",
    " <p>It is critical to only do this step <em>after</em> the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.</p>\n",
    " <p>It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>\n",
    "\n",
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><p><strong>Important note about <code>compile()</code> and <code>trainable</code></strong></p></a>\n",
    "<p>Calling <code>compile()</code> on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the <code>trainable</code>\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until <code>compile</code> is called again. Hence, if you change any <code>trainable</code> value, make sure\n",
    " to call <code>compile()</code> again on your\n",
    "model for your changes to be taken into account.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_detection_model_bkup = span_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ==================== FOLD# 0 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/3\n",
      "11212/11212 [==============================] - 261s 23ms/sample - loss: 12.3471 - starts_0_loss: 2.7402 - stops_0_loss: 3.2976 - starts_1_loss: 2.8784 - stops_1_loss: 3.4304 - starts_0_accuracy: 0.3466 - stops_0_accuracy: 0.0725 - starts_1_accuracy: 0.3501 - stops_1_accuracy: 0.0670 - val_loss: 11.2429 - val_starts_0_loss: 2.4861 - val_stops_0_loss: 2.9533 - val_starts_1_loss: 2.5962 - val_stops_1_loss: 3.1995 - val_starts_0_accuracy: 0.3546 - val_stops_0_accuracy: 0.1002 - val_starts_1_accuracy: 0.3553 - val_stops_1_accuracy: 0.0603\n",
      "Epoch 2/3\n",
      "11212/11212 [==============================] - 243s 22ms/sample - loss: 10.7090 - starts_0_loss: 2.4258 - stops_0_loss: 2.8601 - starts_1_loss: 2.4469 - stops_1_loss: 2.9760 - starts_0_accuracy: 0.3389 - stops_0_accuracy: 0.1456 - starts_1_accuracy: 0.3522 - stops_1_accuracy: 0.1171 - val_loss: 9.7526 - val_starts_0_loss: 2.1868 - val_stops_0_loss: 2.5271 - val_starts_1_loss: 2.2570 - val_stops_1_loss: 2.7715 - val_starts_0_accuracy: 0.3646 - val_stops_0_accuracy: 0.2397 - val_starts_1_accuracy: 0.3564 - val_stops_1_accuracy: 0.1720\n",
      "Epoch 3/3\n",
      "11212/11212 [==============================] - 247s 22ms/sample - loss: 10.0848 - starts_0_loss: 2.3119 - stops_0_loss: 2.6770 - starts_1_loss: 2.3164 - stops_1_loss: 2.7799 - starts_0_accuracy: 0.3464 - stops_0_accuracy: 0.2122 - starts_1_accuracy: 0.3530 - stops_1_accuracy: 0.1815 - val_loss: 9.1844 - val_starts_0_loss: 2.1076 - val_stops_0_loss: 2.3635 - val_starts_1_loss: 2.1567 - val_stops_1_loss: 2.5521 - val_starts_0_accuracy: 0.3789 - val_stops_0_accuracy: 0.3403 - val_starts_1_accuracy: 0.3692 - val_stops_1_accuracy: 0.2747\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/3\n",
      "11212/11212 [==============================] - 265s 24ms/sample - loss: 9.7332 - starts_0_loss: 2.2355 - stops_0_loss: 2.5569 - starts_1_loss: 2.2620 - stops_1_loss: 2.6788 - starts_0_accuracy: 0.3542 - stops_0_accuracy: 0.2447 - starts_1_accuracy: 0.3577 - stops_1_accuracy: 0.2236 - val_loss: 8.9184 - val_starts_0_loss: 2.0353 - val_stops_0_loss: 2.2501 - val_starts_1_loss: 2.1202 - val_stops_1_loss: 2.5065 - val_starts_0_accuracy: 0.3789 - val_stops_0_accuracy: 0.3585 - val_starts_1_accuracy: 0.3678 - val_stops_1_accuracy: 0.2972\n",
      "Epoch 2/3\n",
      "11212/11212 [==============================] - 251s 22ms/sample - loss: 9.6903 - starts_0_loss: 2.2206 - stops_0_loss: 2.5461 - starts_1_loss: 2.2523 - stops_1_loss: 2.6711 - starts_0_accuracy: 0.3574 - stops_0_accuracy: 0.2590 - starts_1_accuracy: 0.3577 - stops_1_accuracy: 0.2252 - val_loss: 8.8714 - val_starts_0_loss: 2.0259 - val_stops_0_loss: 2.2375 - val_starts_1_loss: 2.1142 - val_stops_1_loss: 2.4880 - val_starts_0_accuracy: 0.3810 - val_stops_0_accuracy: 0.3560 - val_starts_1_accuracy: 0.3664 - val_stops_1_accuracy: 0.3015\n",
      "Epoch 3/3\n",
      "11212/11212 [==============================] - 244s 22ms/sample - loss: 9.6450 - starts_0_loss: 2.2101 - stops_0_loss: 2.5324 - starts_1_loss: 2.2439 - stops_1_loss: 2.6588 - starts_0_accuracy: 0.3571 - stops_0_accuracy: 0.2558 - starts_1_accuracy: 0.3577 - stops_1_accuracy: 0.2254 - val_loss: 8.8428 - val_starts_0_loss: 2.0209 - val_stops_0_loss: 2.2249 - val_starts_1_loss: 2.1118 - val_stops_1_loss: 2.4794 - val_starts_0_accuracy: 0.3810 - val_stops_0_accuracy: 0.3575 - val_starts_1_accuracy: 0.3664 - val_stops_1_accuracy: 0.2997\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "11212/11212 [==============================] - 427s 38ms/sample - loss: 8.7230 - starts_0_loss: 1.9870 - stops_0_loss: 2.2431 - starts_1_loss: 2.0735 - stops_1_loss: 2.4196 - starts_0_accuracy: 0.3898 - stops_0_accuracy: 0.3346 - starts_1_accuracy: 0.3731 - stops_1_accuracy: 0.3014 - val_loss: 8.2013 - val_starts_0_loss: 1.8410 - val_stops_0_loss: 2.0930 - val_starts_1_loss: 1.9464 - val_stops_1_loss: 2.3159 - val_starts_0_accuracy: 0.4178 - val_stops_0_accuracy: 0.3721 - val_starts_1_accuracy: 0.3882 - val_stops_1_accuracy: 0.3375\n",
      "Epoch 2/3\n",
      "11212/11212 [==============================] - 406s 36ms/sample - loss: 8.2795 - starts_0_loss: 1.8783 - stops_0_loss: 2.0990 - starts_1_loss: 1.9906 - stops_1_loss: 2.3110 - starts_0_accuracy: 0.4076 - stops_0_accuracy: 0.3701 - starts_1_accuracy: 0.3837 - stops_1_accuracy: 0.3375 - val_loss: 8.0041 - val_starts_0_loss: 1.7818 - val_stops_0_loss: 2.0341 - val_starts_1_loss: 1.9128 - val_stops_1_loss: 2.2670 - val_starts_0_accuracy: 0.4281 - val_stops_0_accuracy: 0.3942 - val_starts_1_accuracy: 0.3949 - val_stops_1_accuracy: 0.3585\n",
      "Epoch 3/3\n",
      "11212/11212 [==============================] - 405s 36ms/sample - loss: 8.0766 - starts_0_loss: 1.8253 - stops_0_loss: 2.0342 - starts_1_loss: 1.9605 - stops_1_loss: 2.2567 - starts_0_accuracy: 0.4177 - stops_0_accuracy: 0.3871 - starts_1_accuracy: 0.3893 - stops_1_accuracy: 0.3574 - val_loss: 7.8232 - val_starts_0_loss: 1.7314 - val_stops_0_loss: 1.9643 - val_starts_1_loss: 1.8936 - val_stops_1_loss: 2.2265 - val_starts_0_accuracy: 0.4352 - val_stops_0_accuracy: 0.4060 - val_starts_1_accuracy: 0.4010 - val_stops_1_accuracy: 0.3657\n",
      "[INFO]  =============== Validation for FOLD# 0 ===============\n",
      "[INFO] 44.86 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 43.63 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 43.52 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 40.60 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 21.09 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 38.36 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 20.67 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 40.84 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 28.67 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 38.78 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 24.40 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 41.65 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 20.60 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 39.42 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 21.82 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 42.98 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t||"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (11212, 100) (11212, 100)\n",
      "[INFO] Prediction shape for validation data:  (2803, 100) (2803, 100)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  10135 out of 11212\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  2532 out of 2803\n",
      "[INFO] Training Jaccard Score:  0.5315548736917475\n",
      "[INFO] Validation Jaccard Score:  0.5273985721624449\n",
      "[INFO] Training for fold: 0 finished at Sun May 31 17:07:19 2020\n",
      "[INFO] ==================== FOLD# 1 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/3\n",
      "11212/11212 [==============================] - 265s 24ms/sample - loss: 7.8747 - starts_0_loss: 1.7757 - stops_0_loss: 1.9930 - starts_1_loss: 1.8974 - stops_1_loss: 2.2089 - starts_0_accuracy: 0.4262 - stops_0_accuracy: 0.3984 - starts_1_accuracy: 0.3972 - stops_1_accuracy: 0.3767 - val_loss: 7.1782 - val_starts_0_loss: 1.5678 - val_stops_0_loss: 1.8003 - val_starts_1_loss: 1.7451 - val_stops_1_loss: 2.0591 - val_starts_0_accuracy: 0.4688 - val_stops_0_accuracy: 0.4270 - val_starts_1_accuracy: 0.4313 - val_stops_1_accuracy: 0.4031\n",
      "Epoch 2/3\n",
      "11212/11212 [==============================] - 246s 22ms/sample - loss: 7.5361 - starts_0_loss: 1.6993 - stops_0_loss: 1.9261 - starts_1_loss: 1.8060 - stops_1_loss: 2.1050 - starts_0_accuracy: 0.4480 - stops_0_accuracy: 0.4188 - starts_1_accuracy: 0.4301 - stops_1_accuracy: 0.4031 - val_loss: 7.0466 - val_starts_0_loss: 1.5462 - val_stops_0_loss: 1.7673 - val_starts_1_loss: 1.7099 - val_stops_1_loss: 2.0185 - val_starts_0_accuracy: 0.4731 - val_stops_0_accuracy: 0.4399 - val_starts_1_accuracy: 0.4577 - val_stops_1_accuracy: 0.4320\n",
      "Epoch 3/3\n",
      "11212/11212 [==============================] - 246s 22ms/sample - loss: 7.3219 - starts_0_loss: 1.6567 - stops_0_loss: 1.8846 - starts_1_loss: 1.7474 - stops_1_loss: 2.0326 - starts_0_accuracy: 0.4626 - stops_0_accuracy: 0.4298 - starts_1_accuracy: 0.4461 - stops_1_accuracy: 0.4208 - val_loss: 6.8320 - val_starts_0_loss: 1.4874 - val_stops_0_loss: 1.7338 - val_starts_1_loss: 1.6590 - val_stops_1_loss: 1.9449 - val_starts_0_accuracy: 0.4873 - val_stops_0_accuracy: 0.4463 - val_starts_1_accuracy: 0.4702 - val_stops_1_accuracy: 0.4467\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/3\n",
      "11212/11212 [==============================] - 264s 24ms/sample - loss: 7.1187 - starts_0_loss: 1.6066 - stops_0_loss: 1.8343 - starts_1_loss: 1.6993 - stops_1_loss: 1.9783 - starts_0_accuracy: 0.4791 - stops_0_accuracy: 0.4359 - starts_1_accuracy: 0.4622 - stops_1_accuracy: 0.4356 - val_loss: 6.8149 - val_starts_0_loss: 1.4855 - val_stops_0_loss: 1.7232 - val_starts_1_loss: 1.6553 - val_stops_1_loss: 1.9444 - val_starts_0_accuracy: 0.4898 - val_stops_0_accuracy: 0.4427 - val_starts_1_accuracy: 0.4691 - val_stops_1_accuracy: 0.4395\n",
      "Epoch 2/3\n",
      "11212/11212 [==============================] - 247s 22ms/sample - loss: 7.1189 - starts_0_loss: 1.6012 - stops_0_loss: 1.8433 - starts_1_loss: 1.6995 - stops_1_loss: 1.9751 - starts_0_accuracy: 0.4798 - stops_0_accuracy: 0.4419 - starts_1_accuracy: 0.4597 - stops_1_accuracy: 0.4371 - val_loss: 6.8169 - val_starts_0_loss: 1.4856 - val_stops_0_loss: 1.7247 - val_starts_1_loss: 1.6557 - val_stops_1_loss: 1.9441 - val_starts_0_accuracy: 0.4884 - val_stops_0_accuracy: 0.4420 - val_starts_1_accuracy: 0.4716 - val_stops_1_accuracy: 0.4399\n",
      "Epoch 3/3\n",
      "11212/11212 [==============================] - 246s 22ms/sample - loss: 7.0817 - starts_0_loss: 1.5916 - stops_0_loss: 1.8326 - starts_1_loss: 1.6920 - stops_1_loss: 1.9659 - starts_0_accuracy: 0.4798 - stops_0_accuracy: 0.4434 - starts_1_accuracy: 0.4636 - stops_1_accuracy: 0.4394 - val_loss: 6.8159 - val_starts_0_loss: 1.4856 - val_stops_0_loss: 1.7255 - val_starts_1_loss: 1.6545 - val_stops_1_loss: 1.9436 - val_starts_0_accuracy: 0.4888 - val_stops_0_accuracy: 0.4438 - val_starts_1_accuracy: 0.4709 - val_stops_1_accuracy: 0.4420\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "11212/11212 [==============================] - 433s 39ms/sample - loss: 6.9801 - starts_0_loss: 1.5557 - stops_0_loss: 1.8055 - starts_1_loss: 1.6613 - stops_1_loss: 1.9577 - starts_0_accuracy: 0.4968 - stops_0_accuracy: 0.4473 - starts_1_accuracy: 0.4749 - stops_1_accuracy: 0.4404 - val_loss: 6.6809 - val_starts_0_loss: 1.4410 - val_stops_0_loss: 1.6945 - val_starts_1_loss: 1.6144 - val_stops_1_loss: 1.9241 - val_starts_0_accuracy: 0.5105 - val_stops_0_accuracy: 0.4559 - val_starts_1_accuracy: 0.4848 - val_stops_1_accuracy: 0.4581\n",
      "Epoch 2/3\n",
      "11212/11212 [==============================] - 411s 37ms/sample - loss: 6.8294 - starts_0_loss: 1.5204 - stops_0_loss: 1.7621 - starts_1_loss: 1.6331 - stops_1_loss: 1.9135 - starts_0_accuracy: 0.4994 - stops_0_accuracy: 0.4588 - starts_1_accuracy: 0.4839 - stops_1_accuracy: 0.4528 - val_loss: 6.6159 - val_starts_0_loss: 1.4230 - val_stops_0_loss: 1.6749 - val_starts_1_loss: 1.5971 - val_stops_1_loss: 1.9133 - val_starts_0_accuracy: 0.5130 - val_stops_0_accuracy: 0.4613 - val_starts_1_accuracy: 0.4955 - val_stops_1_accuracy: 0.4631\n",
      "Epoch 3/3\n",
      "11212/11212 [==============================] - 413s 37ms/sample - loss: 6.7066 - starts_0_loss: 1.4913 - stops_0_loss: 1.7222 - starts_1_loss: 1.6077 - stops_1_loss: 1.8848 - starts_0_accuracy: 0.5077 - stops_0_accuracy: 0.4712 - starts_1_accuracy: 0.4906 - stops_1_accuracy: 0.4605 - val_loss: 6.5740 - val_starts_0_loss: 1.4091 - val_stops_0_loss: 1.6667 - val_starts_1_loss: 1.5831 - val_stops_1_loss: 1.9070 - val_starts_0_accuracy: 0.5262 - val_stops_0_accuracy: 0.4691 - val_starts_1_accuracy: 0.4998 - val_stops_1_accuracy: 0.4706\n",
      "[INFO]  =============== Validation for FOLD# 1 ===============\n",
      "[INFO] 56.96 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 52.15 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 52.62 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 46.91 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 50.53 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 50.94 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 34.34 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 41.87 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 54.73 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 49.93 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 36.86 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 43.49 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 49.12 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 53.26 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 34.73 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 42.83 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (11212, 100) (11212, 100)\n",
      "[INFO] Prediction shape for validation data:  (2803, 100) (2803, 100)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  8041 out of 11212\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  2047 out of 2803\n",
      "[INFO] Training Jaccard Score:  0.5250666551523291\n",
      "[INFO] Validation Jaccard Score:  0.5129233500933836\n",
      "[INFO] Training for fold: 1 finished at Sun May 31 17:55:28 2020\n",
      "[INFO] ==================== FOLD# 2 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/3\n",
      "11212/11212 [==============================] - 272s 24ms/sample - loss: 6.8666 - starts_0_loss: 1.5306 - stops_0_loss: 1.7662 - starts_1_loss: 1.6378 - stops_1_loss: 1.9328 - starts_0_accuracy: 0.4929 - stops_0_accuracy: 0.4615 - starts_1_accuracy: 0.4790 - stops_1_accuracy: 0.4571 - val_loss: 5.9780 - val_starts_0_loss: 1.3203 - val_stops_0_loss: 1.5058 - val_starts_1_loss: 1.4542 - val_stops_1_loss: 1.7037 - val_starts_0_accuracy: 0.5555 - val_stops_0_accuracy: 0.5091 - val_starts_1_accuracy: 0.5391 - val_stops_1_accuracy: 0.5027\n",
      "Epoch 2/3\n",
      "11212/11212 [==============================] - 246s 22ms/sample - loss: 6.7623 - starts_0_loss: 1.5108 - stops_0_loss: 1.7460 - starts_1_loss: 1.6082 - stops_1_loss: 1.8974 - starts_0_accuracy: 0.4976 - stops_0_accuracy: 0.4625 - starts_1_accuracy: 0.4917 - stops_1_accuracy: 0.4592 - val_loss: 5.9803 - val_starts_0_loss: 1.3146 - val_stops_0_loss: 1.5034 - val_starts_1_loss: 1.4531 - val_stops_1_loss: 1.7202 - val_starts_0_accuracy: 0.5537 - val_stops_0_accuracy: 0.5080 - val_starts_1_accuracy: 0.5405 - val_stops_1_accuracy: 0.5048\n",
      "Epoch 3/3\n",
      "11212/11212 [==============================] - 252s 22ms/sample - loss: 6.6933 - starts_0_loss: 1.4951 - stops_0_loss: 1.7290 - starts_1_loss: 1.5902 - stops_1_loss: 1.8789 - starts_0_accuracy: 0.5023 - stops_0_accuracy: 0.4600 - starts_1_accuracy: 0.4941 - stops_1_accuracy: 0.4580 - val_loss: 5.9900 - val_starts_0_loss: 1.3195 - val_stops_0_loss: 1.5061 - val_starts_1_loss: 1.4536 - val_stops_1_loss: 1.7167 - val_starts_0_accuracy: 0.5444 - val_stops_0_accuracy: 0.5037 - val_starts_1_accuracy: 0.5334 - val_stops_1_accuracy: 0.5027\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/3\n",
      "11212/11212 [==============================] - 270s 24ms/sample - loss: 6.4855 - starts_0_loss: 1.4486 - stops_0_loss: 1.6692 - starts_1_loss: 1.5411 - stops_1_loss: 1.8264 - starts_0_accuracy: 0.5163 - stops_0_accuracy: 0.4783 - starts_1_accuracy: 0.5071 - stops_1_accuracy: 0.4780 - val_loss: 5.9520 - val_starts_0_loss: 1.3072 - val_stops_0_loss: 1.4964 - val_starts_1_loss: 1.4466 - val_stops_1_loss: 1.7089 - val_starts_0_accuracy: 0.5444 - val_stops_0_accuracy: 0.5087 - val_starts_1_accuracy: 0.5387 - val_stops_1_accuracy: 0.5052\n",
      "Epoch 2/3\n",
      "11212/11212 [==============================] - 249s 22ms/sample - loss: 6.4355 - starts_0_loss: 1.4374 - stops_0_loss: 1.6629 - starts_1_loss: 1.5308 - stops_1_loss: 1.8045 - starts_0_accuracy: 0.5185 - stops_0_accuracy: 0.4824 - starts_1_accuracy: 0.5088 - stops_1_accuracy: 0.4813 - val_loss: 5.9378 - val_starts_0_loss: 1.3031 - val_stops_0_loss: 1.4954 - val_starts_1_loss: 1.4431 - val_stops_1_loss: 1.7036 - val_starts_0_accuracy: 0.5491 - val_stops_0_accuracy: 0.5055 - val_starts_1_accuracy: 0.5380 - val_stops_1_accuracy: 0.5037\n",
      "Epoch 3/3\n",
      "11212/11212 [==============================] - 249s 22ms/sample - loss: 6.3977 - starts_0_loss: 1.4226 - stops_0_loss: 1.6600 - starts_1_loss: 1.5138 - stops_1_loss: 1.8014 - starts_0_accuracy: 0.5244 - stops_0_accuracy: 0.4824 - starts_1_accuracy: 0.5211 - stops_1_accuracy: 0.4847 - val_loss: 5.9260 - val_starts_0_loss: 1.3001 - val_stops_0_loss: 1.4924 - val_starts_1_loss: 1.4410 - val_stops_1_loss: 1.7001 - val_starts_0_accuracy: 0.5505 - val_stops_0_accuracy: 0.5059 - val_starts_1_accuracy: 0.5405 - val_stops_1_accuracy: 0.5062\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "11212/11212 [==============================] - 438s 39ms/sample - loss: 6.3714 - starts_0_loss: 1.4202 - stops_0_loss: 1.6460 - starts_1_loss: 1.5156 - stops_1_loss: 1.7894 - starts_0_accuracy: 0.5254 - stops_0_accuracy: 0.4881 - starts_1_accuracy: 0.5177 - stops_1_accuracy: 0.4905 - val_loss: 5.8678 - val_starts_0_loss: 1.2813 - val_stops_0_loss: 1.4808 - val_starts_1_loss: 1.4247 - val_stops_1_loss: 1.6885 - val_starts_0_accuracy: 0.5573 - val_stops_0_accuracy: 0.5152 - val_starts_1_accuracy: 0.5458 - val_stops_1_accuracy: 0.5091\n",
      "Epoch 2/3\n",
      "11212/11212 [==============================] - 409s 36ms/sample - loss: 6.3256 - starts_0_loss: 1.4071 - stops_0_loss: 1.6355 - starts_1_loss: 1.5028 - stops_1_loss: 1.7798 - starts_0_accuracy: 0.5259 - stops_0_accuracy: 0.4854 - starts_1_accuracy: 0.5211 - stops_1_accuracy: 0.4875 - val_loss: 5.8360 - val_starts_0_loss: 1.2807 - val_stops_0_loss: 1.4658 - val_starts_1_loss: 1.4279 - val_stops_1_loss: 1.6699 - val_starts_0_accuracy: 0.5540 - val_stops_0_accuracy: 0.5162 - val_starts_1_accuracy: 0.5487 - val_stops_1_accuracy: 0.5134\n",
      "Epoch 3/3\n",
      "11212/11212 [==============================] - 411s 37ms/sample - loss: 6.2003 - starts_0_loss: 1.3771 - stops_0_loss: 1.5966 - starts_1_loss: 1.4774 - stops_1_loss: 1.7493 - starts_0_accuracy: 0.5333 - stops_0_accuracy: 0.4967 - starts_1_accuracy: 0.5248 - stops_1_accuracy: 0.4974 - val_loss: 5.7953 - val_starts_0_loss: 1.2733 - val_stops_0_loss: 1.4523 - val_starts_1_loss: 1.4171 - val_stops_1_loss: 1.6606 - val_starts_0_accuracy: 0.5583 - val_stops_0_accuracy: 0.5169 - val_starts_1_accuracy: 0.5555 - val_stops_1_accuracy: 0.5198\n",
      "[INFO]  =============== Validation for FOLD# 2 ===============\n",
      "[INFO] 59.86 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 54.92 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 55.83 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 51.69 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 58.78 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 57.59 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 41.70 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 50.33 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 61.17 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 55.58 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 45.13 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 50.02 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 58.57 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 61.25 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 41.62 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 54.14 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (11212, 100) (11212, 100)\n",
      "[INFO] Prediction shape for validation data:  (2803, 100) (2803, 100)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  7640 out of 11212\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  1979 out of 2803\n",
      "[INFO] Training Jaccard Score:  0.5456726077854626\n",
      "[INFO] Validation Jaccard Score:  0.5484772065328438\n",
      "[INFO] Training for fold: 2 finished at Sun May 31 18:44:02 2020\n",
      "[INFO] ==================== FOLD# 3 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/3\n",
      "11212/11212 [==============================] - 264s 24ms/sample - loss: 6.4606 - starts_0_loss: 1.4461 - stops_0_loss: 1.6527 - starts_1_loss: 1.5474 - stops_1_loss: 1.8139 - starts_0_accuracy: 0.5191 - stops_0_accuracy: 0.4864 - starts_1_accuracy: 0.5095 - stops_1_accuracy: 0.4829 - val_loss: 5.5639 - val_starts_0_loss: 1.1865 - val_stops_0_loss: 1.4029 - val_starts_1_loss: 1.3363 - val_stops_1_loss: 1.6331 - val_starts_0_accuracy: 0.5829 - val_stops_0_accuracy: 0.5362 - val_starts_1_accuracy: 0.5722 - val_stops_1_accuracy: 0.5405\n",
      "Epoch 2/3\n",
      "11212/11212 [==============================] - 249s 22ms/sample - loss: 6.3247 - starts_0_loss: 1.4227 - stops_0_loss: 1.6179 - starts_1_loss: 1.5168 - stops_1_loss: 1.7670 - starts_0_accuracy: 0.5232 - stops_0_accuracy: 0.4933 - starts_1_accuracy: 0.5182 - stops_1_accuracy: 0.4909 - val_loss: 5.5225 - val_starts_0_loss: 1.2109 - val_stops_0_loss: 1.3888 - val_starts_1_loss: 1.3288 - val_stops_1_loss: 1.5898 - val_starts_0_accuracy: 0.5829 - val_stops_0_accuracy: 0.5448 - val_starts_1_accuracy: 0.5747 - val_stops_1_accuracy: 0.5423\n",
      "Epoch 3/3\n",
      "11212/11212 [==============================] - 251s 22ms/sample - loss: 6.2856 - starts_0_loss: 1.4095 - stops_0_loss: 1.6107 - starts_1_loss: 1.5030 - stops_1_loss: 1.7617 - starts_0_accuracy: 0.5269 - stops_0_accuracy: 0.4955 - starts_1_accuracy: 0.5177 - stops_1_accuracy: 0.4949 - val_loss: 5.6967 - val_starts_0_loss: 1.2551 - val_stops_0_loss: 1.4174 - val_starts_1_loss: 1.3816 - val_stops_1_loss: 1.6420 - val_starts_0_accuracy: 0.5612 - val_stops_0_accuracy: 0.5266 - val_starts_1_accuracy: 0.5598 - val_stops_1_accuracy: 0.5341\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/3\n",
      "11212/11212 [==============================] - 269s 24ms/sample - loss: 6.1704 - starts_0_loss: 1.3828 - stops_0_loss: 1.5849 - starts_1_loss: 1.4706 - stops_1_loss: 1.7321 - starts_0_accuracy: 0.5351 - stops_0_accuracy: 0.4985 - starts_1_accuracy: 0.5326 - stops_1_accuracy: 0.5017 - val_loss: 5.6046 - val_starts_0_loss: 1.2160 - val_stops_0_loss: 1.4040 - val_starts_1_loss: 1.3563 - val_stops_1_loss: 1.6278 - val_starts_0_accuracy: 0.5762 - val_stops_0_accuracy: 0.5294 - val_starts_1_accuracy: 0.5648 - val_stops_1_accuracy: 0.5380\n",
      "Epoch 2/3\n",
      "11212/11212 [==============================] - 252s 22ms/sample - loss: 6.0958 - starts_0_loss: 1.3594 - stops_0_loss: 1.5731 - starts_1_loss: 1.4506 - stops_1_loss: 1.7117 - starts_0_accuracy: 0.5506 - stops_0_accuracy: 0.5017 - starts_1_accuracy: 0.5417 - stops_1_accuracy: 0.5078 - val_loss: 5.5868 - val_starts_0_loss: 1.2136 - val_stops_0_loss: 1.4013 - val_starts_1_loss: 1.3504 - val_stops_1_loss: 1.6209 - val_starts_0_accuracy: 0.5801 - val_stops_0_accuracy: 0.5330 - val_starts_1_accuracy: 0.5662 - val_stops_1_accuracy: 0.5412\n",
      "Epoch 3/3\n",
      "11212/11212 [==============================] - 250s 22ms/sample - loss: 6.0700 - starts_0_loss: 1.3624 - stops_0_loss: 1.5610 - starts_1_loss: 1.4504 - stops_1_loss: 1.6958 - starts_0_accuracy: 0.5438 - stops_0_accuracy: 0.5092 - starts_1_accuracy: 0.5404 - stops_1_accuracy: 0.5131 - val_loss: 5.5724 - val_starts_0_loss: 1.2107 - val_stops_0_loss: 1.3983 - val_starts_1_loss: 1.3465 - val_stops_1_loss: 1.6161 - val_starts_0_accuracy: 0.5783 - val_stops_0_accuracy: 0.5323 - val_starts_1_accuracy: 0.5672 - val_stops_1_accuracy: 0.5412\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "11212/11212 [==============================] - 433s 39ms/sample - loss: 6.0433 - starts_0_loss: 1.3526 - stops_0_loss: 1.5533 - starts_1_loss: 1.4442 - stops_1_loss: 1.6931 - starts_0_accuracy: 0.5474 - stops_0_accuracy: 0.5029 - starts_1_accuracy: 0.5376 - stops_1_accuracy: 0.5153 - val_loss: 5.5564 - val_starts_0_loss: 1.2056 - val_stops_0_loss: 1.3937 - val_starts_1_loss: 1.3407 - val_stops_1_loss: 1.6156 - val_starts_0_accuracy: 0.5780 - val_stops_0_accuracy: 0.5355 - val_starts_1_accuracy: 0.5705 - val_stops_1_accuracy: 0.5408\n",
      "Epoch 2/3\n",
      "11212/11212 [==============================] - 413s 37ms/sample - loss: 6.0215 - starts_0_loss: 1.3409 - stops_0_loss: 1.5542 - starts_1_loss: 1.4347 - stops_1_loss: 1.6915 - starts_0_accuracy: 0.5500 - stops_0_accuracy: 0.5074 - starts_1_accuracy: 0.5396 - stops_1_accuracy: 0.5116 - val_loss: 5.5448 - val_starts_0_loss: 1.2018 - val_stops_0_loss: 1.3921 - val_starts_1_loss: 1.3379 - val_stops_1_loss: 1.6123 - val_starts_0_accuracy: 0.5719 - val_stops_0_accuracy: 0.5362 - val_starts_1_accuracy: 0.5658 - val_stops_1_accuracy: 0.5419\n",
      "Epoch 3/3\n",
      "11212/11212 [==============================] - 414s 37ms/sample - loss: 5.9569 - starts_0_loss: 1.3336 - stops_0_loss: 1.5282 - starts_1_loss: 1.4274 - stops_1_loss: 1.6673 - starts_0_accuracy: 0.5458 - stops_0_accuracy: 0.5149 - starts_1_accuracy: 0.5400 - stops_1_accuracy: 0.5221 - val_loss: 5.5433 - val_starts_0_loss: 1.2006 - val_stops_0_loss: 1.3860 - val_starts_1_loss: 1.3404 - val_stops_1_loss: 1.6155 - val_starts_0_accuracy: 0.5783 - val_stops_0_accuracy: 0.5433 - val_starts_1_accuracy: 0.5712 - val_stops_1_accuracy: 0.5419\n",
      "[INFO]  =============== Validation for FOLD# 3 ===============\n",
      "[INFO] 61.02 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 57.46 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 57.83 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 54.33 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 61.97 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 61.88 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 48.81 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 50.64 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 64.90 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 60.03 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 51.38 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 50.50 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 61.56 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 65.26 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 48.53 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 52.20 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (11212, 100) (11212, 100)\n",
      "[INFO] Prediction shape for validation data:  (2803, 100) (2803, 100)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  7713 out of 11212\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  1919 out of 2803\n",
      "[INFO] Training Jaccard Score:  0.546382694353408\n",
      "[INFO] Validation Jaccard Score:  0.5461245507288603\n",
      "[INFO] Training for fold: 3 finished at Sun May 31 19:32:36 2020\n",
      "[INFO] ==================== FOLD# 4 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/3\n",
      "11212/11212 [==============================] - 270s 24ms/sample - loss: 6.1482 - starts_0_loss: 1.3736 - stops_0_loss: 1.5646 - starts_1_loss: 1.4741 - stops_1_loss: 1.7353 - starts_0_accuracy: 0.5396 - stops_0_accuracy: 0.5029 - starts_1_accuracy: 0.5310 - stops_1_accuracy: 0.5046 - val_loss: 5.3800 - val_starts_0_loss: 1.1965 - val_stops_0_loss: 1.3297 - val_starts_1_loss: 1.3156 - val_stops_1_loss: 1.5401 - val_starts_0_accuracy: 0.5780 - val_stops_0_accuracy: 0.5615 - val_starts_1_accuracy: 0.5787 - val_stops_1_accuracy: 0.5587\n",
      "Epoch 2/3\n",
      "11212/11212 [==============================] - 256s 23ms/sample - loss: 6.0721 - starts_0_loss: 1.3656 - stops_0_loss: 1.5434 - starts_1_loss: 1.4608 - stops_1_loss: 1.7019 - starts_0_accuracy: 0.5388 - stops_0_accuracy: 0.5052 - starts_1_accuracy: 0.5330 - stops_1_accuracy: 0.5132 - val_loss: 5.4081 - val_starts_0_loss: 1.2047 - val_stops_0_loss: 1.3336 - val_starts_1_loss: 1.3230 - val_stops_1_loss: 1.5502 - val_starts_0_accuracy: 0.5826 - val_stops_0_accuracy: 0.5605 - val_starts_1_accuracy: 0.5787 - val_stops_1_accuracy: 0.5544\n",
      "Epoch 3/3\n",
      "11212/11212 [==============================] - 253s 23ms/sample - loss: 6.0346 - starts_0_loss: 1.3483 - stops_0_loss: 1.5469 - starts_1_loss: 1.4449 - stops_1_loss: 1.6953 - starts_0_accuracy: 0.5466 - stops_0_accuracy: 0.5073 - starts_1_accuracy: 0.5417 - stops_1_accuracy: 0.5135 - val_loss: 5.4794 - val_starts_0_loss: 1.2230 - val_stops_0_loss: 1.3466 - val_starts_1_loss: 1.3517 - val_stops_1_loss: 1.5615 - val_starts_0_accuracy: 0.5829 - val_stops_0_accuracy: 0.5558 - val_starts_1_accuracy: 0.5694 - val_stops_1_accuracy: 0.5530\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/3\n",
      "11212/11212 [==============================] - 278s 25ms/sample - loss: 5.8658 - starts_0_loss: 1.3138 - stops_0_loss: 1.5023 - starts_1_loss: 1.4054 - stops_1_loss: 1.6445 - starts_0_accuracy: 0.5550 - stops_0_accuracy: 0.5204 - starts_1_accuracy: 0.5473 - stops_1_accuracy: 0.5280 - val_loss: 5.4500 - val_starts_0_loss: 1.2138 - val_stops_0_loss: 1.3402 - val_starts_1_loss: 1.3437 - val_stops_1_loss: 1.5550 - val_starts_0_accuracy: 0.5804 - val_stops_0_accuracy: 0.5530 - val_starts_1_accuracy: 0.5733 - val_stops_1_accuracy: 0.5548\n",
      "Epoch 2/3\n",
      "11212/11212 [==============================] - 254s 23ms/sample - loss: 5.8082 - starts_0_loss: 1.3056 - stops_0_loss: 1.4921 - starts_1_loss: 1.3918 - stops_1_loss: 1.6185 - starts_0_accuracy: 0.5549 - stops_0_accuracy: 0.5232 - starts_1_accuracy: 0.5518 - stops_1_accuracy: 0.5330 - val_loss: 5.4442 - val_starts_0_loss: 1.2129 - val_stops_0_loss: 1.3390 - val_starts_1_loss: 1.3426 - val_stops_1_loss: 1.5527 - val_starts_0_accuracy: 0.5844 - val_stops_0_accuracy: 0.5558 - val_starts_1_accuracy: 0.5747 - val_stops_1_accuracy: 0.5551\n",
      "Epoch 3/3\n",
      "11212/11212 [==============================] - 262s 23ms/sample - loss: 5.8086 - starts_0_loss: 1.3012 - stops_0_loss: 1.4927 - starts_1_loss: 1.3885 - stops_1_loss: 1.6269 - starts_0_accuracy: 0.5573 - stops_0_accuracy: 0.5178 - starts_1_accuracy: 0.5557 - stops_1_accuracy: 0.5308 - val_loss: 5.4371 - val_starts_0_loss: 1.2123 - val_stops_0_loss: 1.3356 - val_starts_1_loss: 1.3408 - val_stops_1_loss: 1.5512 - val_starts_0_accuracy: 0.5883 - val_stops_0_accuracy: 0.5565 - val_starts_1_accuracy: 0.5787 - val_stops_1_accuracy: 0.5576\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "11212/11212 [==============================] - 446s 40ms/sample - loss: 5.8324 - starts_0_loss: 1.3100 - stops_0_loss: 1.4890 - starts_1_loss: 1.4034 - stops_1_loss: 1.6294 - starts_0_accuracy: 0.5560 - stops_0_accuracy: 0.5178 - starts_1_accuracy: 0.5549 - stops_1_accuracy: 0.5272 - val_loss: 5.4509 - val_starts_0_loss: 1.2167 - val_stops_0_loss: 1.3348 - val_starts_1_loss: 1.3427 - val_stops_1_loss: 1.5600 - val_starts_0_accuracy: 0.5858 - val_stops_0_accuracy: 0.5612 - val_starts_1_accuracy: 0.5751 - val_stops_1_accuracy: 0.5565\n",
      "Epoch 2/3\n",
      "11212/11212 [==============================] - 419s 37ms/sample - loss: 5.7365 - starts_0_loss: 1.2884 - stops_0_loss: 1.4615 - starts_1_loss: 1.3794 - stops_1_loss: 1.6070 - starts_0_accuracy: 0.5576 - stops_0_accuracy: 0.5287 - starts_1_accuracy: 0.5539 - stops_1_accuracy: 0.5327 - val_loss: 5.4330 - val_starts_0_loss: 1.2210 - val_stops_0_loss: 1.3240 - val_starts_1_loss: 1.3413 - val_stops_1_loss: 1.5483 - val_starts_0_accuracy: 0.5847 - val_stops_0_accuracy: 0.5623 - val_starts_1_accuracy: 0.5751 - val_stops_1_accuracy: 0.5601\n",
      "Epoch 3/3\n",
      "11212/11212 [==============================] - 416s 37ms/sample - loss: 5.7199 - starts_0_loss: 1.2861 - stops_0_loss: 1.4577 - starts_1_loss: 1.3781 - stops_1_loss: 1.5970 - starts_0_accuracy: 0.5627 - stops_0_accuracy: 0.5327 - starts_1_accuracy: 0.5597 - stops_1_accuracy: 0.5448 - val_loss: 5.3933 - val_starts_0_loss: 1.2088 - val_stops_0_loss: 1.3131 - val_starts_1_loss: 1.3382 - val_stops_1_loss: 1.5348 - val_starts_0_accuracy: 0.5879 - val_stops_0_accuracy: 0.5676 - val_starts_1_accuracy: 0.5822 - val_stops_1_accuracy: 0.5648\n",
      "[INFO]  =============== Validation for FOLD# 4 ===============\n",
      "[INFO] 62.45 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 59.97 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 58.79 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 56.76 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 65.07 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 60.90 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 49.00 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 53.94 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 66.24 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 58.88 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 50.88 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 53.45 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 65.34 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 64.32 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 50.18 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 56.50 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V23_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (11212, 100) (11212, 100)\n",
      "[INFO] Prediction shape for validation data:  (2803, 100) (2803, 100)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  7640 out of 11212\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  1934 out of 2803\n",
      "[INFO] Training Jaccard Score:  0.555859316187246\n",
      "[INFO] Validation Jaccard Score:  0.5448266276614884\n",
      "[INFO] Training for fold: 4 finished at Sun May 31 20:22:08 2020\n",
      "Sun May 31 20:22:08 2020\n"
     ]
    }
   ],
   "source": [
    "for num, (t_index, v_index) in enumerate(sss.split(X_span, Y_span_stops)): # NOTICE the swap of val and train indices, on purpose for faster training\n",
    "    print(\"[INFO] ==================== FOLD#\", num, \"====================\")\n",
    "    \n",
    "    if num > 0:\n",
    "        del span_history\n",
    "        del span_detection_model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    \n",
    "    span_detection_model = span_detection_model_bkup\n",
    "    \n",
    "    span_mcp = ModelCheckpoint(filepath=RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\", monitor='val_loss',\n",
    "                               verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "    \n",
    "    span_csvl = CSVLogger(filename=RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(num)+\".csv\",\n",
    "                          separator=\",\",\n",
    "                          append=True)\n",
    "    \n",
    "    print(\"[INFO] Training only the final layers at higher learning rates.\")\n",
    "    span_detection_model.layers[3].trainable = False\n",
    "    adam = Adam(learning_rate=MAX_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[0],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl])\n",
    "    \n",
    "    print(\"[INFO] Training only the final layers at lower learning rates.\")\n",
    "    adam = Adam(learning_rate=MID_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[1],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl])\n",
    "    \n",
    "    print(\"[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\")\n",
    "    span_detection_model.layers[3].trainable = True\n",
    "    adam = Adam(learning_rate=MIN_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[2],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl])\n",
    "    \n",
    "    pred_train = span_detection_model.predict(x = {\"att_flags\":X_span_att[t_index],\n",
    "                                                   \"words\":X_span[t_index],\n",
    "                                                   \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                              batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "    pred_val = span_detection_model.predict(x = {\"att_flags\":X_span_att[v_index],\n",
    "                                                 \"words\":X_span[v_index],\n",
    "                                                 \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                            batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    pred_starts_train, pred_stops_train = (pred_train[0]/2.0 + pred_train[2]/2.0), (pred_train[1]/2.0 + pred_train[3]/2.0)\n",
    "    pred_starts_val, pred_stops_val = (pred_val[0]/2.0 + pred_val[2]/2.0), (pred_val[1]/2.0 + pred_val[3]/2.0)\n",
    "    \n",
    "    # Accumulate test results after training every fold\n",
    "    pred_test_fold = span_detection_model.predict(x = {\"att_flags\":X_span_att_test,\n",
    "                                                       \"words\":X_span_test,\n",
    "                                                       \"token_ids\":np.zeros_like(X_span_att_test)},\n",
    "                                                  batch_size=PREDICT_BATCH_SIZE)\n",
    "    if num==0:\n",
    "        pred_test = []\n",
    "        pred_test.append(pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test.append(pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    else:\n",
    "        pred_test[0] += (pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test[1] += (pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    \n",
    "    # Tabulate\n",
    "    preds = {\n",
    "        \"train\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_span_starts[t_index],\n",
    "                \"y_pred\":pred_train[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_span_stops[t_index],\n",
    "                \"y_pred\":pred_train[1].argmax(axis=1)\n",
    "            }\n",
    "        },\n",
    "        \"valid\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_span_starts[v_index],\n",
    "                \"y_pred\":pred_val[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_span_stops[v_index],\n",
    "                \"y_pred\":pred_val[1].argmax(axis=1)\n",
    "            }        \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print_metrics(pred_dict=preds)\n",
    "\n",
    "    print(\"[INFO] Prediction shape for training data: \", pred_starts_train.shape, pred_stops_train.shape)\n",
    "    print(\"[INFO] Prediction shape for validation data: \", pred_starts_val.shape, pred_stops_val.shape)\n",
    "\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for training data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_train.argmax(axis=1),\n",
    "                                  pred_stops_train.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_train.shape[0])\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for validation data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_val.argmax(axis=1),\n",
    "                                  pred_stops_val.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_val.shape[0],)\n",
    "\n",
    "    pred_words_train = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[t_index],\n",
    "                                                                                                                                   pred_starts_train.argmax(axis=1),\n",
    "                                                                                                                                   pred_stops_train.argmax(axis=1))]\n",
    "    \n",
    "    pred_words_val = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[v_index],\n",
    "                                                                                                                                 pred_starts_val.argmax(axis=1),\n",
    "                                                                                                                                 pred_stops_val.argmax(axis=1))]\n",
    "\n",
    "    print(\"[INFO] Training Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in t_index],\n",
    "                                                          pred_words_train)]))\n",
    "    print(\"[INFO] Validation Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in v_index],\n",
    "                                                          pred_words_val)]))\n",
    "    print(\"[INFO] Training for fold:\", num, \"finished at\", ctime(time()))\n",
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Prediction shape for testing data:  (3534, 100) (3534, 100)\n"
     ]
    }
   ],
   "source": [
    "pred_starts_test, pred_stops_test = pred_test[0]/NUM_FOLDS, pred_test[1]/NUM_FOLDS\n",
    "print(\"[INFO] Prediction shape for testing data: \", pred_starts_test.shape, pred_stops_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal predictions (StartIndex less than EndIndex) for testing data: 2826 out of 3534\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal predictions (StartIndex less than EndIndex) for testing data:\",\n",
    "      sum([s<e for s,e in zip(pred_starts_test.argmax(axis=1),\n",
    "                              pred_stops_test.argmax(axis=1))]), \n",
    "      \"out of\",\n",
    "      pred_starts_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words_test = [\n",
    "    post_process(tokenizer.decode(t[s:e+1])) if s<e else post_process(tokenizer.decode(t[e:])) for t,s,e in zip(X_span_test,\n",
    "                                                                                                              pred_starts_test.argmax(axis=1),\n",
    "                                                                                                              pred_stops_test.argmax(axis=1))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " yes! im down to 50% full on my dvr  i was at 98% like 3 days ago... lol i swear if i didnt have a dvr i would never watch tv  neutral \n",
      "1\n",
      "38\n",
      "yes! im down to 50% full on my dvr i was at 98% like 3 days ago... lol i swear if i didnt have a dvr i would never watch tv\n"
     ]
    }
   ],
   "source": [
    "check_idx = 158\n",
    "#print([[t,i,j,k] for t,i,j,k in zip(tokenizer.decode(),X_test[check_idx],pred_starts_test[check_idx],pred_stops_test[check_idx])])\n",
    "print(tokenizer.decode(X_span_test[check_idx]))\n",
    "print(pred_starts_test.argmax(axis=1)[check_idx])\n",
    "print(pred_stops_test.argmax(axis=1)[check_idx])\n",
    "print(post_process(tokenizer.decode(X_span_test[check_idx][pred_starts_test.argmax(axis=1)[check_idx]:1+pred_stops_test.argmax(axis=1)[check_idx]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span['selected_text'] = pred_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[\"selected_text\"] = np.where(test_df_span[\"sentiment\"] == \"neutral\",\n",
    "                                         test_df_span[\"text\"],\n",
    "                                         test_df_span[\"selected_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[[\"textID\", \"selected_text\"]].to_csv(RESULTS_DIR+\"submission_\"+MODEL_PREFIX+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>pansy  wtf codeh?!</td>\n",
       "      <td>negative</td>\n",
       "      <td>wtf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>grr @ naplan. just finished my commerce &amp; geo exams  good luck (Y)</td>\n",
       "      <td>positive</td>\n",
       "      <td>good luck (y)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3514</th>\n",
       "      <td>I just noticed that  They are so ridiculous</td>\n",
       "      <td>negative</td>\n",
       "      <td>ridiculous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>One of the most random phone calls ever. My god kill me now.</td>\n",
       "      <td>negative</td>\n",
       "      <td>god kill me now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>...oh. hahahahahaha but no, seriously, i tell the wait staff they look great all the time, its the easiest nice thing to do</td>\n",
       "      <td>positive</td>\n",
       "      <td>...oh. hahahahahaha but no, seriously, i tell the wait staff they look great all the time, its the easiest nice thing to do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>Hey, sorry I got off last night!</td>\n",
       "      <td>negative</td>\n",
       "      <td>sorry i got off last night!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3475</th>\n",
       "      <td>My room is too hot to sleep in.</td>\n",
       "      <td>negative</td>\n",
       "      <td>too hot to sleep in.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3522</th>\n",
       "      <td>no school today, teacher cancelled the lesson  chillin`</td>\n",
       "      <td>positive</td>\n",
       "      <td>chillin`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>its weird how thinking about may 2nd totally changed my crappy mood to happiness...i miss  so much</td>\n",
       "      <td>positive</td>\n",
       "      <td>its weird how thinking about may 2nd totally changed my crappy mood to happiness...i miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642</th>\n",
       "      <td>I`m all scared and bruised.</td>\n",
       "      <td>negative</td>\n",
       "      <td>scared and bruised.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>Jesus Christ meadowhall could do with better air con! Too hot</td>\n",
       "      <td>negative</td>\n",
       "      <td>too hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3194</th>\n",
       "      <td>_fan76 ROFL!  THAT`S my problem, only two hands</td>\n",
       "      <td>negative</td>\n",
       "      <td>that`s my problem, only two hands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>I meant looking like a tiger - stupid predictive text</td>\n",
       "      <td>negative</td>\n",
       "      <td>stupid predictive text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>Sorry - I have failed to grasp your meaning</td>\n",
       "      <td>negative</td>\n",
       "      <td>sorry - i have failed to grasp your meaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>_tha lol  congrats!</td>\n",
       "      <td>positive</td>\n",
       "      <td>congrats!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>oh no my tweetdeck is malfunctioning :O il have web will its fixd</td>\n",
       "      <td>negative</td>\n",
       "      <td>malfunctioning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>now i`m done! good nite</td>\n",
       "      <td>positive</td>\n",
       "      <td>good nite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>Finished watching Episode 4&amp;5 of Ruby Metaprogramming Screencasts. Lots of cool tips, great! But need to more coding practices to master.</td>\n",
       "      <td>positive</td>\n",
       "      <td>cool tips, great! but need to more coding practices to master.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>is having trouble breathing through the pain..and now i have a **** fever...   (^~^)&lt;^&gt;</td>\n",
       "      <td>negative</td>\n",
       "      <td>trouble breathing through the pain..and now i have a **** fever... (^~^)&lt;^&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3443</th>\n",
       "      <td>excited about this week! trying to have a productive monday. trying. hard.</td>\n",
       "      <td>positive</td>\n",
       "      <td>excited about this week! trying to have a productive monday. trying. hard.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>i love you so much tay (: youre so amazing &lt;3 you should come to denmark, we love you here</td>\n",
       "      <td>positive</td>\n",
       "      <td>i love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>:  nice to see you on twitter!</td>\n",
       "      <td>positive</td>\n",
       "      <td>nice to see you on twitter!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Cool. That`d be fantastic!</td>\n",
       "      <td>positive</td>\n",
       "      <td>cool. that`d be fantastic!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>Yes,... I blame you all</td>\n",
       "      <td>negative</td>\n",
       "      <td>blame you all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>Ok I`m frustrated, there is hella dust between the screens of my blackberry</td>\n",
       "      <td>negative</td>\n",
       "      <td>frustrated, there is hella dust between the screens of my blackberry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "1533                                                                                                                         pansy  wtf codeh?!   \n",
       "3095                                                                         grr @ naplan. just finished my commerce & geo exams  good luck (Y)   \n",
       "3514                                                                                                I just noticed that  They are so ridiculous   \n",
       "1113                                                                               One of the most random phone calls ever. My god kill me now.   \n",
       "2360                ...oh. hahahahahaha but no, seriously, i tell the wait staff they look great all the time, its the easiest nice thing to do   \n",
       "1942                                                                                                           Hey, sorry I got off last night!   \n",
       "3475                                                                                                            My room is too hot to sleep in.   \n",
       "3522                                                                                    no school today, teacher cancelled the lesson  chillin`   \n",
       "2481                                         its weird how thinking about may 2nd totally changed my crappy mood to happiness...i miss  so much   \n",
       "2642                                                                                                                I`m all scared and bruised.   \n",
       "431                                                                               Jesus Christ meadowhall could do with better air con! Too hot   \n",
       "3194                                                                                            _fan76 ROFL!  THAT`S my problem, only two hands   \n",
       "2319                                                                                      I meant looking like a tiger - stupid predictive text   \n",
       "554                                                                                                 Sorry - I have failed to grasp your meaning   \n",
       "3072                                                                                                                        _tha lol  congrats!   \n",
       "733                                                                           oh no my tweetdeck is malfunctioning :O il have web will its fixd   \n",
       "2842                                                                                                                    now i`m done! good nite   \n",
       "1632  Finished watching Episode 4&5 of Ruby Metaprogramming Screencasts. Lots of cool tips, great! But need to more coding practices to master.   \n",
       "2154                                                    is having trouble breathing through the pain..and now i have a **** fever...   (^~^)<^>   \n",
       "3443                                                                 excited about this week! trying to have a productive monday. trying. hard.   \n",
       "2245                                                 i love you so much tay (: youre so amazing <3 you should come to denmark, we love you here   \n",
       "2564                                                                                                             :  nice to see you on twitter!   \n",
       "860                                                                                                                  Cool. That`d be fantastic!   \n",
       "2092                                                                                                                    Yes,... I blame you all   \n",
       "329                                                                 Ok I`m frustrated, there is hella dust between the screens of my blackberry   \n",
       "\n",
       "     sentiment  \\\n",
       "1533  negative   \n",
       "3095  positive   \n",
       "3514  negative   \n",
       "1113  negative   \n",
       "2360  positive   \n",
       "1942  negative   \n",
       "3475  negative   \n",
       "3522  positive   \n",
       "2481  positive   \n",
       "2642  negative   \n",
       "431   negative   \n",
       "3194  negative   \n",
       "2319  negative   \n",
       "554   negative   \n",
       "3072  positive   \n",
       "733   negative   \n",
       "2842  positive   \n",
       "1632  positive   \n",
       "2154  negative   \n",
       "3443  positive   \n",
       "2245  positive   \n",
       "2564  positive   \n",
       "860   positive   \n",
       "2092  negative   \n",
       "329   negative   \n",
       "\n",
       "                                                                                                                    selected_text  \n",
       "1533                                                                                                                          wtf  \n",
       "3095                                                                                                                good luck (y)  \n",
       "3514                                                                                                                   ridiculous  \n",
       "1113                                                                                                             god kill me now.  \n",
       "2360  ...oh. hahahahahaha but no, seriously, i tell the wait staff they look great all the time, its the easiest nice thing to do  \n",
       "1942                                                                                                  sorry i got off last night!  \n",
       "3475                                                                                                         too hot to sleep in.  \n",
       "3522                                                                                                                     chillin`  \n",
       "2481                                    its weird how thinking about may 2nd totally changed my crappy mood to happiness...i miss  \n",
       "2642                                                                                                          scared and bruised.  \n",
       "431                                                                                                                       too hot  \n",
       "3194                                                                                            that`s my problem, only two hands  \n",
       "2319                                                                                                       stupid predictive text  \n",
       "554                                                                                   sorry - i have failed to grasp your meaning  \n",
       "3072                                                                                                                    congrats!  \n",
       "733                                                                                                                malfunctioning  \n",
       "2842                                                                                                                    good nite  \n",
       "1632                                                               cool tips, great! but need to more coding practices to master.  \n",
       "2154                                                  trouble breathing through the pain..and now i have a **** fever... (^~^)<^>  \n",
       "3443                                                   excited about this week! trying to have a productive monday. trying. hard.  \n",
       "2245                                                                                                                       i love  \n",
       "2564                                                                                                  nice to see you on twitter!  \n",
       "860                                                                                                    cool. that`d be fantastic!  \n",
       "2092                                                                                                                blame you all  \n",
       "329                                                          frustrated, there is hella dust between the screens of my blackberry  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_span.loc[test_df_span.sentiment!=\"neutral\"][[\"text\", \"sentiment\",\"selected_text\"]].sample(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
