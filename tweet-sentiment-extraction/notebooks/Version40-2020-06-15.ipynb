{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V40\" # OnlySpanModelKF1 2X2Tasks LabelSmoothed with NeutralSamples and Better Preprocessing\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 16\n",
    "PREDICT_BATCH_SIZE = 512\n",
    "DROPOUT = 0.3\n",
    "LABEL_SMOOTHING_PARAM = 0.2\n",
    "\n",
    "RUN_ON_SAMPLE = False\n",
    "EXCLUDE_NEUTRAL_CLASS = True\n",
    "NUM_EPOCHS = [10, 10, 3]\n",
    "NUM_FOLDS = 3\n",
    "#LRs = [5e-3, 1e-4, 1e-6]\n",
    "MAX_LR = 5e-3 #5e-4 #5e-3 #3e-5\n",
    "MID_LR = 5e-4 #5e-5 #1e-4 #3e-5\n",
    "MIN_LR = 5e-5 #5e-6 #1e-6 #3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"../results/\"\n",
    "DATA_DIR = \"../data/\"\n",
    "MODEL_DIR = \"../data/models/roberta-base/\"\n",
    "EXT_MODEL_DIR = \"../data/models/roberta-tokenizer/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, Layer, LeakyReLU, Subtract\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    a = set(str1)\n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeded_value = 12345\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(seeded_value)\n",
    "tf.random.set_seed(seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 15 21:37:46 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0', '2.8.0', '0.5.2']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "if not os.path.exists(MODEL_DIR+\"tokenizers\"):\n",
    "    os.mkdir(MODEL_DIR+\"tokenizers\")\n",
    "\n",
    "if not os.path.exists(MODEL_DIR+\"tokenizers/roberta_tokenizer\"):\n",
    "    os.mkdir(MODEL_DIR+\"tokenizers/roberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\"  target=\"_blank\"><h2 id=\"limiting_gpu_memory_growth\" data-text=\"Limiting GPU memory growth\" tabindex=\"0\">Limiting GPU memory growth</h2></a>\n",
    "<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\"><code translate=\"no\" dir=\"ltr\">CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the <code translate=\"no\" dir=\"ltr\">tf.config.experimental.set_visible_devices</code> method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file=EXT_MODEL_DIR+'/vocab.json',\n",
    "                                             merges_file=EXT_MODEL_DIR+'/merges.txt',                                         \n",
    "                                             add_prefix_space=True,\n",
    "                                             lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(EXT_MODEL_DIR+\"/special_tokens_map.json\") as f:\n",
    "    special_tokens = json.load(f)\n",
    "\n",
    "tokenizer.add_special_tokens([i for i in special_tokens.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.get_vocab_size(); VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  0\n",
      "selected_text  object  0\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27481, 'selected_text': 22464, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count        27481   \n",
      "unique       27481   \n",
      "top     4c43e57a4e   \n",
      "freq             1   \n",
      "\n",
      "                                                                                                                          text  \\\n",
      "count                                                                                                                    27481   \n",
      "unique                                                                                                                   27481   \n",
      "top     ? Polly Scattergood`s new single out today! Download the Please Don`t Touch EP on iTunes, it`s ace!  http://tr.im/kpK8   \n",
      "freq                                                                                                                         1   \n",
      "\n",
      "       selected_text sentiment  \n",
      "count          27481     27481  \n",
      "unique         22464         3  \n",
      "top             good   neutral  \n",
      "freq             199     11118  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\").fillna('')\n",
    "\n",
    "print(pd.concat((df_span.dtypes, df_span.isna().sum()), axis=1))\n",
    "print(df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df_span[i].nunique() for i in df_span.columns})\n",
    "print(df_span.describe())\n",
    "df_span.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count         3534   \n",
      "unique        3534   \n",
      "top     94977637ab   \n",
      "freq             1   \n",
      "\n",
      "                                                                             text  \\\n",
      "count                                                                        3534   \n",
      "unique                                                                       3534   \n",
      "top      I`ll be one of your 'groovy guys' any time, shortie short  #photofollows   \n",
      "freq                                                                            1   \n",
      "\n",
      "       sentiment  \n",
      "count       3534  \n",
      "unique         3  \n",
      "top      neutral  \n",
      "freq        1430  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_span = pd.read_csv(DATA_DIR+\"test.csv\", encoding=\"utf8\").fillna('')\n",
    "print(pd.concat((test_df_span.dtypes, test_df_span.isna().sum()), axis=1))\n",
    "print(test_df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df_span[i].nunique() for i in test_df_span.columns})\n",
    "print(test_df_span.describe())\n",
    "test_df_span.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text\"] = df_span[\"text\"].astype(str)\n",
    "df_span[\"selected_text\"] = df_span[\"selected_text\"].astype(str)\n",
    "test_df_span[\"text\"] = test_df_span[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_addspace(text:str) -> str:\n",
    "    text = text.lower()\n",
    "    text = \" \" + text.strip(\" \") + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(text:str, selected_text:str) -> (str, str, int, int):\n",
    "    \n",
    "    text, selected_text = text.lower(), selected_text.lower()\n",
    "    \n",
    "    text = trim_addspace(text)\n",
    "    \n",
    "    substring_ = re.findall(pattern=\"\\\\s[^\\s]*?\"+re.escape(selected_text)+\"[^\\s]*?\\\\s\", string=text)[0]\n",
    "    \n",
    "    return pd.Series([text, \" \"+substring_.strip(\" \"), text.find(substring_), len(substring_) + text.find(substring_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[[\"text_mod\", \"selected_text_mod\", \"start\", \"stop\"]] = df_span[['text','selected_text']].apply(lambda x: find_indices(x.text, x.selected_text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'textID': '4eac33d1c0',\n",
       " 'text': ' wish we could come see u on Denver  husband lost his job and can`t afford it',\n",
       " 'selected_text': 'd lost',\n",
       " 'sentiment': 'negative',\n",
       " 'text_mod': ' wish we could come see u on denver  husband lost his job and can`t afford it ',\n",
       " 'selected_text_mod': ' husband lost',\n",
       " 'start': 36,\n",
       " 'stop': 50}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span.iloc[27476].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span['text_mod'] = test_df_span['text'].apply(trim_addspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'textID': {12154: 'adfbcc6806'},\n",
       " 'text': {12154: 'i wanna see `up` tonight, but no one will go with me. whhhyyy'},\n",
       " 'selected_text': {12154: 'but no one will go with me.'},\n",
       " 'sentiment': {12154: 'negative'},\n",
       " 'text_mod': {12154: ' i wanna see `up` tonight, but no one will go with me. whhhyyy '},\n",
       " 'selected_text_mod': {12154: ' but no one will go with me.'},\n",
       " 'start': {12154: 26},\n",
       " 'stop': {12154: 55}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span.loc[df_span.text_mod.str.contains(\"tonight, but no one will go\")].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning for span detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neutral': [7974], 'negative': [2430], 'positive': [1313]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{t:tokenizer.encode(\" \"+t).ids for t in df_span.sentiment.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text_mod\"] = \"<s>\" + df_span['text_mod'] + \"</s></s> \" + df_span.sentiment + \" </s>\"\n",
    "test_df_span[\"text_mod\"] = \"<s>\" + test_df_span['text_mod'] + \"</s></s> \" + test_df_span.sentiment + \" </s>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclusions for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCLUDE_NEUTRAL_CLASS: (16363, 8)\n"
     ]
    }
   ],
   "source": [
    "if EXCLUDE_NEUTRAL_CLASS:\n",
    "    df_span = df_span.loc[df_span.sentiment!=\"neutral\"].copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"EXCLUDE_NEUTRAL_CLASS:\", df_span.shape)\n",
    "\n",
    "if RUN_ON_SAMPLE:\n",
    "    df_span = df_span.sample(2000).copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"Train RUN_ON_SAMPLE\", df_span.shape)\n",
    "    test_df_span = test_df_span.sample(2000).copy()\n",
    "    test_df_span = test_df_span.reset_index(drop=True)\n",
    "    print(\"Test  RUN_ON_SAMPLE\", test_df_span.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_tokens = tokenizer.encode_batch(df_span.text_mod.tolist())\n",
    "Y_span_tokens = tokenizer.encode_batch(df_span.selected_text_mod.tolist())\n",
    "X_span_tokens_test = tokenizer.encode_batch(test_df_span.text_mod.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = [i.ids for i in X_span_tokens]\n",
    "Y_span = [i.ids for i in Y_span_tokens]\n",
    "X_span_test = [i.ids for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_att = [i.attention_mask for i in X_span_tokens]\n",
    "Y_span_att = [i.attention_mask for i in Y_span_tokens] # Useless\n",
    "X_span_att_test = [i.attention_mask for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN_SPAN = max([len(i) for i in X_span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremities(l_string, s_string, print_it=False):\n",
    "    len_l = len(l_string)\n",
    "    len_s = len(s_string)\n",
    "    \n",
    "    for i in range(len_l - len_s + 1):\n",
    "        if (i + len_s) <= len_l:\n",
    "            substring = l_string[i:i+len_s]\n",
    "            if substring == s_string:\n",
    "                if print_it:\n",
    "                    print(l_string)\n",
    "                    print(substring)\n",
    "                    print(i, i+len_s, substring)\n",
    "                \n",
    "                start_vector, end_vector = [0] * len_l, [0] * len_l\n",
    "                start_vector[i], end_vector[i+len_s-1] = 1, 1\n",
    "                \n",
    "                return (start_vector, end_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16363 \t: #Processed\n",
      "2 \t: # of Anomalies\n"
     ]
    }
   ],
   "source": [
    "Y_span_starts, Y_span_stops = [], []\n",
    "anomaly_idx, counter = [], 0\n",
    "for num, (i,j) in enumerate(zip(X_span_tokens, Y_span_tokens)):\n",
    "    x,y = i.ids, j.ids\n",
    "    try:\n",
    "        s,e = get_extremities(x, y)\n",
    "        Y_span_starts.append(s)\n",
    "        Y_span_stops.append(e)\n",
    "    except TypeError as t:\n",
    "        counter += 1\n",
    "        anomaly_idx.append(num)\n",
    "        Y_span_starts.append([0]*15)\n",
    "        Y_span_stops.append([0]*15)\n",
    "print(num + 1, \"\\t: #Processed\")\n",
    "\n",
    "print(counter,\"\\t: # of Anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " If you`re flying to NZ www.hot.co.nz is a great way to get results in aggregate.\n",
      "----------\n",
      "great\n",
      "----------\n",
      "[['<s>', 0, 0, 0], ['Ġif', 114, 0, 0], ['Ġyou', 47, 0, 0], ['`', 12905, 0, 0], ['re', 241, 0, 0], ['Ġflying', 4731, 0, 0], ['Ġto', 7, 0, 0], ['Ġn', 295, 0, 0], ['z', 329, 0, 0], ['Ġwww', 1662, 0, 0], ['.', 4, 0, 0], ['hot', 10120, 0, 0], ['.', 4, 0, 0], ['co', 876, 0, 0], ['.', 4, 0, 0], ['nz', 22973, 0, 0], ['Ġis', 16, 0, 0], ['Ġa', 10, 0, 0], ['Ġgreat', 372, 1, 1], ['Ġway', 169, 0, 0], ['Ġto', 7, 0, 0], ['Ġget', 120, 0, 0], ['Ġresults', 775, 0, 0], ['Ġin', 11, 0, 0], ['Ġaggregate', 13884, 0, 0], ['.', 4, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0], ['</s>', 2, 0, 0], ['Ġpositive', 1313, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0]]\n",
      "----------\n",
      "[[372, 'Ġgreat']]\n"
     ]
    }
   ],
   "source": [
    "check_idx = 1333\n",
    "print(df_span.text[check_idx])\n",
    "print(\"----------\")\n",
    "print(df_span.selected_text[check_idx])\n",
    "print(\"----------\")\n",
    "print([[i,j,k,l] for i,j,k,l in zip(X_span_tokens[check_idx].tokens,\n",
    "                                    X_span_tokens[check_idx].ids,\n",
    "                                    Y_span_starts[check_idx],\n",
    "                                    Y_span_stops[check_idx])])\n",
    "print(\"----------\")\n",
    "print([[i,j] for i,j in zip(Y_span_tokens[check_idx].ids,\n",
    "                            Y_span_tokens[check_idx].tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "\n",
    "Y_span_starts = pad_sequences(Y_span_starts, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")#.argmax(axis=1)\n",
    "Y_span_stops = pad_sequences(Y_span_stops, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")#.argmax(axis=1)\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAX_SEQ_LEN_SPAN': 77,\n",
      " 'VOCAB_SIZE': 50265,\n",
      " 'X_span': (16363, 77),\n",
      " 'X_span_att': (16363, 77),\n",
      " 'X_span_att_test': (3534, 77),\n",
      " 'X_span_test': (3534, 77),\n",
      " 'Y_span': (16363, 77),\n",
      " 'Y_span_starts': (16363, 77),\n",
      " 'Y_span_stops': (16363, 77)}\n"
     ]
    }
   ],
   "source": [
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \"Y_span_starts\" : Y_span_starts.shape,\n",
    "    \"Y_span_stops\" : Y_span_stops.shape,\n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "    \"VOCAB_SIZE\":VOCAB_SIZE,\n",
    "    \"MAX_SEQ_LEN_SPAN\":MAX_SEQ_LEN_SPAN\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_flag = np.isin(Y_span_stops.argmax(axis=1),\n",
    "                    np.unique(Y_span_stops.argmax(axis=1),\n",
    "                              return_counts=True)[0][np.unique(Y_span_stops.argmax(axis=1),\n",
    "                                                               return_counts=True)[1]>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16359, 16363, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(keep_flag), df_span.shape[0], df_span.shape[0] - sum(keep_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (16363, 77) \t: X  \n",
      " (16363, 77) \t: X_att  \n",
      " (16363, 77) \t: Y  \n",
      " (16363, 77) \t: Y_starts  \n",
      " (16363, 77) \t: Y_stops  \n",
      " (3534, 77) \t: X_test  \n",
      " (3534, 77) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = X_span[keep_flag]\n",
    "X_span_att = X_span_att[keep_flag]\n",
    "Y_span = Y_span[keep_flag]\n",
    "Y_span_starts = Y_span_starts[keep_flag]\n",
    "Y_span_stops = Y_span_stops[keep_flag]\n",
    "X_span_test = X_span_test\n",
    "X_span_att_test = X_span_att_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (16359, 77) \t: X  \n",
      " (16359, 77) \t: X_att  \n",
      " (16359, 77) \t: Y  \n",
      " (16359, 77) \t: Y_starts  \n",
      " (16359, 77) \t: Y_stops  \n",
      " (3534, 77) \t: X_test  \n",
      " (3534, 77) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_span_words = [tokenizer.decode(i) for i in Y_span]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 77)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LEN =  MAX_SEQ_LEN_SPAN\n",
    "MAX_SEQ_LEN, MAX_SEQ_LEN_SPAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X_span': (16359, 77),\n",
      " 'X_span_att': (16359, 77),\n",
      " 'X_span_att_test': (3534, 77),\n",
      " 'X_span_test': (3534, 77),\n",
      " 'Y_span': (16359, 77)}\n"
     ]
    }
   ],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span_starts = pad_sequences(Y_span_starts, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span_stops = pad_sequences(Y_span_stops, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_sequences = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"words\")\n",
    "    input_att_flags = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"att_flags\")\n",
    "    input_token_ids = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"token_ids\")\n",
    "    \n",
    "    config = RobertaConfig.from_pretrained(MODEL_DIR+'config.json')\n",
    "    roberta_model = TFRobertaModel.from_pretrained(MODEL_DIR+'tf_model.h5', config=config)\n",
    "    x = roberta_model(inputs=input_sequences, attention_mask=input_att_flags, token_type_ids=input_token_ids)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(DROPOUT)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    output_starts_0 = tf.keras.layers.Activation('softmax', name=\"starts_0\")(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(DROPOUT)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    output_stops_0 = tf.keras.layers.Activation('softmax', name=\"stops_0\")(x2)\n",
    "    \n",
    "    output_subtract = tf.keras.layers.Subtract()([output_starts_0, output_stops_0])\n",
    "    output_flat = concatenate([output_starts_0, output_stops_0, output_subtract])\n",
    "    output_starts_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"starts_1\")(output_flat)\n",
    "    output_stops_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"stops_1\")(output_flat)\n",
    "    \n",
    "    span_detection_model = Model([input_att_flags, input_sequences, input_token_ids],\n",
    "                                 [output_starts_0, output_stops_0, output_starts_1, output_stops_1])\n",
    "    \n",
    "    return span_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_detection_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 77)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 77)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 77)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 77, 768), (N 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 77, 768)      0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 77, 768)      0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 77, 768)      1180416     dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 77, 768)      1180416     dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 77, 768)      0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 77, 768)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 77, 1)        769         leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 77, 1)        769         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 77)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 77)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "starts_0 (Activation)           (None, 77)           0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "stops_0 (Activation)            (None, 77)           0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 77)           0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 231)          0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "                                                                 subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "starts_1 (Dense)                (None, 77)           17864       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "stops_1 (Dense)                 (None, 77)           17864       concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 127,043,730\n",
      "Trainable params: 127,043,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "span_detection_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span Detection Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_FOLDS):\n",
    "    if os.path.exists(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\"):\n",
    "        os.remove(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(pred_dict):\n",
    "    print(\"[INFO] \",\"=\"*15,\"Validation for FOLD#\", num, \"=\"*15)\n",
    "    funcs = [accuracy_score, f1_score, precision_score, recall_score, confusion_matrix]\n",
    "    for f in funcs:\n",
    "        for data_set in [\"train\",\"valid\"]:\n",
    "            for var in [\"starts\", \"stops\"]:\n",
    "                if f in [accuracy_score]:\n",
    "                    res = f(**pred_dict[data_set][var])\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "                elif f in [confusion_matrix]:\n",
    "                    res = f(**pred_dict[data_set][var], labels=np.arange(MAX_SEQ_LEN))\n",
    "                    np.savetxt(X=res, fmt='%i', delimiter=\",\",\n",
    "                               fname=RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                    print(\"[INFO] \\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__, \"\\t||\", \n",
    "                          RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                else:\n",
    "                    res = f(**pred_dict[data_set][var], average=\"macro\")\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "        print(\"=======================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(string):\n",
    "    string = re.sub(pattern=\" (negative|positive|neutral)[ ]+$\", repl=\"\", string=string)\n",
    "    string = re.sub(pattern=\" +\", repl=\" \", string=string.strip())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><h2 id=\"finetuning\">Fine-tuning</h2></a>\n",
    "<p>Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.</p>\n",
    " <p>This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.</p>\n",
    " <p>It is critical to only do this step <em>after</em> the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.</p>\n",
    " <p>It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>\n",
    "\n",
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><p><strong>Important note about <code>compile()</code> and <code>trainable</code></strong></p></a>\n",
    "<p>Calling <code>compile()</code> on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the <code>trainable</code>\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until <code>compile</code> is called again. Hence, if you change any <code>trainable</code> value, make sure\n",
    " to call <code>compile()</code> again on your\n",
    "model for your changes to be taken into account.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossWeightAdjust(Callback):\n",
    "    def __init__(self, alpha, beta, gamma, delta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "    \n",
    "    # customize your behavior\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        losses = np.array([v for k,v in logs.items() if k in ['val_starts_0_loss', 'val_stops_0_loss', 'val_starts_1_loss', 'val_stops_1_loss']], dtype=np.float64)\n",
    "        \n",
    "        total_loss = np.sum(losses)\n",
    "        \n",
    "        losses = (losses - 0.5*losses.min()) / (losses.max() - 0.5*losses.min())\n",
    "        losses = losses/np.sum(losses)\n",
    "\n",
    "        K.set_value(self.alpha, losses[0])\n",
    "        K.set_value(self.beta, losses[1])\n",
    "        K.set_value(self.gamma, losses[2])\n",
    "        K.set_value(self.delta, losses[3])\n",
    "        \n",
    "        print(\"\\n Loss weights recalibrated to alpha = %s, beta = %s, gamma = %s, delta = %s \" % (np.round(losses[0],2),\n",
    "                                                                                                  np.round(losses[1],2),\n",
    "                                                                                                  np.round(losses[2],2),\n",
    "                                                                                                  np.round(losses[3],2)))\n",
    "        print(\"Total Val Loss\", np.round(total_loss,3))\n",
    "        logger.info(\"Loss weights recalibrated to alpha = %s, beta = %s, gamma = %s, delta = %s \" % (K.get_value(self.alpha),\n",
    "                                                                                                     K.get_value(self.beta),\n",
    "                                                                                                     K.get_value(self.gamma),\n",
    "                                                                                                     K.get_value(self.delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What does the Loss Weight Adjust Callback do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17266986, 0.1622123 , 0.33177851, 0.33333934])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = np.array([2.7892, 2.7021, 4.1144, 4.1274])\n",
    "losses = (losses - 0.5*losses.min()) / (losses.max() - 0.5*losses.min())\n",
    "losses = losses/np.sum(losses)\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=seeded_value)\n",
    "t_index, v_index = train_test_split(np.arange(X_span.shape[0]), shuffle=True, random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 12269 samples, validate on 4090 samples\n",
      "Epoch 1/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 4.8485 - starts_0_loss: 3.0120 - stops_0_loss: 3.0306 - starts_1_loss: 3.2232 - stops_1_loss: 3.3761 - starts_0_accuracy: 0.3619 - stops_0_accuracy: 0.4008 - starts_1_accuracy: 0.3294 - stops_1_accuracy: 0.3149\n",
      " Loss weights recalibrated to alpha = 0.23, beta = 0.26, gamma = 0.26, delta = 0.25 \n",
      "Total Val Loss 11.575\n",
      "12269/12269 [==============================] - 144s 12ms/sample - loss: 4.8479 - starts_0_loss: 3.0120 - stops_0_loss: 3.0305 - starts_1_loss: 3.2227 - stops_1_loss: 3.3753 - starts_0_accuracy: 0.3619 - stops_0_accuracy: 0.4008 - starts_1_accuracy: 0.3295 - stops_1_accuracy: 0.3151 - val_loss: 4.3535 - val_starts_0_loss: 2.7807 - val_stops_0_loss: 2.9384 - val_starts_1_loss: 2.9360 - val_stops_1_loss: 2.9200 - val_starts_0_accuracy: 0.4137 - val_stops_0_accuracy: 0.3902 - val_starts_1_accuracy: 0.3557 - val_stops_1_accuracy: 0.4320\n",
      "Epoch 2/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.8883 - starts_0_loss: 2.9192 - stops_0_loss: 2.7883 - starts_1_loss: 2.8879 - stops_1_loss: 2.9618 - starts_0_accuracy: 0.3820 - stops_0_accuracy: 0.4271 - starts_1_accuracy: 0.3743 - stops_1_accuracy: 0.4412\n",
      " Loss weights recalibrated to alpha = 0.23, beta = 0.22, gamma = 0.27, delta = 0.28 \n",
      "Total Val Loss 10.932\n",
      "12269/12269 [==============================] - 128s 10ms/sample - loss: 2.8881 - starts_0_loss: 2.9192 - stops_0_loss: 2.7882 - starts_1_loss: 2.8876 - stops_1_loss: 2.9614 - starts_0_accuracy: 0.3822 - stops_0_accuracy: 0.4273 - starts_1_accuracy: 0.3744 - stops_1_accuracy: 0.4413 - val_loss: 2.7351 - val_starts_0_loss: 2.6240 - val_stops_0_loss: 2.5722 - val_starts_1_loss: 2.8313 - val_stops_1_loss: 2.9044 - val_starts_0_accuracy: 0.4406 - val_stops_0_accuracy: 0.4460 - val_starts_1_accuracy: 0.3711 - val_stops_1_accuracy: 0.4711\n",
      "Epoch 3/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.8598 - starts_0_loss: 2.9467 - stops_0_loss: 2.8151 - starts_1_loss: 2.8025 - stops_1_loss: 2.8782 - starts_0_accuracy: 0.3916 - stops_0_accuracy: 0.4290 - starts_1_accuracy: 0.3992 - stops_1_accuracy: 0.4510\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.24, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.942\n",
      "12269/12269 [==============================] - 128s 10ms/sample - loss: 2.8598 - starts_0_loss: 2.9468 - stops_0_loss: 2.8154 - starts_1_loss: 2.8023 - stops_1_loss: 2.8780 - starts_0_accuracy: 0.3917 - stops_0_accuracy: 0.4290 - starts_1_accuracy: 0.3992 - stops_1_accuracy: 0.4511 - val_loss: 2.7407 - val_starts_0_loss: 2.6663 - val_stops_0_loss: 2.6942 - val_starts_1_loss: 2.7648 - val_stops_1_loss: 2.8167 - val_starts_0_accuracy: 0.4306 - val_stops_0_accuracy: 0.4433 - val_starts_1_accuracy: 0.4037 - val_stops_1_accuracy: 0.4765\n",
      "Epoch 4/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.8250 - starts_0_loss: 2.9087 - stops_0_loss: 2.8130 - starts_1_loss: 2.7591 - stops_1_loss: 2.8243 - starts_0_accuracy: 0.4035 - stops_0_accuracy: 0.4445 - starts_1_accuracy: 0.4155 - stops_1_accuracy: 0.4677\n",
      " Loss weights recalibrated to alpha = 0.26, beta = 0.23, gamma = 0.25, delta = 0.26 \n",
      "Total Val Loss 10.99\n",
      "12269/12269 [==============================] - 125s 10ms/sample - loss: 2.8249 - starts_0_loss: 2.9087 - stops_0_loss: 2.8129 - starts_1_loss: 2.7591 - stops_1_loss: 2.8241 - starts_0_accuracy: 0.4034 - stops_0_accuracy: 0.4445 - starts_1_accuracy: 0.4154 - stops_1_accuracy: 0.4678 - val_loss: 2.7480 - val_starts_0_loss: 2.8065 - val_stops_0_loss: 2.6501 - val_starts_1_loss: 2.7366 - val_stops_1_loss: 2.7969 - val_starts_0_accuracy: 0.4230 - val_stops_0_accuracy: 0.4364 - val_starts_1_accuracy: 0.4181 - val_stops_1_accuracy: 0.4619\n",
      "Epoch 5/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.8126 - starts_0_loss: 2.9006 - stops_0_loss: 2.8102 - starts_1_loss: 2.7411 - stops_1_loss: 2.7946 - starts_0_accuracy: 0.4158 - stops_0_accuracy: 0.4440 - starts_1_accuracy: 0.4245 - stops_1_accuracy: 0.4763\n",
      " Loss weights recalibrated to alpha = 0.26, beta = 0.23, gamma = 0.25, delta = 0.26 \n",
      "Total Val Loss 10.926\n",
      "12269/12269 [==============================] - 125s 10ms/sample - loss: 2.8126 - starts_0_loss: 2.9011 - stops_0_loss: 2.8099 - starts_1_loss: 2.7413 - stops_1_loss: 2.7944 - starts_0_accuracy: 0.4158 - stops_0_accuracy: 0.4441 - starts_1_accuracy: 0.4244 - stops_1_accuracy: 0.4763 - val_loss: 2.7346 - val_starts_0_loss: 2.8102 - val_stops_0_loss: 2.6072 - val_starts_1_loss: 2.7208 - val_stops_1_loss: 2.7872 - val_starts_0_accuracy: 0.4186 - val_stops_0_accuracy: 0.4467 - val_starts_1_accuracy: 0.4237 - val_stops_1_accuracy: 0.4763\n",
      "Epoch 6/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.7910 - starts_0_loss: 2.8791 - stops_0_loss: 2.7877 - starts_1_loss: 2.7247 - stops_1_loss: 2.7678 - starts_0_accuracy: 0.4215 - stops_0_accuracy: 0.4510 - starts_1_accuracy: 0.4282 - stops_1_accuracy: 0.4818\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.24, gamma = 0.25, delta = 0.27 \n",
      "Total Val Loss 10.969\n",
      "12269/12269 [==============================] - 125s 10ms/sample - loss: 2.7910 - starts_0_loss: 2.8793 - stops_0_loss: 2.7877 - starts_1_loss: 2.7245 - stops_1_loss: 2.7678 - starts_0_accuracy: 0.4216 - stops_0_accuracy: 0.4511 - starts_1_accuracy: 0.4282 - stops_1_accuracy: 0.4819 - val_loss: 2.7438 - val_starts_0_loss: 2.6940 - val_stops_0_loss: 2.6812 - val_starts_1_loss: 2.7301 - val_stops_1_loss: 2.8633 - val_starts_0_accuracy: 0.4235 - val_stops_0_accuracy: 0.4330 - val_starts_1_accuracy: 0.4076 - val_stops_1_accuracy: 0.4628\n",
      "Epoch 7/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.7759 - starts_0_loss: 2.8587 - stops_0_loss: 2.7896 - starts_1_loss: 2.7058 - stops_1_loss: 2.7542 - starts_0_accuracy: 0.4283 - stops_0_accuracy: 0.4504 - starts_1_accuracy: 0.4430 - stops_1_accuracy: 0.4853\n",
      " Loss weights recalibrated to alpha = 0.26, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.756\n",
      "12269/12269 [==============================] - 125s 10ms/sample - loss: 2.7760 - starts_0_loss: 2.8594 - stops_0_loss: 2.7894 - starts_1_loss: 2.7059 - stops_1_loss: 2.7539 - starts_0_accuracy: 0.4282 - stops_0_accuracy: 0.4506 - starts_1_accuracy: 0.4430 - stops_1_accuracy: 0.4855 - val_loss: 2.6908 - val_starts_0_loss: 2.7236 - val_stops_0_loss: 2.5488 - val_starts_1_loss: 2.7583 - val_stops_1_loss: 2.7254 - val_starts_0_accuracy: 0.4134 - val_stops_0_accuracy: 0.4555 - val_starts_1_accuracy: 0.4054 - val_stops_1_accuracy: 0.4868\n",
      "Epoch 8/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.7692 - starts_0_loss: 2.8715 - stops_0_loss: 2.7679 - starts_1_loss: 2.7040 - stops_1_loss: 2.7349 - starts_0_accuracy: 0.4218 - stops_0_accuracy: 0.4555 - starts_1_accuracy: 0.4461 - stops_1_accuracy: 0.4887\n",
      " Loss weights recalibrated to alpha = 0.26, beta = 0.22, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.7\n",
      "12269/12269 [==============================] - 125s 10ms/sample - loss: 2.7690 - starts_0_loss: 2.8715 - stops_0_loss: 2.7675 - starts_1_loss: 2.7038 - stops_1_loss: 2.7347 - starts_0_accuracy: 0.4218 - stops_0_accuracy: 0.4555 - starts_1_accuracy: 0.4461 - stops_1_accuracy: 0.4888 - val_loss: 2.6793 - val_starts_0_loss: 2.7123 - val_stops_0_loss: 2.5329 - val_starts_1_loss: 2.7333 - val_stops_1_loss: 2.7210 - val_starts_0_accuracy: 0.4098 - val_stops_0_accuracy: 0.4611 - val_starts_1_accuracy: 0.4127 - val_stops_1_accuracy: 0.4990\n",
      "Epoch 9/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.7679 - starts_0_loss: 2.8718 - stops_0_loss: 2.7632 - starts_1_loss: 2.7027 - stops_1_loss: 2.7345 - starts_0_accuracy: 0.4279 - stops_0_accuracy: 0.4550 - starts_1_accuracy: 0.4417 - stops_1_accuracy: 0.4925\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.782\n",
      "12269/12269 [==============================] - 126s 10ms/sample - loss: 2.7680 - starts_0_loss: 2.8722 - stops_0_loss: 2.7631 - starts_1_loss: 2.7029 - stops_1_loss: 2.7344 - starts_0_accuracy: 0.4277 - stops_0_accuracy: 0.4550 - starts_1_accuracy: 0.4415 - stops_1_accuracy: 0.4925 - val_loss: 2.6986 - val_starts_0_loss: 2.7070 - val_stops_0_loss: 2.5954 - val_starts_1_loss: 2.7326 - val_stops_1_loss: 2.7469 - val_starts_0_accuracy: 0.4262 - val_stops_0_accuracy: 0.4589 - val_starts_1_accuracy: 0.4247 - val_stops_1_accuracy: 0.4870\n",
      "Epoch 10/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.7508 - starts_0_loss: 2.8429 - stops_0_loss: 2.7629 - starts_1_loss: 2.6771 - stops_1_loss: 2.7235 - starts_0_accuracy: 0.4336 - stops_0_accuracy: 0.4578 - starts_1_accuracy: 0.4554 - stops_1_accuracy: 0.4947\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.22, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.697\n",
      "12269/12269 [==============================] - 125s 10ms/sample - loss: 2.7507 - starts_0_loss: 2.8434 - stops_0_loss: 2.7623 - starts_1_loss: 2.6772 - stops_1_loss: 2.7231 - starts_0_accuracy: 0.4335 - stops_0_accuracy: 0.4581 - starts_1_accuracy: 0.4553 - stops_1_accuracy: 0.4948 - val_loss: 2.6774 - val_starts_0_loss: 2.7018 - val_stops_0_loss: 2.5321 - val_starts_1_loss: 2.7363 - val_stops_1_loss: 2.7271 - val_starts_0_accuracy: 0.4130 - val_stops_0_accuracy: 0.4567 - val_starts_1_accuracy: 0.4117 - val_stops_1_accuracy: 0.4897\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 12269 samples, validate on 4090 samples\n",
      "Epoch 1/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.6316 - starts_0_loss: 2.6638 - stops_0_loss: 2.5639 - starts_1_loss: 2.6465 - stops_1_loss: 2.6434 - starts_0_accuracy: 0.4405 - stops_0_accuracy: 0.4603 - starts_1_accuracy: 0.4456 - stops_1_accuracy: 0.5108\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.181\n",
      "12269/12269 [==============================] - 143s 12ms/sample - loss: 2.6315 - starts_0_loss: 2.6636 - stops_0_loss: 2.5639 - starts_1_loss: 2.6464 - stops_1_loss: 2.6435 - starts_0_accuracy: 0.4407 - stops_0_accuracy: 0.4602 - starts_1_accuracy: 0.4456 - stops_1_accuracy: 0.5108 - val_loss: 2.5487 - val_starts_0_loss: 2.5185 - val_stops_0_loss: 2.4535 - val_starts_1_loss: 2.6211 - val_stops_1_loss: 2.5876 - val_starts_0_accuracy: 0.4733 - val_stops_0_accuracy: 0.4758 - val_starts_1_accuracy: 0.4548 - val_stops_1_accuracy: 0.5232\n",
      "Epoch 2/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.6054 - starts_0_loss: 2.6372 - stops_0_loss: 2.5378 - starts_1_loss: 2.6284 - stops_1_loss: 2.6127 - starts_0_accuracy: 0.4446 - stops_0_accuracy: 0.4661 - starts_1_accuracy: 0.4488 - stops_1_accuracy: 0.5214\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.184\n",
      "12269/12269 [==============================] - 125s 10ms/sample - loss: 2.6055 - starts_0_loss: 2.6370 - stops_0_loss: 2.5380 - starts_1_loss: 2.6284 - stops_1_loss: 2.6130 - starts_0_accuracy: 0.4446 - stops_0_accuracy: 0.4659 - starts_1_accuracy: 0.4488 - stops_1_accuracy: 0.5214 - val_loss: 2.5491 - val_starts_0_loss: 2.5243 - val_stops_0_loss: 2.4613 - val_starts_1_loss: 2.6192 - val_stops_1_loss: 2.5796 - val_starts_0_accuracy: 0.4707 - val_stops_0_accuracy: 0.4697 - val_starts_1_accuracy: 0.4548 - val_stops_1_accuracy: 0.5205\n",
      "Epoch 3/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5933 - starts_0_loss: 2.6339 - stops_0_loss: 2.5245 - starts_1_loss: 2.6180 - stops_1_loss: 2.5917 - starts_0_accuracy: 0.4500 - stops_0_accuracy: 0.4710 - starts_1_accuracy: 0.4542 - stops_1_accuracy: 0.5295\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.162\n",
      "12269/12269 [==============================] - 125s 10ms/sample - loss: 2.5933 - starts_0_loss: 2.6339 - stops_0_loss: 2.5244 - starts_1_loss: 2.6181 - stops_1_loss: 2.5916 - starts_0_accuracy: 0.4498 - stops_0_accuracy: 0.4710 - starts_1_accuracy: 0.4542 - stops_1_accuracy: 0.5296 - val_loss: 2.5433 - val_starts_0_loss: 2.5301 - val_stops_0_loss: 2.4493 - val_starts_1_loss: 2.6126 - val_stops_1_loss: 2.5700 - val_starts_0_accuracy: 0.4736 - val_stops_0_accuracy: 0.4778 - val_starts_1_accuracy: 0.4538 - val_stops_1_accuracy: 0.5281\n",
      "Epoch 4/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5911 - starts_0_loss: 2.6205 - stops_0_loss: 2.5291 - starts_1_loss: 2.6115 - stops_1_loss: 2.5978 - starts_0_accuracy: 0.4537 - stops_0_accuracy: 0.4640 - starts_1_accuracy: 0.4556 - stops_1_accuracy: 0.5241\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.27, delta = 0.25 \n",
      "Total Val Loss 10.141\n",
      "12269/12269 [==============================] - 126s 10ms/sample - loss: 2.5911 - starts_0_loss: 2.6206 - stops_0_loss: 2.5291 - starts_1_loss: 2.6115 - stops_1_loss: 2.5979 - starts_0_accuracy: 0.4536 - stops_0_accuracy: 0.4639 - starts_1_accuracy: 0.4555 - stops_1_accuracy: 0.5241 - val_loss: 2.5381 - val_starts_0_loss: 2.5143 - val_stops_0_loss: 2.4498 - val_starts_1_loss: 2.6157 - val_stops_1_loss: 2.5611 - val_starts_0_accuracy: 0.4721 - val_stops_0_accuracy: 0.4682 - val_starts_1_accuracy: 0.4531 - val_stops_1_accuracy: 0.5269\n",
      "Epoch 5/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5825 - starts_0_loss: 2.6225 - stops_0_loss: 2.5149 - starts_1_loss: 2.6069 - stops_1_loss: 2.5806 - starts_0_accuracy: 0.4523 - stops_0_accuracy: 0.4686 - starts_1_accuracy: 0.4601 - stops_1_accuracy: 0.5241\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.127\n",
      "12269/12269 [==============================] - 125s 10ms/sample - loss: 2.5826 - starts_0_loss: 2.6227 - stops_0_loss: 2.5149 - starts_1_loss: 2.6070 - stops_1_loss: 2.5807 - starts_0_accuracy: 0.4522 - stops_0_accuracy: 0.4686 - starts_1_accuracy: 0.4600 - stops_1_accuracy: 0.5241 - val_loss: 2.5347 - val_starts_0_loss: 2.5187 - val_stops_0_loss: 2.4405 - val_starts_1_loss: 2.6060 - val_stops_1_loss: 2.5622 - val_starts_0_accuracy: 0.4721 - val_stops_0_accuracy: 0.4785 - val_starts_1_accuracy: 0.4557 - val_stops_1_accuracy: 0.5286\n",
      "Epoch 6/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5715 - starts_0_loss: 2.6136 - stops_0_loss: 2.5057 - starts_1_loss: 2.5974 - stops_1_loss: 2.5640 - starts_0_accuracy: 0.4572 - stops_0_accuracy: 0.4783 - starts_1_accuracy: 0.4643 - stops_1_accuracy: 0.5375\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "Total Val Loss 10.117\n",
      "12269/12269 [==============================] - 126s 10ms/sample - loss: 2.5715 - starts_0_loss: 2.6136 - stops_0_loss: 2.5055 - starts_1_loss: 2.5974 - stops_1_loss: 2.5638 - starts_0_accuracy: 0.4572 - stops_0_accuracy: 0.4782 - starts_1_accuracy: 0.4641 - stops_1_accuracy: 0.5375 - val_loss: 2.5321 - val_starts_0_loss: 2.5153 - val_stops_0_loss: 2.4425 - val_starts_1_loss: 2.6026 - val_stops_1_loss: 2.5565 - val_starts_0_accuracy: 0.4765 - val_stops_0_accuracy: 0.4746 - val_starts_1_accuracy: 0.4575 - val_stops_1_accuracy: 0.5269\n",
      "Epoch 7/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5688 - starts_0_loss: 2.6067 - stops_0_loss: 2.5075 - starts_1_loss: 2.5913 - stops_1_loss: 2.5648 - starts_0_accuracy: 0.4595 - stops_0_accuracy: 0.4727 - starts_1_accuracy: 0.4660 - stops_1_accuracy: 0.5332\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.25 \n",
      "Total Val Loss 10.12\n",
      "12269/12269 [==============================] - 126s 10ms/sample - loss: 2.5687 - starts_0_loss: 2.6068 - stops_0_loss: 2.5074 - starts_1_loss: 2.5912 - stops_1_loss: 2.5648 - starts_0_accuracy: 0.4595 - stops_0_accuracy: 0.4727 - starts_1_accuracy: 0.4661 - stops_1_accuracy: 0.5332 - val_loss: 2.5326 - val_starts_0_loss: 2.5168 - val_stops_0_loss: 2.4469 - val_starts_1_loss: 2.6008 - val_stops_1_loss: 2.5554 - val_starts_0_accuracy: 0.4743 - val_stops_0_accuracy: 0.4756 - val_starts_1_accuracy: 0.4614 - val_stops_1_accuracy: 0.5306\n",
      "Epoch 8/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5658 - starts_0_loss: 2.6072 - stops_0_loss: 2.5025 - starts_1_loss: 2.5917 - stops_1_loss: 2.5571 - starts_0_accuracy: 0.4542 - stops_0_accuracy: 0.4737 - starts_1_accuracy: 0.4653 - stops_1_accuracy: 0.5355\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.25 \n",
      "Total Val Loss 10.112\n",
      "12269/12269 [==============================] - 177s 14ms/sample - loss: 2.5657 - starts_0_loss: 2.6070 - stops_0_loss: 2.5024 - starts_1_loss: 2.5915 - stops_1_loss: 2.5570 - starts_0_accuracy: 0.4542 - stops_0_accuracy: 0.4738 - starts_1_accuracy: 0.4654 - stops_1_accuracy: 0.5355 - val_loss: 2.5302 - val_starts_0_loss: 2.5201 - val_stops_0_loss: 2.4464 - val_starts_1_loss: 2.5943 - val_stops_1_loss: 2.5507 - val_starts_0_accuracy: 0.4707 - val_stops_0_accuracy: 0.4770 - val_starts_1_accuracy: 0.4587 - val_stops_1_accuracy: 0.5313\n",
      "Epoch 9/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5618 - starts_0_loss: 2.6041 - stops_0_loss: 2.5030 - starts_1_loss: 2.5847 - stops_1_loss: 2.5511 - starts_0_accuracy: 0.4602 - stops_0_accuracy: 0.4752 - starts_1_accuracy: 0.4688 - stops_1_accuracy: 0.5330\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.24, gamma = 0.26, delta = 0.25 \n",
      "Total Val Loss 10.104\n",
      "12269/12269 [==============================] - 126s 10ms/sample - loss: 2.5617 - starts_0_loss: 2.6040 - stops_0_loss: 2.5029 - starts_1_loss: 2.5845 - stops_1_loss: 2.5510 - starts_0_accuracy: 0.4602 - stops_0_accuracy: 0.4750 - starts_1_accuracy: 0.4687 - stops_1_accuracy: 0.5329 - val_loss: 2.5283 - val_starts_0_loss: 2.5152 - val_stops_0_loss: 2.4483 - val_starts_1_loss: 2.5944 - val_stops_1_loss: 2.5465 - val_starts_0_accuracy: 0.4765 - val_stops_0_accuracy: 0.4787 - val_starts_1_accuracy: 0.4582 - val_stops_1_accuracy: 0.5308\n",
      "Epoch 10/10\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.5570 - starts_0_loss: 2.5939 - stops_0_loss: 2.4996 - starts_1_loss: 2.5785 - stops_1_loss: 2.5520 - starts_0_accuracy: 0.4591 - stops_0_accuracy: 0.4716 - starts_1_accuracy: 0.4691 - stops_1_accuracy: 0.5320\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.25 \n",
      "Total Val Loss 10.083\n",
      "12269/12269 [==============================] - 125s 10ms/sample - loss: 2.5571 - starts_0_loss: 2.5940 - stops_0_loss: 2.4995 - starts_1_loss: 2.5785 - stops_1_loss: 2.5522 - starts_0_accuracy: 0.4590 - stops_0_accuracy: 0.4716 - starts_1_accuracy: 0.4691 - stops_1_accuracy: 0.5318 - val_loss: 2.5230 - val_starts_0_loss: 2.5170 - val_stops_0_loss: 2.4356 - val_starts_1_loss: 2.5869 - val_stops_1_loss: 2.5435 - val_starts_0_accuracy: 0.4763 - val_stops_0_accuracy: 0.4787 - val_starts_1_accuracy: 0.4672 - val_stops_1_accuracy: 0.5303\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 12269 samples, validate on 4090 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.4614 - starts_0_loss: 2.4855 - stops_0_loss: 2.3684 - starts_1_loss: 2.5079 - stops_1_loss: 2.4752 - starts_0_accuracy: 0.4857 - stops_0_accuracy: 0.5202 - starts_1_accuracy: 0.4918 - stops_1_accuracy: 0.5603\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.22, gamma = 0.27, delta = 0.27 \n",
      "Total Val Loss 9.414\n",
      "12269/12269 [==============================] - 290s 24ms/sample - loss: 2.4612 - starts_0_loss: 2.4849 - stops_0_loss: 2.3683 - starts_1_loss: 2.5073 - stops_1_loss: 2.4752 - starts_0_accuracy: 0.4859 - stops_0_accuracy: 0.5201 - starts_1_accuracy: 0.4921 - stops_1_accuracy: 0.5601 - val_loss: 2.3571 - val_starts_0_loss: 2.3172 - val_stops_0_loss: 2.2234 - val_starts_1_loss: 2.4419 - val_stops_1_loss: 2.4312 - val_starts_0_accuracy: 0.5352 - val_stops_0_accuracy: 0.5790 - val_starts_1_accuracy: 0.5301 - val_stops_1_accuracy: 0.5939\n",
      "Epoch 2/3\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 2.6506 - starts_0_loss: 2.7336 - stops_0_loss: 2.6476 - starts_1_loss: 2.5780 - stops_1_loss: 2.6506 - starts_0_accuracy: 0.4356 - stops_0_accuracy: 0.4662 - starts_1_accuracy: 0.5059 - stops_1_accuracy: 0.4959\n",
      " Loss weights recalibrated to alpha = 0.29, beta = 0.29, gamma = 0.18, delta = 0.24 \n",
      "Total Val Loss 15.924\n",
      "12269/12269 [==============================] - 272s 22ms/sample - loss: 2.6519 - starts_0_loss: 2.7357 - stops_0_loss: 2.6499 - starts_1_loss: 2.5787 - stops_1_loss: 2.6521 - starts_0_accuracy: 0.4352 - stops_0_accuracy: 0.4657 - starts_1_accuracy: 0.5057 - stops_1_accuracy: 0.4954 - val_loss: 3.9564 - val_starts_0_loss: 4.3412 - val_stops_0_loss: 4.3422 - val_starts_1_loss: 3.3837 - val_stops_1_loss: 3.8564 - val_starts_0_accuracy: 0.0677 - val_stops_0_accuracy: 0.0572 - val_starts_1_accuracy: 0.3267 - val_stops_1_accuracy: 0.0531\n",
      "Epoch 3/3\n",
      "12256/12269 [============================>.] - ETA: 0s - loss: 4.0565 - starts_0_loss: 4.3501 - stops_0_loss: 4.3472 - starts_1_loss: 3.3894 - stops_1_loss: 3.8625 - starts_0_accuracy: 0.0137 - stops_0_accuracy: 0.0122 - starts_1_accuracy: 0.3283 - stops_1_accuracy: 0.0488\n",
      " Loss weights recalibrated to alpha = 0.29, beta = 0.29, gamma = 0.18, delta = 0.24 \n",
      "Total Val Loss 15.892\n",
      "12269/12269 [==============================] - 271s 22ms/sample - loss: 4.0564 - starts_0_loss: 4.3502 - stops_0_loss: 4.3471 - starts_1_loss: 3.3891 - stops_1_loss: 3.8624 - starts_0_accuracy: 0.0137 - stops_0_accuracy: 0.0123 - starts_1_accuracy: 0.3284 - stops_1_accuracy: 0.0487 - val_loss: 4.0426 - val_starts_0_loss: 4.3388 - val_stops_0_loss: 4.3379 - val_starts_1_loss: 3.3786 - val_stops_1_loss: 3.8371 - val_starts_0_accuracy: 0.0650 - val_stops_0_accuracy: 0.0247 - val_starts_1_accuracy: 0.3267 - val_stops_1_accuracy: 0.0531\n",
      "[INFO]  =============== Validation for FOLD# 0 ===============\n",
      "[INFO] 6.26 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 2.47 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 6.36 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 2.47 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 3.70 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 1.57 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 4.09 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 1.89 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 4.70 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 1.88 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 5.39 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 2.34 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 4.71 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 1.56 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 4.94 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 1.76 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t||"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V40_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V40_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V40_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V40_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (12269, 77) (12269, 77)\n",
      "[INFO] Prediction shape for validation data:  (4090, 77) (4090, 77)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  0 out of 12269\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  0 out of 4090\n",
      "[INFO] Training Jaccard Score:  0.365011407696606\n",
      "[INFO] Validation Jaccard Score:  0.35868533681917325\n",
      "[INFO] Training for fold: 0 finished at Mon Jun 15 22:37:10 2020\n",
      "Mon Jun 15 22:37:10 2020\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "alpha = K.variable(0.25)\n",
    "beta = K.variable(0.25)\n",
    "gamma = K.variable(0.25)\n",
    "delta = K.variable(0.75)\n",
    "\n",
    "span_mcp = ModelCheckpoint(filepath=RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\", monitor='val_loss',\n",
    "                           verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "span_csvl = CSVLogger(filename=RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(num)+\".csv\",\n",
    "                      separator=\",\",\n",
    "                      append=True)\n",
    "\n",
    "print(\"[INFO] Training only the final layers at higher learning rates.\")\n",
    "span_detection_model.layers[3].trainable = False\n",
    "adam = Adam(learning_rate=MAX_LR)\n",
    "span_detection_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=LABEL_SMOOTHING_PARAM),\n",
    "                             optimizer=adam,\n",
    "                             metrics=['accuracy'],\n",
    "                             loss_weights={\"starts_0\":alpha,\"stops_0\":beta,\"starts_1\":gamma,\"stops_1\":delta})\n",
    "np.random.shuffle(t_index)\n",
    "np.random.shuffle(v_index)\n",
    "span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                           \"words\":X_span[t_index],\n",
    "                                           \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                        y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                           \"stops_0\":Y_span_stops[t_index], \n",
    "                                           \"starts_1\":Y_span_starts[t_index],\n",
    "                                           \"stops_1\":Y_span_stops[t_index]},\n",
    "                                        shuffle=False,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        epochs=NUM_EPOCHS[0],\n",
    "                                        validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                          \"words\":X_span[v_index],\n",
    "                                                          \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                         {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                          \"stops_0\":Y_span_stops[v_index], \n",
    "                                                          \"starts_1\":Y_span_starts[v_index],\n",
    "                                                          \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                        verbose=1,\n",
    "                                        callbacks=[span_mcp, span_csvl, LossWeightAdjust(alpha=alpha, beta=beta, gamma=gamma, delta=delta)])\n",
    "\n",
    "print(\"[INFO] Training only the final layers at lower learning rates.\")\n",
    "adam = Adam(learning_rate=MID_LR)\n",
    "span_detection_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=LABEL_SMOOTHING_PARAM),\n",
    "                             optimizer=adam,\n",
    "                             metrics=['accuracy'],\n",
    "                             loss_weights={\"starts_0\":alpha,\"stops_0\":beta,\"starts_1\":gamma,\"stops_1\":delta})\n",
    "np.random.shuffle(t_index)\n",
    "np.random.shuffle(v_index)\n",
    "span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                           \"words\":X_span[t_index],\n",
    "                                           \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                        y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                           \"stops_0\":Y_span_stops[t_index], \n",
    "                                           \"starts_1\":Y_span_starts[t_index],\n",
    "                                           \"stops_1\":Y_span_stops[t_index]},\n",
    "                                        shuffle=False,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        epochs=NUM_EPOCHS[1],\n",
    "                                        validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                          \"words\":X_span[v_index],\n",
    "                                                          \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                         {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                          \"stops_0\":Y_span_stops[v_index], \n",
    "                                                          \"starts_1\":Y_span_starts[v_index],\n",
    "                                                          \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                        verbose=1,\n",
    "                                        callbacks=[span_mcp, span_csvl, LossWeightAdjust(alpha=alpha, beta=beta, gamma=gamma, delta=delta)])\n",
    "\n",
    "print(\"[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\")\n",
    "span_detection_model.layers[3].trainable = True\n",
    "adam = Adam(learning_rate=MIN_LR)\n",
    "span_detection_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=LABEL_SMOOTHING_PARAM),\n",
    "                             optimizer=adam,\n",
    "                             metrics=['accuracy'],\n",
    "                             loss_weights={\"starts_0\":alpha,\"stops_0\":beta,\"starts_1\":gamma,\"stops_1\":delta})\n",
    "np.random.shuffle(t_index)\n",
    "np.random.shuffle(v_index)\n",
    "span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                           \"words\":X_span[t_index],\n",
    "                                           \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                        y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                           \"stops_0\":Y_span_stops[t_index],\n",
    "                                           \"starts_1\":Y_span_starts[t_index],\n",
    "                                           \"stops_1\":Y_span_stops[t_index]},\n",
    "                                        shuffle=False,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        epochs=NUM_EPOCHS[2],\n",
    "                                        validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                          \"words\":X_span[v_index],\n",
    "                                                          \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                         {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                          \"stops_0\":Y_span_stops[v_index], \n",
    "                                                          \"starts_1\":Y_span_starts[v_index],\n",
    "                                                          \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                        verbose=1,\n",
    "                                        callbacks=[span_mcp, span_csvl, LossWeightAdjust(alpha=alpha, beta=beta, gamma=gamma, delta=delta)])\n",
    "\n",
    "# Loading best weights per fold\n",
    "span_detection_model.load_weights(RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\")\n",
    "\n",
    "pred_train = span_detection_model.predict(x = {\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                          batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "pred_val = span_detection_model.predict(x = {\"att_flags\":X_span_att[v_index],\n",
    "                                             \"words\":X_span[v_index],\n",
    "                                             \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                        batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "pred_starts_train, pred_stops_train = (pred_train[0]/2.0 + pred_train[2]/2.0), (pred_train[1]/2.0 + pred_train[3]/2.0)\n",
    "pred_starts_val, pred_stops_val = (pred_val[0]/2.0 + pred_val[2]/2.0), (pred_val[1]/2.0 + pred_val[3]/2.0)\n",
    "\n",
    "# Accumulate test results after training every fold\n",
    "pred_test_fold = span_detection_model.predict(x = {\"att_flags\":X_span_att_test,\n",
    "                                                   \"words\":X_span_test,\n",
    "                                                   \"token_ids\":np.zeros_like(X_span_att_test)},\n",
    "                                              batch_size=PREDICT_BATCH_SIZE)\n",
    "if num==0:\n",
    "    pred_test = []\n",
    "    pred_test.append(pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "    pred_test.append(pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "\n",
    "# Tabulate\n",
    "preds = {\n",
    "    \"train\":{\n",
    "        \"starts\":{\n",
    "            \"y_true\":Y_span_starts[t_index].argmax(axis=1),\n",
    "            \"y_pred\":pred_train[0].argmax(axis=1)\n",
    "        },\n",
    "        \"stops\":{\n",
    "            \"y_true\":Y_span_stops[t_index].argmax(axis=1),\n",
    "            \"y_pred\":pred_train[1].argmax(axis=1)\n",
    "        }\n",
    "    },\n",
    "    \"valid\":{\n",
    "        \"starts\":{\n",
    "            \"y_true\":Y_span_starts[v_index].argmax(axis=1),\n",
    "            \"y_pred\":pred_val[0].argmax(axis=1)\n",
    "        },\n",
    "        \"stops\":{\n",
    "            \"y_true\":Y_span_stops[v_index].argmax(axis=1),\n",
    "            \"y_pred\":pred_val[1].argmax(axis=1)\n",
    "        }        \n",
    "    }\n",
    "}\n",
    "\n",
    "print_metrics(pred_dict=preds)\n",
    "\n",
    "print(\"[INFO] Prediction shape for training data: \", pred_starts_train.shape, pred_stops_train.shape)\n",
    "print(\"[INFO] Prediction shape for validation data: \", pred_starts_val.shape, pred_stops_val.shape)\n",
    "\n",
    "print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for training data: \",\n",
    "      sum([s<e for s,e in zip(pred_starts_train.argmax(axis=1),\n",
    "                              pred_stops_train.argmax(axis=1))]),\n",
    "      \"out of\", pred_starts_train.shape[0])\n",
    "print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for validation data: \",\n",
    "      sum([s<e for s,e in zip(pred_starts_val.argmax(axis=1),\n",
    "                              pred_stops_val.argmax(axis=1))]),\n",
    "      \"out of\", pred_starts_val.shape[0],)\n",
    "\n",
    "pred_words_train = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[t_index],\n",
    "                                                                                                                               pred_starts_train.argmax(axis=1),\n",
    "                                                                                                                               pred_stops_train.argmax(axis=1))]\n",
    "\n",
    "pred_words_val = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[v_index],\n",
    "                                                                                                                             pred_starts_val.argmax(axis=1),\n",
    "                                                                                                                             pred_stops_val.argmax(axis=1))]\n",
    "\n",
    "print(\"[INFO] Training Jaccard Score: \",\n",
    "      np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in t_index],\n",
    "                                                      pred_words_train)]))\n",
    "print(\"[INFO] Validation Jaccard Score: \",\n",
    "      np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in v_index],\n",
    "                                                      pred_words_val)]))\n",
    "print(\"[INFO] Training for fold:\", num, \"finished at\", ctime(time()))\n",
    "\n",
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Prediction shape for testing data:  (3534, 77) (3534, 77)\n"
     ]
    }
   ],
   "source": [
    "pred_starts_test, pred_stops_test = pred_test[0]/NUM_FOLDS, pred_test[1]/NUM_FOLDS\n",
    "print(\"[INFO] Prediction shape for testing data: \", pred_starts_test.shape, pred_stops_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal predictions (StartIndex less than EndIndex) for testing data: 0 out of 3534\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal predictions (StartIndex less than EndIndex) for testing data:\",\n",
    "      sum([s<e for s,e in zip(pred_starts_test.argmax(axis=1),\n",
    "                              pred_stops_test.argmax(axis=1))]), \n",
    "      \"out of\",\n",
    "      pred_starts_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words_test = [\n",
    "    post_process(tokenizer.decode(t[s:e+1])) if s<e else post_process(tokenizer.decode(t[e:])) for t,s,e in zip(X_span_test,\n",
    "                                                                                                              pred_starts_test.argmax(axis=1),\n",
    "                                                                                                              pred_stops_test.argmax(axis=1))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sad i missed  going away party due to too much work, but i really am gonna miss that kid  negative \n",
      "1\n",
      "1\n",
      "sad\n"
     ]
    }
   ],
   "source": [
    "check_idx = 1111\n",
    "#print([[t,i,j,k] for t,i,j,k in zip(tokenizer.decode(),X_test[check_idx],pred_starts_test[check_idx],pred_stops_test[check_idx])])\n",
    "print(tokenizer.decode(X_span_test[check_idx]))\n",
    "print(pred_starts_test.argmax(axis=1)[check_idx])\n",
    "print(pred_stops_test.argmax(axis=1)[check_idx])\n",
    "print(post_process(tokenizer.decode(X_span_test[check_idx][pred_starts_test.argmax(axis=1)[check_idx]:1+pred_stops_test.argmax(axis=1)[check_idx]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span['selected_text'] = pred_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[\"selected_text\"] = np.where(test_df_span[\"sentiment\"] == \"neutral\",\n",
    "                                         test_df_span[\"text\"],\n",
    "                                         test_df_span[\"selected_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[[\"textID\", \"selected_text\"]].to_csv(RESULTS_DIR+\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>Ian and I are going to a matinee tomorrow, or that`s the plan at least...it`s going to be too expensive for 3d at night</td>\n",
       "      <td>negative</td>\n",
       "      <td>ian and i are going to a matinee tomorrow, or that`s the plan at least...it`s going to be too expensive for 3d at night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2603</th>\n",
       "      <td>'If there`s a camel up a hill' and 'I`ll give you plankton' ....HILARIOUS!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>'if there`s a camel up a hill' and 'i`ll give you plankton' ....hilarious!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2604</th>\n",
       "      <td>Oh My God ..  sad day ..</td>\n",
       "      <td>negative</td>\n",
       "      <td>oh my god .. sad day ..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874</th>\n",
       "      <td>had nutella croissant+mango+melon+coffee and a lovely stalk of iris for breakfast in bed courtesy of N  Have the house to myself til noon</td>\n",
       "      <td>positive</td>\n",
       "      <td>had nutella croissant+mango+melon+coffee and a lovely stalk of iris for breakfast in bed courtesy of n have the house to myself til noon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>http://twitpic.com/670ar - this is post guitar hero **** kicking</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://twitpic.com/670ar - this is post guitar hero **** kicking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>_Fenton i didnt even finish cleaning my room cuz i went to a party ahhhh  ITS STILL MESSY</td>\n",
       "      <td>negative</td>\n",
       "      <td>_fenton i didnt even finish cleaning my room cuz i went to a party ahhhh its still messy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>little beetle not feeling any love  Searches are bring up zilch, bar 2 peeps that appear funny !!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>little beetle not feeling any love searches are bring up zilch, bar 2 peeps that appear funny !!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>eeek!! Your coming!!!! Im soo excited to see you on Thursday!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>eeek!! your coming!!!! im soo excited to see you on thursday!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>home alone and no one left me any gummy bears</td>\n",
       "      <td>negative</td>\n",
       "      <td>home alone and no one left me any gummy bears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796</th>\n",
       "      <td>Completely EXCELLENT Dave Matthews concert! Jason Mraz opened &amp; I heard 'stay or leave' &amp; 'crush' for the first time live</td>\n",
       "      <td>positive</td>\n",
       "      <td>completely excellent dave matthews concert! jason mraz opened &amp; i heard 'stay or leave' &amp; 'crush' for the first time live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>it`s trashy and sensationalist...so yes</td>\n",
       "      <td>negative</td>\n",
       "      <td>it`s trashy and sensationalist...so yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>Mommas day is may 10th! Don`t forget to do something nice for your mommyyy</td>\n",
       "      <td>positive</td>\n",
       "      <td>mommas day is may 10th! don`t forget to do something nice for your mommyyy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>read something happy?</td>\n",
       "      <td>positive</td>\n",
       "      <td>read something happy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>http://twitpic.com/4wp8s - My ear hurts, and THIS is my medicine. GUM</td>\n",
       "      <td>negative</td>\n",
       "      <td>http://twitpic.com/4wp8s - my ear hurts, and this is my medicine. gum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>Will have to it`s only across the water and all. Just damned expensive</td>\n",
       "      <td>negative</td>\n",
       "      <td>will have to it`s only across the water and all. just damned expensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>@_supernatural_ http://twitpic.com/66l83 - I really miss her.</td>\n",
       "      <td>negative</td>\n",
       "      <td>@_supernatural_ http://twitpic.com/66l83 - i really miss her.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>OH. I am just so tired.</td>\n",
       "      <td>negative</td>\n",
       "      <td>oh. i am just so tired.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>spirit week! Tuesday have to be in make up by eight so I can die later. School wednesday and thursday then, finally sleep in on Friday!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>spirit week! tuesday have to be in make up by eight so i can die later. school wednesday and thursday then, finally sleep in on friday!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>has anyone else seen this? the  DM notification email says 'reply on we web.' i am all for cool accents, but it`s weird online</td>\n",
       "      <td>negative</td>\n",
       "      <td>has anyone else seen this? the dm notification email says 'reply on we web.' i am all for cool accents, but it`s weird online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>Yeah yesterday I turned 16. My parents rented me out a hummer limo. It was pretty cool</td>\n",
       "      <td>positive</td>\n",
       "      <td>yeah yesterday i turned 16. my parents rented me out a hummer limo. it was pretty cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>playing at my old school playground still the same except them lame **** got rid of the tire swing</td>\n",
       "      <td>negative</td>\n",
       "      <td>playing at my old school playground still the same except them lame **** got rid of the tire swing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3061</th>\n",
       "      <td>... sounds like Jeff has the best job in the world!</td>\n",
       "      <td>positive</td>\n",
       "      <td>... sounds like jeff has the best job in the world!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>Please let me know if it is allright DE i need to know but first just wake up a little and enjoy the cofee  ****</td>\n",
       "      <td>positive</td>\n",
       "      <td>please let me know if it is allright de i need to know but first just wake up a little and enjoy the cofee ****</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>Get yer freak on-it`s Monday! Have a Great one, if it`s possible to have a great Monday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>get yer freak on-it`s monday! have a great one, if it`s possible to have a great monday!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>We`re off.  I`m sorry. Parents coming in around 10-11, gonna eat with them, then house-hunt in Richardson. RAAAAIN CHEEECK</td>\n",
       "      <td>negative</td>\n",
       "      <td>we`re off. i`m sorry. parents coming in around 10-11, gonna eat with them, then house-hunt in richardson. raaaain cheeeck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "613                     Ian and I are going to a matinee tomorrow, or that`s the plan at least...it`s going to be too expensive for 3d at night   \n",
       "2603                                                                'If there`s a camel up a hill' and 'I`ll give you plankton' ....HILARIOUS!!   \n",
       "2604                                                                                                                   Oh My God ..  sad day ..   \n",
       "1874  had nutella croissant+mango+melon+coffee and a lovely stalk of iris for breakfast in bed courtesy of N  Have the house to myself til noon   \n",
       "1347                                                                           http://twitpic.com/670ar - this is post guitar hero **** kicking   \n",
       "2016                                                  _Fenton i didnt even finish cleaning my room cuz i went to a party ahhhh  ITS STILL MESSY   \n",
       "1316                                         little beetle not feeling any love  Searches are bring up zilch, bar 2 peeps that appear funny !!!   \n",
       "1915                                                                             eeek!! Your coming!!!! Im soo excited to see you on Thursday!!   \n",
       "1673                                                                                              home alone and no one left me any gummy bears   \n",
       "2796                  Completely EXCELLENT Dave Matthews concert! Jason Mraz opened & I heard 'stay or leave' & 'crush' for the first time live   \n",
       "2896                                                                                                    it`s trashy and sensationalist...so yes   \n",
       "1922                                                                 Mommas day is may 10th! Don`t forget to do something nice for your mommyyy   \n",
       "2157                                                                                                                      read something happy?   \n",
       "87                                                                        http://twitpic.com/4wp8s - My ear hurts, and THIS is my medicine. GUM   \n",
       "2289                                                                     Will have to it`s only across the water and all. Just damned expensive   \n",
       "1485                                                                              @_supernatural_ http://twitpic.com/66l83 - I really miss her.   \n",
       "709                                                                                                                     OH. I am just so tired.   \n",
       "1884   spirit week! Tuesday have to be in make up by eight so I can die later. School wednesday and thursday then, finally sleep in on Friday!!   \n",
       "1704             has anyone else seen this? the  DM notification email says 'reply on we web.' i am all for cool accents, but it`s weird online   \n",
       "2748                                                     Yeah yesterday I turned 16. My parents rented me out a hummer limo. It was pretty cool   \n",
       "403                                          playing at my old school playground still the same except them lame **** got rid of the tire swing   \n",
       "3061                                                                                        ... sounds like Jeff has the best job in the world!   \n",
       "1487                           Please let me know if it is allright DE i need to know but first just wake up a little and enjoy the cofee  ****   \n",
       "2764                                                   Get yer freak on-it`s Monday! Have a Great one, if it`s possible to have a great Monday!   \n",
       "1887                 We`re off.  I`m sorry. Parents coming in around 10-11, gonna eat with them, then house-hunt in Richardson. RAAAAIN CHEEECK   \n",
       "\n",
       "     sentiment  \\\n",
       "613   negative   \n",
       "2603  positive   \n",
       "2604  negative   \n",
       "1874  positive   \n",
       "1347  negative   \n",
       "2016  negative   \n",
       "1316  positive   \n",
       "1915  positive   \n",
       "1673  negative   \n",
       "2796  positive   \n",
       "2896  negative   \n",
       "1922  positive   \n",
       "2157  positive   \n",
       "87    negative   \n",
       "2289  negative   \n",
       "1485  negative   \n",
       "709   negative   \n",
       "1884  negative   \n",
       "1704  negative   \n",
       "2748  positive   \n",
       "403   negative   \n",
       "3061  positive   \n",
       "1487  positive   \n",
       "2764  positive   \n",
       "1887  negative   \n",
       "\n",
       "                                                                                                                                 selected_text  \n",
       "613                    ian and i are going to a matinee tomorrow, or that`s the plan at least...it`s going to be too expensive for 3d at night  \n",
       "2603                                                               'if there`s a camel up a hill' and 'i`ll give you plankton' ....hilarious!!  \n",
       "2604                                                                                                                   oh my god .. sad day ..  \n",
       "1874  had nutella croissant+mango+melon+coffee and a lovely stalk of iris for breakfast in bed courtesy of n have the house to myself til noon  \n",
       "1347                                                                          http://twitpic.com/670ar - this is post guitar hero **** kicking  \n",
       "2016                                                  _fenton i didnt even finish cleaning my room cuz i went to a party ahhhh its still messy  \n",
       "1316                                         little beetle not feeling any love searches are bring up zilch, bar 2 peeps that appear funny !!!  \n",
       "1915                                                                            eeek!! your coming!!!! im soo excited to see you on thursday!!  \n",
       "1673                                                                                             home alone and no one left me any gummy bears  \n",
       "2796                 completely excellent dave matthews concert! jason mraz opened & i heard 'stay or leave' & 'crush' for the first time live  \n",
       "2896                                                                                                   it`s trashy and sensationalist...so yes  \n",
       "1922                                                                mommas day is may 10th! don`t forget to do something nice for your mommyyy  \n",
       "2157                                                                                                                     read something happy?  \n",
       "87                                                                       http://twitpic.com/4wp8s - my ear hurts, and this is my medicine. gum  \n",
       "2289                                                                    will have to it`s only across the water and all. just damned expensive  \n",
       "1485                                                                             @_supernatural_ http://twitpic.com/66l83 - i really miss her.  \n",
       "709                                                                                                                    oh. i am just so tired.  \n",
       "1884  spirit week! tuesday have to be in make up by eight so i can die later. school wednesday and thursday then, finally sleep in on friday!!  \n",
       "1704             has anyone else seen this? the dm notification email says 'reply on we web.' i am all for cool accents, but it`s weird online  \n",
       "2748                                                    yeah yesterday i turned 16. my parents rented me out a hummer limo. it was pretty cool  \n",
       "403                                         playing at my old school playground still the same except them lame **** got rid of the tire swing  \n",
       "3061                                                                                       ... sounds like jeff has the best job in the world!  \n",
       "1487                           please let me know if it is allright de i need to know but first just wake up a little and enjoy the cofee ****  \n",
       "2764                                                  get yer freak on-it`s monday! have a great one, if it`s possible to have a great monday!  \n",
       "1887                 we`re off. i`m sorry. parents coming in around 10-11, gonna eat with them, then house-hunt in richardson. raaaain cheeeck  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_span.loc[test_df_span.sentiment!=\"neutral\"][[\"text\", \"sentiment\",\"selected_text\"]].sample(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
