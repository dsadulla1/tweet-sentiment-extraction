{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V37\" # OnlySpanModelKF3 2X2Tasks LabelSmoothed with NeutralSamples and Better Preprocessing\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 16\n",
    "PREDICT_BATCH_SIZE = 512\n",
    "DROPOUT = 0.3\n",
    "LABEL_SMOOTHING_PARAM = 0.2\n",
    "\n",
    "RUN_ON_SAMPLE = False\n",
    "EXCLUDE_NEUTRAL_CLASS = False\n",
    "NUM_EPOCHS = [3, 3, 1]\n",
    "NUM_FOLDS = 3\n",
    "#LRs = [5e-3, 1e-4, 1e-6]\n",
    "MAX_LR = 5e-3 #5e-4 #5e-3 #3e-5\n",
    "MID_LR = 5e-4 #5e-5 #1e-4 #3e-5\n",
    "MIN_LR = 1e-6 #5e-6 #1e-6 #3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"../results/\"\n",
    "DATA_DIR = \"../data/\"\n",
    "MODEL_DIR = \"../data/models/roberta-base/\"\n",
    "EXT_MODEL_DIR = \"../data/models/roberta-tokenizer/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, Layer, LeakyReLU, Subtract\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    a = set(str1)\n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeded_value = 753951\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(seeded_value)\n",
    "tf.random.set_seed(seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 14 15:06:13 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0', '2.8.0', '0.5.2']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "if not os.path.exists(MODEL_DIR+\"tokenizers\"):\n",
    "    os.mkdir(MODEL_DIR+\"tokenizers\")\n",
    "\n",
    "if not os.path.exists(MODEL_DIR+\"tokenizers/roberta_tokenizer\"):\n",
    "    os.mkdir(MODEL_DIR+\"tokenizers/roberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\"  target=\"_blank\"><h2 id=\"limiting_gpu_memory_growth\" data-text=\"Limiting GPU memory growth\" tabindex=\"0\">Limiting GPU memory growth</h2></a>\n",
    "<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\"><code translate=\"no\" dir=\"ltr\">CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the <code translate=\"no\" dir=\"ltr\">tf.config.experimental.set_visible_devices</code> method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file=EXT_MODEL_DIR+'/vocab.json',\n",
    "                                             merges_file=EXT_MODEL_DIR+'/merges.txt',                                         \n",
    "                                             add_prefix_space=True,\n",
    "                                             lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(EXT_MODEL_DIR+\"/special_tokens_map.json\") as f:\n",
    "    special_tokens = json.load(f)\n",
    "\n",
    "tokenizer.add_special_tokens([i for i in special_tokens.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.get_vocab_size(); VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  0\n",
      "selected_text  object  0\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27481, 'selected_text': 22464, 'sentiment': 3}\n",
      "            textID                                             text  \\\n",
      "count        27481                                            27481   \n",
      "unique       27481                                            27481   \n",
      "top     a3283b0178  Just got up from a nap.. Relaxing for the night   \n",
      "freq             1                                                1   \n",
      "\n",
      "       selected_text sentiment  \n",
      "count          27481     27481  \n",
      "unique         22464         3  \n",
      "top             good   neutral  \n",
      "freq             199     11118  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\").fillna('')\n",
    "\n",
    "print(pd.concat((df_span.dtypes, df_span.isna().sum()), axis=1))\n",
    "print(df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df_span[i].nunique() for i in df_span.columns})\n",
    "print(df_span.describe())\n",
    "df_span.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID                            text sentiment\n",
      "count         3534                            3534      3534\n",
      "unique        3534                            3534         3\n",
      "top     c8b610923d   well thank your phone for me.   neutral\n",
      "freq             1                               1      1430\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_span = pd.read_csv(DATA_DIR+\"test.csv\", encoding=\"utf8\").fillna('')\n",
    "print(pd.concat((test_df_span.dtypes, test_df_span.isna().sum()), axis=1))\n",
    "print(test_df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df_span[i].nunique() for i in test_df_span.columns})\n",
    "print(test_df_span.describe())\n",
    "test_df_span.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text\"] = df_span[\"text\"].astype(str)\n",
    "df_span[\"selected_text\"] = df_span[\"selected_text\"].astype(str)\n",
    "test_df_span[\"text\"] = test_df_span[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_addspace(text:str) -> str:\n",
    "    text = text.lower()\n",
    "    text = \" \" + text.strip(\" \") + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(text:str, selected_text:str) -> (str, str, int, int):\n",
    "    \n",
    "    text, selected_text = text.lower(), selected_text.lower()\n",
    "    \n",
    "    text = trim_addspace(text)\n",
    "    \n",
    "    substring_ = re.findall(pattern=\"\\\\s[^\\s]*?\"+re.escape(selected_text)+\"[^\\s]*?\\\\s\", string=text)[0]\n",
    "    \n",
    "    return pd.Series([text, \" \"+substring_.strip(\" \"), text.find(substring_), len(substring_) + text.find(substring_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[[\"text_mod\", \"selected_text_mod\", \"start\", \"stop\"]] = df_span[['text','selected_text']].apply(lambda x: find_indices(x.text, x.selected_text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'textID': '4eac33d1c0',\n",
       " 'text': ' wish we could come see u on Denver  husband lost his job and can`t afford it',\n",
       " 'selected_text': 'd lost',\n",
       " 'sentiment': 'negative',\n",
       " 'text_mod': ' wish we could come see u on denver  husband lost his job and can`t afford it ',\n",
       " 'selected_text_mod': ' husband lost',\n",
       " 'start': 36,\n",
       " 'stop': 50}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span.iloc[27476].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span['text_mod'] = test_df_span['text'].apply(trim_addspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'textID': {12154: 'adfbcc6806'},\n",
       " 'text': {12154: 'i wanna see `up` tonight, but no one will go with me. whhhyyy'},\n",
       " 'selected_text': {12154: 'but no one will go with me.'},\n",
       " 'sentiment': {12154: 'negative'},\n",
       " 'text_mod': {12154: ' i wanna see `up` tonight, but no one will go with me. whhhyyy '},\n",
       " 'selected_text_mod': {12154: ' but no one will go with me.'},\n",
       " 'start': {12154: 26},\n",
       " 'stop': {12154: 55}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span.loc[df_span.text_mod.str.contains(\"tonight, but no one will go\")].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning for span detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neutral': [7974], 'negative': [2430], 'positive': [1313]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{t:tokenizer.encode(\" \"+t).ids for t in df_span.sentiment.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text_mod\"] = \"<s>\" + df_span['text_mod'] + \"</s> </s> \" + df_span.sentiment + \" </s>\"\n",
    "test_df_span[\"text_mod\"] = \"<s>\" + test_df_span['text_mod'] + \"</s> </s> \" + test_df_span.sentiment + \" </s>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclusions for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXCLUDE_NEUTRAL_CLASS:\n",
    "    df_span = df_span.loc[df_span.sentiment!=\"neutral\"].copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"EXCLUDE_NEUTRAL_CLASS:\", df_span.shape)\n",
    "\n",
    "\n",
    "if RUN_ON_SAMPLE:\n",
    "    df_span = df_span.sample(2000).copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"Train RUN_ON_SAMPLE\", df_span.shape)\n",
    "    test_df_span = test_df_span.sample(2000).copy()\n",
    "    test_df_span = test_df_span.reset_index(drop=True)\n",
    "    print(\"Test  RUN_ON_SAMPLE\", test_df_span.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_tokens = tokenizer.encode_batch(df_span.text_mod.tolist())\n",
    "Y_span_tokens = tokenizer.encode_batch(df_span.selected_text_mod.tolist())\n",
    "X_span_tokens_test = tokenizer.encode_batch(test_df_span.text_mod.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = [i.ids for i in X_span_tokens]\n",
    "Y_span = [i.ids for i in Y_span_tokens]\n",
    "X_span_test = [i.ids for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_att = [i.attention_mask for i in X_span_tokens]\n",
    "Y_span_att = [i.attention_mask for i in Y_span_tokens] # Useless\n",
    "X_span_att_test = [i.attention_mask for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN_SPAN = max([len(i) for i in X_span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremities(l_string, s_string, print_it=False):\n",
    "    len_l = len(l_string)\n",
    "    len_s = len(s_string)\n",
    "    \n",
    "    for i in range(len_l - len_s + 1):\n",
    "        if (i + len_s) <= len_l:\n",
    "            substring = l_string[i:i+len_s]\n",
    "            if substring == s_string:\n",
    "                if print_it:\n",
    "                    print(l_string)\n",
    "                    print(substring)\n",
    "                    print(i, i+len_s, substring)\n",
    "                \n",
    "                start_vector, end_vector = [0] * len_l, [0] * len_l\n",
    "                start_vector[i], end_vector[i+len_s-1] = 1, 1\n",
    "                \n",
    "                return (start_vector, end_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27481 \t: #Processed\n",
      "2 \t: # of Anomalies\n"
     ]
    }
   ],
   "source": [
    "Y_span_starts, Y_span_stops = [], []\n",
    "anomaly_idx, counter = [], 0\n",
    "for num, (i,j) in enumerate(zip(X_span_tokens, Y_span_tokens)):\n",
    "    x,y = i.ids, j.ids\n",
    "    try:\n",
    "        s,e = get_extremities(x, y)\n",
    "        Y_span_starts.append(s)\n",
    "        Y_span_stops.append(e)\n",
    "    except TypeError as t:\n",
    "        counter += 1\n",
    "        anomaly_idx.append(num)\n",
    "        Y_span_starts.append([0]*15)\n",
    "        Y_span_stops.append([0]*15)\n",
    "print(num + 1, \"\\t: #Processed\")\n",
    "\n",
    "print(counter,\"\\t: # of Anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, yourself. Enjoy London. Watch out for the Hackneys. They`re mental.\n",
      "They`re mental.\n",
      "[['<s>', 0, 0, 0], ['Ġhello', 20760, 0, 0], [',', 6, 0, 0], ['Ġyourself', 2512, 0, 0], ['.', 4, 0, 0], ['Ġenjoy', 2254, 0, 0], ['Ġl', 784, 0, 0], ['ondon', 24639, 0, 0], ['.', 4, 0, 0], ['Ġwatch', 1183, 0, 0], ['Ġout', 66, 0, 0], ['Ġfor', 13, 0, 0], ['Ġthe', 5, 0, 0], ['Ġhack', 14157, 0, 0], ['neys', 30915, 0, 0], ['.', 4, 0, 0], ['Ġthey', 51, 1, 0], ['`', 12905, 0, 0], ['re', 241, 0, 0], ['Ġmental', 2536, 0, 0], ['.', 4, 0, 1], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0], ['Ġnegative', 2430, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0]]\n",
      "[[51, 'Ġthey'], [12905, '`'], [241, 're'], [2536, 'Ġmental'], [4, '.']]\n"
     ]
    }
   ],
   "source": [
    "check_idx = 1572\n",
    "print(df_span.text[check_idx])\n",
    "print(df_span.selected_text[check_idx])\n",
    "print([[i,j,k,l] for i,j,k,l in zip(X_span_tokens[check_idx].tokens,\n",
    "                                    X_span_tokens[check_idx].ids,\n",
    "                                    Y_span_starts[check_idx],\n",
    "                                    Y_span_stops[check_idx])])\n",
    "print([[i,j] for i,j in zip(Y_span_tokens[check_idx].ids,\n",
    "                            Y_span_tokens[check_idx].tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "\n",
    "Y_span_starts = pad_sequences(Y_span_starts, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")#.argmax(axis=1)\n",
    "Y_span_stops = pad_sequences(Y_span_stops, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")#.argmax(axis=1)\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAX_SEQ_LEN_SPAN': 108,\n",
      " 'VOCAB_SIZE': 50265,\n",
      " 'X_span': (27481, 108),\n",
      " 'X_span_att': (27481, 108),\n",
      " 'X_span_att_test': (3534, 108),\n",
      " 'X_span_test': (3534, 108),\n",
      " 'Y_span': (27481, 108),\n",
      " 'Y_span_starts': (27481, 108),\n",
      " 'Y_span_stops': (27481, 108)}\n"
     ]
    }
   ],
   "source": [
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \"Y_span_starts\" : Y_span_starts.shape,\n",
    "    \"Y_span_stops\" : Y_span_stops.shape,\n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "    \"VOCAB_SIZE\":VOCAB_SIZE,\n",
    "    \"MAX_SEQ_LEN_SPAN\":MAX_SEQ_LEN_SPAN\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_flag = np.isin(Y_span_stops.argmax(axis=1),\n",
    "                    np.unique(Y_span_stops.argmax(axis=1),\n",
    "                              return_counts=True)[0][np.unique(Y_span_stops.argmax(axis=1),\n",
    "                                                               return_counts=True)[1]>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27475, 27481, 6)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(keep_flag), df_span.shape[0], df_span.shape[0] - sum(keep_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (27481, 108) \t: X  \n",
      " (27481, 108) \t: X_att  \n",
      " (27481, 108) \t: Y  \n",
      " (27481, 108) \t: Y_starts  \n",
      " (27481, 108) \t: Y_stops  \n",
      " (3534, 108) \t: X_test  \n",
      " (3534, 108) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = X_span[keep_flag]\n",
    "X_span_att = X_span_att[keep_flag]\n",
    "Y_span = Y_span[keep_flag]\n",
    "Y_span_starts = Y_span_starts[keep_flag]\n",
    "Y_span_stops = Y_span_stops[keep_flag]\n",
    "X_span_test = X_span_test\n",
    "X_span_att_test = X_span_att_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (27475, 108) \t: X  \n",
      " (27475, 108) \t: X_att  \n",
      " (27475, 108) \t: Y  \n",
      " (27475, 108) \t: Y_starts  \n",
      " (27475, 108) \t: Y_stops  \n",
      " (3534, 108) \t: X_test  \n",
      " (3534, 108) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_span_words = [tokenizer.decode(i) for i in Y_span]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 108)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LEN =  MAX_SEQ_LEN_SPAN\n",
    "MAX_SEQ_LEN, MAX_SEQ_LEN_SPAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X_span': (27475, 108),\n",
      " 'X_span_att': (27475, 108),\n",
      " 'X_span_att_test': (3534, 108),\n",
      " 'X_span_test': (3534, 108),\n",
      " 'Y_span': (27475, 108)}\n"
     ]
    }
   ],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span_starts = pad_sequences(Y_span_starts, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span_stops = pad_sequences(Y_span_stops, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_sequences = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"words\")\n",
    "    input_att_flags = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"att_flags\")\n",
    "    input_token_ids = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"token_ids\")\n",
    "    \n",
    "    config = RobertaConfig.from_pretrained(MODEL_DIR+'config.json')\n",
    "    roberta_model = TFRobertaModel.from_pretrained(MODEL_DIR+'tf_model.h5', config=config)\n",
    "    x = roberta_model(inputs=input_sequences, attention_mask=input_att_flags, token_type_ids=input_token_ids)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(DROPOUT)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    output_starts_0 = tf.keras.layers.Activation('softmax', name=\"starts_0\")(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(DROPOUT)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    output_stops_0 = tf.keras.layers.Activation('softmax', name=\"stops_0\")(x2)\n",
    "    \n",
    "    output_subtract = tf.keras.layers.Subtract()([output_starts_0, output_stops_0])\n",
    "    output_flat = concatenate([output_starts_0, output_stops_0, output_subtract])\n",
    "    output_starts_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"starts_1\")(output_flat)\n",
    "    output_stops_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"stops_1\")(output_flat)\n",
    "    \n",
    "    span_detection_model = Model([input_att_flags, input_sequences, input_token_ids],\n",
    "                                 [output_starts_0, output_stops_0, output_starts_1, output_stops_1])\n",
    "    \n",
    "    return span_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_detection_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 108)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 108)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 108)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 108, 768), ( 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 108, 768)     0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 108, 768)     0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 108, 768)     1180416     dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 108, 768)     1180416     dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 108, 768)     0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 108, 768)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 108, 1)       769         leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 108, 1)       769         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 108)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 108)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "starts_0 (Activation)           (None, 108)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "stops_0 (Activation)            (None, 108)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 108)          0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 324)          0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "                                                                 subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "starts_1 (Dense)                (None, 108)          35100       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "stops_1 (Dense)                 (None, 108)          35100       concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 127,078,202\n",
      "Trainable params: 127,078,202\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "span_detection_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span Detection Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_FOLDS):\n",
    "    if os.path.exists(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\"):\n",
    "        os.remove(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(pred_dict):\n",
    "    print(\"[INFO] \",\"=\"*15,\"Validation for FOLD#\", num, \"=\"*15)\n",
    "    funcs = [accuracy_score, f1_score, precision_score, recall_score, confusion_matrix]\n",
    "    for f in funcs:\n",
    "        for data_set in [\"train\",\"valid\"]:\n",
    "            for var in [\"starts\", \"stops\"]:\n",
    "                if f in [accuracy_score]:\n",
    "                    res = f(**pred_dict[data_set][var])\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "                elif f in [confusion_matrix]:\n",
    "                    res = f(**pred_dict[data_set][var], labels=np.arange(MAX_SEQ_LEN))\n",
    "                    np.savetxt(X=res, fmt='%i', delimiter=\",\",\n",
    "                               fname=RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                    print(\"[INFO] \\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__, \"\\t||\", \n",
    "                          RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                else:\n",
    "                    res = f(**pred_dict[data_set][var], average=\"macro\")\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "        print(\"=======================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(string):\n",
    "    string = re.sub(pattern=\" (negative|positive|neutral)[ ]+$\", repl=\"\", string=string)\n",
    "    string = re.sub(pattern=\" +\", repl=\" \", string=string.strip())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><h2 id=\"finetuning\">Fine-tuning</h2></a>\n",
    "<p>Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.</p>\n",
    " <p>This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.</p>\n",
    " <p>It is critical to only do this step <em>after</em> the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.</p>\n",
    " <p>It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>\n",
    "\n",
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><p><strong>Important note about <code>compile()</code> and <code>trainable</code></strong></p></a>\n",
    "<p>Calling <code>compile()</code> on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the <code>trainable</code>\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until <code>compile</code> is called again. Hence, if you change any <code>trainable</code> value, make sure\n",
    " to call <code>compile()</code> again on your\n",
    "model for your changes to be taken into account.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossWeightAdjust(Callback):\n",
    "    def __init__(self, alpha, beta, gamma, delta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "    \n",
    "    # customize your behavior\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        losses = np.array([v for k,v in logs.items() if k in ['val_starts_0_loss', 'val_stops_0_loss', 'val_starts_1_loss', 'val_stops_1_loss']], dtype=np.float64)\n",
    "        losses = (losses - 0.5*losses.min()) / (losses.max() - 0.5*losses.min())\n",
    "        losses = losses/np.sum(losses)\n",
    "\n",
    "        K.set_value(self.alpha, losses[0])\n",
    "        K.set_value(self.beta, losses[1])\n",
    "        K.set_value(self.gamma, losses[2])\n",
    "        K.set_value(self.delta, losses[3])\n",
    "        \n",
    "        print(\"\\n Loss weights recalibrated to alpha = %s, beta = %s, gamma = %s, delta = %s \" % (np.round(losses[0],2),\n",
    "                                                                                                  np.round(losses[1],2),\n",
    "                                                                                                  np.round(losses[2],2),\n",
    "                                                                                                  np.round(losses[3],2)))\n",
    "        \n",
    "        logger.info(\"Loss weights recalibrated to alpha = %s, beta = %s, gamma = %s, delta = %s \" % (K.get_value(self.alpha), K.get_value(self.beta), K.get_value(self.gamma), K.get_value(self.delta)))\n",
    "        #logger.info(\"epoch %s, alpha = %s, beta = %s, gamma = %s, delta = %s\" % (epoch, K.get_value(self.alpha), K.get_value(self.beta), K.get_value(self.gamma), K.get_value(self.delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What does the Loss Weight Adjust Callback do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17266986, 0.1622123 , 0.33177851, 0.33333934])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = np.array([2.7892, 2.7021, 4.1144, 4.1274])\n",
    "losses = (losses - 0.5*losses.min()) / (losses.max() - 0.5*losses.min())\n",
    "losses = losses/np.sum(losses)\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ==================== FOLD# 0 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 18316 samples, validate on 9159 samples\n",
      "Epoch 1/3\n",
      "18304/18316 [============================>.] - ETA: 0s - loss: 4.5309 - starts_0_loss: 2.8040 - stops_0_loss: 2.9706 - starts_1_loss: 2.8678 - stops_1_loss: 3.1605 - starts_0_accuracy: 0.5164 - stops_0_accuracy: 0.5834 - starts_1_accuracy: 0.5807 - stops_1_accuracy: 0.5074\n",
      " Loss weights recalibrated to alpha = 0.21, beta = 0.32, gamma = 0.23, delta = 0.23 \n",
      "18316/18316 [==============================] - 282s 15ms/sample - loss: 4.5314 - starts_0_loss: 2.8052 - stops_0_loss: 2.9710 - starts_1_loss: 2.8683 - stops_1_loss: 3.1606 - starts_0_accuracy: 0.5162 - stops_0_accuracy: 0.5832 - starts_1_accuracy: 0.5805 - stops_1_accuracy: 0.5073 - val_loss: 4.0621 - val_starts_0_loss: 2.5348 - val_stops_0_loss: 3.1498 - val_starts_1_loss: 2.6372 - val_stops_1_loss: 2.6429 - val_starts_0_accuracy: 0.5441 - val_stops_0_accuracy: 0.6146 - val_starts_1_accuracy: 0.5810 - val_stops_1_accuracy: 0.6137\n",
      "Epoch 2/3\n",
      "18304/18316 [============================>.] - ETA: 0s - loss: 2.6635 - starts_0_loss: 2.7135 - stops_0_loss: 2.5997 - starts_1_loss: 2.6148 - stops_1_loss: 2.7533 - starts_0_accuracy: 0.5252 - stops_0_accuracy: 0.5965 - starts_1_accuracy: 0.5859 - stops_1_accuracy: 0.5915\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "18316/18316 [==============================] - 269s 15ms/sample - loss: 2.6637 - starts_0_loss: 2.7136 - stops_0_loss: 2.6002 - starts_1_loss: 2.6148 - stops_1_loss: 2.7535 - starts_0_accuracy: 0.5251 - stops_0_accuracy: 0.5965 - starts_1_accuracy: 0.5859 - stops_1_accuracy: 0.5915 - val_loss: 2.5455 - val_starts_0_loss: 2.5744 - val_stops_0_loss: 2.4255 - val_starts_1_loss: 2.6045 - val_stops_1_loss: 2.6247 - val_starts_0_accuracy: 0.5561 - val_stops_0_accuracy: 0.6199 - val_starts_1_accuracy: 0.5817 - val_stops_1_accuracy: 0.6193\n",
      "Epoch 3/3\n",
      "18304/18316 [============================>.] - ETA: 0s - loss: 2.6603 - starts_0_loss: 2.7434 - stops_0_loss: 2.6196 - starts_1_loss: 2.5772 - stops_1_loss: 2.6970 - starts_0_accuracy: 0.5225 - stops_0_accuracy: 0.5945 - starts_1_accuracy: 0.5887 - stops_1_accuracy: 0.5895\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.25, gamma = 0.25, delta = 0.26 \n",
      "18316/18316 [==============================] - 264s 14ms/sample - loss: 2.6605 - starts_0_loss: 2.7433 - stops_0_loss: 2.6200 - starts_1_loss: 2.5772 - stops_1_loss: 2.6976 - starts_0_accuracy: 0.5225 - stops_0_accuracy: 0.5944 - starts_1_accuracy: 0.5887 - stops_1_accuracy: 0.5894 - val_loss: 2.5192 - val_starts_0_loss: 2.4794 - val_stops_0_loss: 2.5150 - val_starts_1_loss: 2.5194 - val_stops_1_loss: 2.5623 - val_starts_0_accuracy: 0.5842 - val_stops_0_accuracy: 0.6218 - val_starts_1_accuracy: 0.5972 - val_stops_1_accuracy: 0.6261\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 18316 samples, validate on 9159 samples\n",
      "Epoch 1/3\n",
      "18304/18316 [============================>.] - ETA: 0s - loss: 2.5047 - starts_0_loss: 2.5333 - stops_0_loss: 2.4421 - starts_1_loss: 2.4935 - stops_1_loss: 2.5489 - starts_0_accuracy: 0.5692 - stops_0_accuracy: 0.6163 - starts_1_accuracy: 0.5965 - stops_1_accuracy: 0.6231\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.25, gamma = 0.25, delta = 0.26 \n",
      "18316/18316 [==============================] - 282s 15ms/sample - loss: 2.5051 - starts_0_loss: 2.5343 - stops_0_loss: 2.4423 - starts_1_loss: 2.4943 - stops_1_loss: 2.5492 - starts_0_accuracy: 0.5690 - stops_0_accuracy: 0.6162 - starts_1_accuracy: 0.5963 - stops_1_accuracy: 0.6230 - val_loss: 2.4641 - val_starts_0_loss: 2.3986 - val_stops_0_loss: 2.4408 - val_starts_1_loss: 2.4792 - val_stops_1_loss: 2.5344 - val_starts_0_accuracy: 0.5912 - val_stops_0_accuracy: 0.6152 - val_starts_1_accuracy: 0.6013 - val_stops_1_accuracy: 0.6221\n",
      "Epoch 2/3\n",
      "18304/18316 [============================>.] - ETA: 0s - loss: 2.4759 - starts_0_loss: 2.5060 - stops_0_loss: 2.4101 - starts_1_loss: 2.4713 - stops_1_loss: 2.5144 - starts_0_accuracy: 0.5735 - stops_0_accuracy: 0.6228 - starts_1_accuracy: 0.6027 - stops_1_accuracy: 0.6334\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.24, gamma = 0.25, delta = 0.26 \n",
      "18316/18316 [==============================] - 266s 15ms/sample - loss: 2.4760 - starts_0_loss: 2.5060 - stops_0_loss: 2.4105 - starts_1_loss: 2.4713 - stops_1_loss: 2.5145 - starts_0_accuracy: 0.5734 - stops_0_accuracy: 0.6227 - starts_1_accuracy: 0.6026 - stops_1_accuracy: 0.6333 - val_loss: 2.4396 - val_starts_0_loss: 2.4059 - val_stops_0_loss: 2.4030 - val_starts_1_loss: 2.4592 - val_stops_1_loss: 2.4861 - val_starts_0_accuracy: 0.5932 - val_stops_0_accuracy: 0.6190 - val_starts_1_accuracy: 0.6079 - val_stops_1_accuracy: 0.6270\n",
      "Epoch 3/3\n",
      "18304/18316 [============================>.] - ETA: 0s - loss: 2.4558 - starts_0_loss: 2.4947 - stops_0_loss: 2.4003 - starts_1_loss: 2.4473 - stops_1_loss: 2.4797 - starts_0_accuracy: 0.5855 - stops_0_accuracy: 0.6346 - starts_1_accuracy: 0.6123 - stops_1_accuracy: 0.6413\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "18316/18316 [==============================] - 267s 15ms/sample - loss: 2.4560 - starts_0_loss: 2.4947 - stops_0_loss: 2.4006 - starts_1_loss: 2.4473 - stops_1_loss: 2.4803 - starts_0_accuracy: 0.5854 - stops_0_accuracy: 0.6344 - starts_1_accuracy: 0.6123 - stops_1_accuracy: 0.6412 - val_loss: 2.4235 - val_starts_0_loss: 2.4189 - val_stops_0_loss: 2.3367 - val_starts_1_loss: 2.4654 - val_stops_1_loss: 2.4686 - val_starts_0_accuracy: 0.5821 - val_stops_0_accuracy: 0.6229 - val_starts_1_accuracy: 0.6018 - val_stops_1_accuracy: 0.6343\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 18316 samples, validate on 9159 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "18304/18316 [============================>.] - ETA: 0s - loss: 2.4343 - starts_0_loss: 2.4773 - stops_0_loss: 2.3619 - starts_1_loss: 2.4463 - stops_1_loss: 2.4461 - starts_0_accuracy: 0.5802 - stops_0_accuracy: 0.6332 - starts_1_accuracy: 0.6102 - stops_1_accuracy: 0.6485\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.24, gamma = 0.27, delta = 0.25 \n",
      "18316/18316 [==============================] - 571s 31ms/sample - loss: 2.4347 - starts_0_loss: 2.4782 - stops_0_loss: 2.3620 - starts_1_loss: 2.4471 - stops_1_loss: 2.4464 - starts_0_accuracy: 0.5800 - stops_0_accuracy: 0.6331 - starts_1_accuracy: 0.6100 - stops_1_accuracy: 0.6483 - val_loss: 2.3417 - val_starts_0_loss: 2.3443 - val_stops_0_loss: 2.2692 - val_starts_1_loss: 2.4290 - val_stops_1_loss: 2.3177 - val_starts_0_accuracy: 0.6044 - val_stops_0_accuracy: 0.6493 - val_starts_1_accuracy: 0.6111 - val_stops_1_accuracy: 0.6798\n",
      "[INFO]  =============== Validation for FOLD# 0 ===============\n",
      "[INFO] 61.93 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 66.00 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 60.44 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 64.93 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 31.76 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 62.69 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 31.89 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 60.74 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 33.23 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 58.76 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 34.63 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 57.16 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 31.44 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 70.60 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 30.89 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 68.56 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V37_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V37_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V37_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V37_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (18316, 108) (18316, 108)\n",
      "[INFO] Prediction shape for validation data:  (9159, 108) (9159, 108)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  16866 out of 18316\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  8462 out of 9159\n",
      "[INFO] Training Jaccard Score:  0.681903762764903\n",
      "[INFO] Validation Jaccard Score:  0.6784706172994983\n",
      "[INFO] Training for fold: 0 finished at Sun Jun 14 15:47:11 2020\n",
      "[INFO] ==================== FOLD# 1 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 18317 samples, validate on 9158 samples\n",
      "Epoch 1/3\n",
      "18304/18317 [============================>.] - ETA: 0s - loss: 4.5194 - starts_0_loss: 2.7845 - stops_0_loss: 2.9748 - starts_1_loss: 2.8663 - stops_1_loss: 3.1507 - starts_0_accuracy: 0.5208 - stops_0_accuracy: 0.5830 - starts_1_accuracy: 0.5776 - stops_1_accuracy: 0.5109\n",
      " Loss weights recalibrated to alpha = 0.22, beta = 0.29, gamma = 0.24, delta = 0.25 \n",
      "18317/18317 [==============================] - 280s 15ms/sample - loss: 4.5195 - starts_0_loss: 2.7843 - stops_0_loss: 2.9754 - starts_1_loss: 2.8661 - stops_1_loss: 3.1507 - starts_0_accuracy: 0.5209 - stops_0_accuracy: 0.5829 - starts_1_accuracy: 0.5776 - stops_1_accuracy: 0.5108 - val_loss: 4.0258 - val_starts_0_loss: 2.5459 - val_stops_0_loss: 2.9077 - val_starts_1_loss: 2.6478 - val_stops_1_loss: 2.6676 - val_starts_0_accuracy: 0.5658 - val_stops_0_accuracy: 0.6123 - val_starts_1_accuracy: 0.5832 - val_stops_1_accuracy: 0.6070\n",
      "Epoch 2/3\n",
      "18304/18317 [============================>.] - ETA: 0s - loss: 2.6559 - starts_0_loss: 2.6977 - stops_0_loss: 2.5978 - starts_1_loss: 2.6074 - stops_1_loss: 2.7336 - starts_0_accuracy: 0.5293 - stops_0_accuracy: 0.6000 - starts_1_accuracy: 0.5842 - stops_1_accuracy: 0.5910\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.26, gamma = 0.25, delta = 0.26 \n",
      "18317/18317 [==============================] - 268s 15ms/sample - loss: 2.6555 - starts_0_loss: 2.6973 - stops_0_loss: 2.5973 - starts_1_loss: 2.6068 - stops_1_loss: 2.7330 - starts_0_accuracy: 0.5295 - stops_0_accuracy: 0.6001 - starts_1_accuracy: 0.5844 - stops_1_accuracy: 0.5910 - val_loss: 2.5607 - val_starts_0_loss: 2.4970 - val_stops_0_loss: 2.6021 - val_starts_1_loss: 2.5419 - val_stops_1_loss: 2.5891 - val_starts_0_accuracy: 0.5715 - val_stops_0_accuracy: 0.6140 - val_starts_1_accuracy: 0.5940 - val_stops_1_accuracy: 0.6155\n",
      "Epoch 3/3\n",
      "18304/18317 [============================>.] - ETA: 0s - loss: 2.6472 - starts_0_loss: 2.7230 - stops_0_loss: 2.5971 - starts_1_loss: 2.5770 - stops_1_loss: 2.6948 - starts_0_accuracy: 0.5227 - stops_0_accuracy: 0.5985 - starts_1_accuracy: 0.5828 - stops_1_accuracy: 0.5922\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.24, gamma = 0.25, delta = 0.26 \n",
      "18317/18317 [==============================] - 266s 15ms/sample - loss: 2.6471 - starts_0_loss: 2.7230 - stops_0_loss: 2.5972 - starts_1_loss: 2.5768 - stops_1_loss: 2.6947 - starts_0_accuracy: 0.5227 - stops_0_accuracy: 0.5985 - starts_1_accuracy: 0.5828 - stops_1_accuracy: 0.5922 - val_loss: 2.5162 - val_starts_0_loss: 2.4812 - val_stops_0_loss: 2.4882 - val_starts_1_loss: 2.5319 - val_stops_1_loss: 2.5613 - val_starts_0_accuracy: 0.5797 - val_stops_0_accuracy: 0.6219 - val_starts_1_accuracy: 0.5958 - val_stops_1_accuracy: 0.6243\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 18317 samples, validate on 9158 samples\n",
      "Epoch 1/3\n",
      "18304/18317 [============================>.] - ETA: 0s - loss: 2.4966 - starts_0_loss: 2.5211 - stops_0_loss: 2.4330 - starts_1_loss: 2.4846 - stops_1_loss: 2.5455 - starts_0_accuracy: 0.5714 - stops_0_accuracy: 0.6190 - starts_1_accuracy: 0.5978 - stops_1_accuracy: 0.6265\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.24, gamma = 0.26, delta = 0.26 \n",
      "18317/18317 [==============================] - 281s 15ms/sample - loss: 2.4967 - starts_0_loss: 2.5210 - stops_0_loss: 2.4334 - starts_1_loss: 2.4846 - stops_1_loss: 2.5457 - starts_0_accuracy: 0.5714 - stops_0_accuracy: 0.6189 - starts_1_accuracy: 0.5978 - stops_1_accuracy: 0.6264 - val_loss: 2.4573 - val_starts_0_loss: 2.4025 - val_stops_0_loss: 2.4016 - val_starts_1_loss: 2.4948 - val_stops_1_loss: 2.5250 - val_starts_0_accuracy: 0.5974 - val_stops_0_accuracy: 0.6143 - val_starts_1_accuracy: 0.5986 - val_stops_1_accuracy: 0.6201\n",
      "Epoch 2/3\n",
      "18304/18317 [============================>.] - ETA: 0s - loss: 2.4692 - starts_0_loss: 2.5025 - stops_0_loss: 2.3991 - starts_1_loss: 2.4635 - stops_1_loss: 2.5083 - starts_0_accuracy: 0.5754 - stops_0_accuracy: 0.6240 - starts_1_accuracy: 0.6006 - stops_1_accuracy: 0.6338\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.24, gamma = 0.26, delta = 0.26 \n",
      "18317/18317 [==============================] - 267s 15ms/sample - loss: 2.4688 - starts_0_loss: 2.5020 - stops_0_loss: 2.3985 - starts_1_loss: 2.4629 - stops_1_loss: 2.5078 - starts_0_accuracy: 0.5756 - stops_0_accuracy: 0.6242 - starts_1_accuracy: 0.6009 - stops_1_accuracy: 0.6340 - val_loss: 2.4356 - val_starts_0_loss: 2.4243 - val_stops_0_loss: 2.3599 - val_starts_1_loss: 2.4640 - val_stops_1_loss: 2.4866 - val_starts_0_accuracy: 0.5863 - val_stops_0_accuracy: 0.6199 - val_starts_1_accuracy: 0.6059 - val_stops_1_accuracy: 0.6273\n",
      "Epoch 3/3\n",
      "18304/18317 [============================>.] - ETA: 0s - loss: 2.4522 - starts_0_loss: 2.4919 - stops_0_loss: 2.3871 - starts_1_loss: 2.4495 - stops_1_loss: 2.4756 - starts_0_accuracy: 0.5810 - stops_0_accuracy: 0.6327 - starts_1_accuracy: 0.6076 - stops_1_accuracy: 0.6459\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.24, gamma = 0.25, delta = 0.26 \n",
      "18317/18317 [==============================] - 269s 15ms/sample - loss: 2.4522 - starts_0_loss: 2.4920 - stops_0_loss: 2.3872 - starts_1_loss: 2.4494 - stops_1_loss: 2.4757 - starts_0_accuracy: 0.5810 - stops_0_accuracy: 0.6327 - starts_1_accuracy: 0.6076 - stops_1_accuracy: 0.6458 - val_loss: 2.4367 - val_starts_0_loss: 2.4199 - val_stops_0_loss: 2.3801 - val_starts_1_loss: 2.4584 - val_stops_1_loss: 2.4829 - val_starts_0_accuracy: 0.5934 - val_stops_0_accuracy: 0.6209 - val_starts_1_accuracy: 0.6045 - val_stops_1_accuracy: 0.6281\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 18317 samples, validate on 9158 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "18304/18317 [============================>.] - ETA: 0s - loss: 2.4132 - starts_0_loss: 2.4539 - stops_0_loss: 2.3348 - starts_1_loss: 2.4372 - stops_1_loss: 2.4229 - starts_0_accuracy: 0.5860 - stops_0_accuracy: 0.6378 - starts_1_accuracy: 0.6068 - stops_1_accuracy: 0.6555\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.27, delta = 0.25 \n",
      "18317/18317 [==============================] - 578s 32ms/sample - loss: 2.4132 - starts_0_loss: 2.4537 - stops_0_loss: 2.3352 - starts_1_loss: 2.4371 - stops_1_loss: 2.4231 - starts_0_accuracy: 0.5861 - stops_0_accuracy: 0.6377 - starts_1_accuracy: 0.6068 - stops_1_accuracy: 0.6555 - val_loss: 2.3349 - val_starts_0_loss: 2.3433 - val_stops_0_loss: 2.2438 - val_starts_1_loss: 2.4376 - val_stops_1_loss: 2.3106 - val_starts_0_accuracy: 0.6102 - val_stops_0_accuracy: 0.6605 - val_starts_1_accuracy: 0.6094 - val_stops_1_accuracy: 0.6877\n",
      "[INFO]  =============== Validation for FOLD# 1 ===============\n",
      "[INFO] 62.10 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 67.29 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 61.02 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 66.05 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 28.45 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 64.78 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 29.73 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 66.16 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 32.42 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 60.75 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 33.82 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 62.54 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 25.88 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 71.31 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 27.36 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 72.29 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V37_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V37_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V37_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V37_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (18317, 108) (18317, 108)\n",
      "[INFO] Prediction shape for validation data:  (9158, 108) (9158, 108)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  16789 out of 18317\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  8397 out of 9158\n",
      "[INFO] Training Jaccard Score:  0.6762098000955359\n",
      "[INFO] Validation Jaccard Score:  0.6750362260146804\n",
      "[INFO] Training for fold: 1 finished at Sun Jun 14 16:27:48 2020\n",
      "[INFO] ==================== FOLD# 2 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 18317 samples, validate on 9158 samples\n",
      "Epoch 1/3\n",
      "18304/18317 [============================>.] - ETA: 0s - loss: 4.5125 - starts_0_loss: 2.7776 - stops_0_loss: 2.9497 - starts_1_loss: 2.8656 - stops_1_loss: 3.1524 - starts_0_accuracy: 0.5176 - stops_0_accuracy: 0.5833 - starts_1_accuracy: 0.5775 - stops_1_accuracy: 0.5073\n",
      " Loss weights recalibrated to alpha = 0.23, beta = 0.31, gamma = 0.23, delta = 0.23 \n",
      "18317/18317 [==============================] - 287s 16ms/sample - loss: 4.5124 - starts_0_loss: 2.7783 - stops_0_loss: 2.9499 - starts_1_loss: 2.8657 - stops_1_loss: 3.1519 - starts_0_accuracy: 0.5175 - stops_0_accuracy: 0.5833 - starts_1_accuracy: 0.5774 - stops_1_accuracy: 0.5074 - val_loss: 4.1164 - val_starts_0_loss: 2.6531 - val_stops_0_loss: 3.1316 - val_starts_1_loss: 2.6403 - val_stops_1_loss: 2.6798 - val_starts_0_accuracy: 0.5105 - val_stops_0_accuracy: 0.6149 - val_starts_1_accuracy: 0.5847 - val_stops_1_accuracy: 0.6139\n",
      "Epoch 2/3\n",
      "18304/18317 [============================>.] - ETA: 0s - loss: 2.6549 - starts_0_loss: 2.6965 - stops_0_loss: 2.5928 - starts_1_loss: 2.6064 - stops_1_loss: 2.7442 - starts_0_accuracy: 0.5349 - stops_0_accuracy: 0.5988 - starts_1_accuracy: 0.5862 - stops_1_accuracy: 0.5906\n",
      " Loss weights recalibrated to alpha = 0.26, beta = 0.23, gamma = 0.25, delta = 0.26 \n",
      "18317/18317 [==============================] - 269s 15ms/sample - loss: 2.6547 - starts_0_loss: 2.6965 - stops_0_loss: 2.5921 - starts_1_loss: 2.6062 - stops_1_loss: 2.7438 - starts_0_accuracy: 0.5347 - stops_0_accuracy: 0.5990 - starts_1_accuracy: 0.5862 - stops_1_accuracy: 0.5908 - val_loss: 2.5235 - val_starts_0_loss: 2.5745 - val_stops_0_loss: 2.4281 - val_starts_1_loss: 2.5335 - val_stops_1_loss: 2.5901 - val_starts_0_accuracy: 0.5489 - val_stops_0_accuracy: 0.6215 - val_starts_1_accuracy: 0.5947 - val_stops_1_accuracy: 0.6198\n",
      "Epoch 3/3\n",
      "18304/18317 [============================>.] - ETA: 0s - loss: 2.6524 - starts_0_loss: 2.7134 - stops_0_loss: 2.6198 - starts_1_loss: 2.5732 - stops_1_loss: 2.6969 - starts_0_accuracy: 0.5256 - stops_0_accuracy: 0.5960 - starts_1_accuracy: 0.5871 - stops_1_accuracy: 0.5931\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.26, delta = 0.26 \n",
      "18317/18317 [==============================] - 266s 14ms/sample - loss: 2.6525 - starts_0_loss: 2.7136 - stops_0_loss: 2.6197 - starts_1_loss: 2.5735 - stops_1_loss: 2.6968 - starts_0_accuracy: 0.5254 - stops_0_accuracy: 0.5959 - starts_1_accuracy: 0.5871 - stops_1_accuracy: 0.5931 - val_loss: 2.4891 - val_starts_0_loss: 2.5066 - val_stops_0_loss: 2.3945 - val_starts_1_loss: 2.5184 - val_stops_1_loss: 2.5274 - val_starts_0_accuracy: 0.5614 - val_stops_0_accuracy: 0.6210 - val_starts_1_accuracy: 0.5958 - val_stops_1_accuracy: 0.6261\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 18317 samples, validate on 9158 samples\n",
      "Epoch 1/3\n",
      "18304/18317 [============================>.] - ETA: 0s - loss: 2.4958 - starts_0_loss: 2.5135 - stops_0_loss: 2.4341 - starts_1_loss: 2.4811 - stops_1_loss: 2.5484 - starts_0_accuracy: 0.5709 - stops_0_accuracy: 0.6150 - starts_1_accuracy: 0.6006 - stops_1_accuracy: 0.6220\n",
      " Loss weights recalibrated to alpha = 0.24, beta = 0.24, gamma = 0.26, delta = 0.26 \n",
      "18317/18317 [==============================] - 282s 15ms/sample - loss: 2.4959 - starts_0_loss: 2.5138 - stops_0_loss: 2.4343 - starts_1_loss: 2.4814 - stops_1_loss: 2.5483 - starts_0_accuracy: 0.5707 - stops_0_accuracy: 0.6151 - starts_1_accuracy: 0.6005 - stops_1_accuracy: 0.6220 - val_loss: 2.4806 - val_starts_0_loss: 2.4160 - val_stops_0_loss: 2.4400 - val_starts_1_loss: 2.5204 - val_stops_1_loss: 2.5405 - val_starts_0_accuracy: 0.5890 - val_stops_0_accuracy: 0.6181 - val_starts_1_accuracy: 0.5958 - val_stops_1_accuracy: 0.6211\n",
      "Epoch 2/3\n",
      "18304/18317 [============================>.] - ETA: 0s - loss: 2.4680 - starts_0_loss: 2.4945 - stops_0_loss: 2.4056 - starts_1_loss: 2.4571 - stops_1_loss: 2.5123 - starts_0_accuracy: 0.5777 - stops_0_accuracy: 0.6218 - starts_1_accuracy: 0.6037 - stops_1_accuracy: 0.6322\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.24, gamma = 0.25, delta = 0.26 \n",
      "18317/18317 [==============================] - 267s 15ms/sample - loss: 2.4677 - starts_0_loss: 2.4945 - stops_0_loss: 2.4051 - starts_1_loss: 2.4570 - stops_1_loss: 2.5117 - starts_0_accuracy: 0.5777 - stops_0_accuracy: 0.6220 - starts_1_accuracy: 0.6038 - stops_1_accuracy: 0.6324 - val_loss: 2.4627 - val_starts_0_loss: 2.4435 - val_stops_0_loss: 2.4126 - val_starts_1_loss: 2.4748 - val_stops_1_loss: 2.5142 - val_starts_0_accuracy: 0.5773 - val_stops_0_accuracy: 0.6198 - val_starts_1_accuracy: 0.6031 - val_stops_1_accuracy: 0.6258\n",
      "Epoch 3/3\n",
      "18304/18317 [============================>.] - ETA: 0s - loss: 2.4521 - starts_0_loss: 2.4841 - stops_0_loss: 2.3961 - starts_1_loss: 2.4349 - stops_1_loss: 2.4901 - starts_0_accuracy: 0.5889 - stops_0_accuracy: 0.6288 - starts_1_accuracy: 0.6139 - stops_1_accuracy: 0.6422\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.24, gamma = 0.25, delta = 0.26 \n",
      "18317/18317 [==============================] - 267s 15ms/sample - loss: 2.4523 - starts_0_loss: 2.4845 - stops_0_loss: 2.3963 - starts_1_loss: 2.4353 - stops_1_loss: 2.4902 - starts_0_accuracy: 0.5889 - stops_0_accuracy: 0.6288 - starts_1_accuracy: 0.6138 - stops_1_accuracy: 0.6422 - val_loss: 2.4680 - val_starts_0_loss: 2.4661 - val_stops_0_loss: 2.4103 - val_starts_1_loss: 2.4820 - val_stops_1_loss: 2.5091 - val_starts_0_accuracy: 0.5748 - val_stops_0_accuracy: 0.6220 - val_starts_1_accuracy: 0.6006 - val_stops_1_accuracy: 0.6246\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 18317 samples, validate on 9158 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "18304/18317 [============================>.] - ETA: 0s - loss: 2.4198 - starts_0_loss: 2.4652 - stops_0_loss: 2.3363 - starts_1_loss: 2.4411 - stops_1_loss: 2.4323 - starts_0_accuracy: 0.5827 - stops_0_accuracy: 0.6355 - starts_1_accuracy: 0.6069 - stops_1_accuracy: 0.6530\n",
      " Loss weights recalibrated to alpha = 0.25, beta = 0.23, gamma = 0.27, delta = 0.25 \n",
      "18317/18317 [==============================] - 578s 32ms/sample - loss: 2.4200 - starts_0_loss: 2.4655 - stops_0_loss: 2.3366 - starts_1_loss: 2.4415 - stops_1_loss: 2.4323 - starts_0_accuracy: 0.5826 - stops_0_accuracy: 0.6355 - starts_1_accuracy: 0.6067 - stops_1_accuracy: 0.6529 - val_loss: 2.3508 - val_starts_0_loss: 2.3576 - val_stops_0_loss: 2.2551 - val_starts_1_loss: 2.4552 - val_stops_1_loss: 2.3301 - val_starts_0_accuracy: 0.6023 - val_stops_0_accuracy: 0.6555 - val_starts_1_accuracy: 0.6034 - val_stops_1_accuracy: 0.6797\n",
      "[INFO]  =============== Validation for FOLD# 2 ===============\n",
      "[INFO] 62.39 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 66.33 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 60.23 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 65.55 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 33.62 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 62.30 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 24.76 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 61.43 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 37.73 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 58.07 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 28.84 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 57.85 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 31.60 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 69.90 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 22.27 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 68.67 \t|| valid \t||"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V37_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V37_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V37_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V37_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (18317, 108) (18317, 108)\n",
      "[INFO] Prediction shape for validation data:  (9158, 108) (9158, 108)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  17062 out of 18317\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  8565 out of 9158\n",
      "[INFO] Training Jaccard Score:  0.6806910109882429\n",
      "[INFO] Validation Jaccard Score:  0.6743926937201156\n",
      "[INFO] Training for fold: 2 finished at Sun Jun 14 17:08:30 2020\n",
      "Sun Jun 14 17:08:30 2020\n"
     ]
    }
   ],
   "source": [
    "for num, (t_index, v_index) in enumerate(span_kf.split(X_span, Y_span_stops)):\n",
    "    print(\"[INFO] ==================== FOLD#\", num, \"====================\")\n",
    "    \n",
    "    if num > 0:\n",
    "        del span_history\n",
    "        del span_detection_model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    \n",
    "    span_detection_model = build_model()\n",
    "    \n",
    "    alpha = K.variable(0.25)\n",
    "    beta = K.variable(0.25)\n",
    "    gamma = K.variable(0.25)\n",
    "    delta = K.variable(0.75)\n",
    "    \n",
    "    span_mcp = ModelCheckpoint(filepath=RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\", monitor='val_loss',\n",
    "                               verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "    \n",
    "    span_csvl = CSVLogger(filename=RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(num)+\".csv\",\n",
    "                          separator=\",\",\n",
    "                          append=True)\n",
    "    \n",
    "    print(\"[INFO] Training only the final layers at higher learning rates.\")\n",
    "    span_detection_model.layers[3].trainable = False\n",
    "    adam = Adam(learning_rate=MAX_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=LABEL_SMOOTHING_PARAM),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":alpha,\"stops_0\":beta,\"starts_1\":gamma,\"stops_1\":delta})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[0],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl, LossWeightAdjust(alpha=alpha, beta=beta, gamma=gamma, delta=delta)])\n",
    "    \n",
    "    print(\"[INFO] Training only the final layers at lower learning rates.\")\n",
    "    adam = Adam(learning_rate=MID_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=LABEL_SMOOTHING_PARAM),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":alpha,\"stops_0\":beta,\"starts_1\":gamma,\"stops_1\":delta})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[1],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl, LossWeightAdjust(alpha=alpha, beta=beta, gamma=gamma, delta=delta)])\n",
    "    \n",
    "    print(\"[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\")\n",
    "    span_detection_model.layers[3].trainable = True\n",
    "    adam = Adam(learning_rate=MIN_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=LABEL_SMOOTHING_PARAM),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":alpha,\"stops_0\":beta,\"starts_1\":gamma,\"stops_1\":delta})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[2],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl, LossWeightAdjust(alpha=alpha, beta=beta, gamma=gamma, delta=delta)])\n",
    "    \n",
    "    # Loading best weights per fold\n",
    "    span_detection_model.load_weights(RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\")\n",
    "    \n",
    "    pred_train = span_detection_model.predict(x = {\"att_flags\":X_span_att[t_index],\n",
    "                                                   \"words\":X_span[t_index],\n",
    "                                                   \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                              batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "    pred_val = span_detection_model.predict(x = {\"att_flags\":X_span_att[v_index],\n",
    "                                                 \"words\":X_span[v_index],\n",
    "                                                 \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                            batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    pred_starts_train, pred_stops_train = (pred_train[0]/2.0 + pred_train[2]/2.0), (pred_train[1]/2.0 + pred_train[3]/2.0)\n",
    "    pred_starts_val, pred_stops_val = (pred_val[0]/2.0 + pred_val[2]/2.0), (pred_val[1]/2.0 + pred_val[3]/2.0)\n",
    "    \n",
    "    # Accumulate test results after training every fold\n",
    "    pred_test_fold = span_detection_model.predict(x = {\"att_flags\":X_span_att_test,\n",
    "                                                       \"words\":X_span_test,\n",
    "                                                       \"token_ids\":np.zeros_like(X_span_att_test)},\n",
    "                                                  batch_size=PREDICT_BATCH_SIZE)\n",
    "    if num==0:\n",
    "        pred_test = []\n",
    "        pred_test.append(pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test.append(pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    else:\n",
    "        pred_test[0] += (pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test[1] += (pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    \n",
    "    # Tabulate\n",
    "    preds = {\n",
    "        \"train\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_span_starts[t_index].argmax(axis=1),\n",
    "                \"y_pred\":pred_train[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_span_stops[t_index].argmax(axis=1),\n",
    "                \"y_pred\":pred_train[1].argmax(axis=1)\n",
    "            }\n",
    "        },\n",
    "        \"valid\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_span_starts[v_index].argmax(axis=1),\n",
    "                \"y_pred\":pred_val[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_span_stops[v_index].argmax(axis=1),\n",
    "                \"y_pred\":pred_val[1].argmax(axis=1)\n",
    "            }        \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print_metrics(pred_dict=preds)\n",
    "\n",
    "    print(\"[INFO] Prediction shape for training data: \", pred_starts_train.shape, pred_stops_train.shape)\n",
    "    print(\"[INFO] Prediction shape for validation data: \", pred_starts_val.shape, pred_stops_val.shape)\n",
    "\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for training data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_train.argmax(axis=1),\n",
    "                                  pred_stops_train.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_train.shape[0])\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for validation data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_val.argmax(axis=1),\n",
    "                                  pred_stops_val.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_val.shape[0],)\n",
    "\n",
    "    pred_words_train = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[t_index],\n",
    "                                                                                                                                   pred_starts_train.argmax(axis=1),\n",
    "                                                                                                                                   pred_stops_train.argmax(axis=1))]\n",
    "    \n",
    "    pred_words_val = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[v_index],\n",
    "                                                                                                                                 pred_starts_val.argmax(axis=1),\n",
    "                                                                                                                                 pred_stops_val.argmax(axis=1))]\n",
    "\n",
    "    print(\"[INFO] Training Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in t_index],\n",
    "                                                          pred_words_train)]))\n",
    "    print(\"[INFO] Validation Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in v_index],\n",
    "                                                          pred_words_val)]))\n",
    "    print(\"[INFO] Training for fold:\", num, \"finished at\", ctime(time()))\n",
    "\n",
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Prediction shape for testing data:  (3534, 108) (3534, 108)\n"
     ]
    }
   ],
   "source": [
    "pred_starts_test, pred_stops_test = pred_test[0]/NUM_FOLDS, pred_test[1]/NUM_FOLDS\n",
    "print(\"[INFO] Prediction shape for testing data: \", pred_starts_test.shape, pred_stops_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal predictions (StartIndex less than EndIndex) for testing data: 3285 out of 3534\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal predictions (StartIndex less than EndIndex) for testing data:\",\n",
    "      sum([s<e for s,e in zip(pred_starts_test.argmax(axis=1),\n",
    "                              pred_stops_test.argmax(axis=1))]), \n",
    "      \"out of\",\n",
    "      pred_starts_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words_test = [\n",
    "    post_process(tokenizer.decode(t[s:e+1])) if s<e else post_process(tokenizer.decode(t[e:])) for t,s,e in zip(X_span_test,\n",
    "                                                                                                              pred_starts_test.argmax(axis=1),\n",
    "                                                                                                              pred_stops_test.argmax(axis=1))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " yes! im down to 50% full on my dvr  i was at 98% like 3 days ago... lol i swear if i didnt have a dvr i would never watch tv   neutral \n",
      "1\n",
      "38\n",
      "yes! im down to 50% full on my dvr i was at 98% like 3 days ago... lol i swear if i didnt have a dvr i would never watch tv\n"
     ]
    }
   ],
   "source": [
    "check_idx = 158\n",
    "#print([[t,i,j,k] for t,i,j,k in zip(tokenizer.decode(),X_test[check_idx],pred_starts_test[check_idx],pred_stops_test[check_idx])])\n",
    "print(tokenizer.decode(X_span_test[check_idx]))\n",
    "print(pred_starts_test.argmax(axis=1)[check_idx])\n",
    "print(pred_stops_test.argmax(axis=1)[check_idx])\n",
    "print(post_process(tokenizer.decode(X_span_test[check_idx][pred_starts_test.argmax(axis=1)[check_idx]:1+pred_stops_test.argmax(axis=1)[check_idx]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span['selected_text'] = pred_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[\"selected_text\"] = np.where(test_df_span[\"sentiment\"] == \"neutral\",\n",
    "                                         test_df_span[\"text\"],\n",
    "                                         test_df_span[\"selected_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[[\"textID\", \"selected_text\"]].to_csv(RESULTS_DIR+\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3051</th>\n",
       "      <td>Happy Mothers` Day to my mom and all the mothers in the world</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy mothers` day to my mom and all the mothers in the world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>thanks for a nice blog post!  should however be given some creds since he has done at least half of the work on it</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks for a nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>It depends on your goals &amp; how much you want to spend  Cannondale, Specialized and Cervelo are all good brands.</td>\n",
       "      <td>positive</td>\n",
       "      <td>it depends on your goals &amp; how much you want to spend cannondale, specialized and cervelo are all good brands.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3101</th>\n",
       "      <td>Oooh you just spoiled my teenage fantasy</td>\n",
       "      <td>negative</td>\n",
       "      <td>oooh you just spoiled my teenage fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>Driving home to trade cars  hopefully it makes it! http://myloc.me/21SL</td>\n",
       "      <td>positive</td>\n",
       "      <td>hopefully it makes it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>Conference call with HP.  They gave me an invalid pass code so I can`t attend</td>\n",
       "      <td>negative</td>\n",
       "      <td>conference call with hp. they gave me an invalid pass code so i can`t attend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>I`m the bird with broken wings, she`s the song i love to sing  you know who you are ****</td>\n",
       "      <td>positive</td>\n",
       "      <td>i`m the bird with broken wings, she`s the song i love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>the day goes on and on...i think im gonna write a song about it! still thinking itï¿½s impossible for me to get a true friend  why????</td>\n",
       "      <td>negative</td>\n",
       "      <td>the day goes on and on...i think im gonna write a song about it! still thinking itï¿½s impossible for me to get a true friend why????</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>guns and roses baby! yay!  was hopin you would blip.fm it</td>\n",
       "      <td>positive</td>\n",
       "      <td>guns and roses baby! yay!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>my stomach hurts</td>\n",
       "      <td>negative</td>\n",
       "      <td>my stomach hurts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>oh mann... Me likey that!! But sadly I`m not bein auctioned on #twpp tonight</td>\n",
       "      <td>negative</td>\n",
       "      <td>oh mann... me likey that!! but sadly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>i always have those for my Champions League parties  Tis awesome</td>\n",
       "      <td>positive</td>\n",
       "      <td>awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>'You guys could have fun in a cardboard box'.. I miss you already, bro.</td>\n",
       "      <td>negative</td>\n",
       "      <td>i miss you already, bro.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>I have been playing skate for two hours. Now i need to get out and actually skate. But it`s too late</td>\n",
       "      <td>negative</td>\n",
       "      <td>i have been playing skate for two hours. now i need to get out and actually skate. but it`s too late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>http://twitpic.com/4wgoq - The Beach was phenomenal 2day</td>\n",
       "      <td>positive</td>\n",
       "      <td>phenomenal 2day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>maybe going to see the hannah montana movie todaaaay  i`ve seen it one time before, but can`t wait to see it again!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>maybe going to see the hannah montana movie todaaaay i`ve seen it one time before, but can`t wait to see it again!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>@_supernatural_ http://twitpic.com/66l83 - I really miss her.</td>\n",
       "      <td>negative</td>\n",
       "      <td>miss her.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3482</th>\n",
       "      <td>Goodnight tweeps.</td>\n",
       "      <td>positive</td>\n",
       "      <td>goodnight tweeps.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>Test another update... sorry for bothering all of u guys</td>\n",
       "      <td>negative</td>\n",
       "      <td>sorry for bothering all of u guys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>Pulled in all directions and not knowing where to go</td>\n",
       "      <td>negative</td>\n",
       "      <td>pulled in all directions and not knowing where to go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>I miss u like cotton candy  &lt;3</td>\n",
       "      <td>negative</td>\n",
       "      <td>i miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>I hope you didn`t take that quote personally!! I`ve been throwing them out here and there all day-guess I shoulda waited a bit</td>\n",
       "      <td>negative</td>\n",
       "      <td>i hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>what a beautiful day not to got to my first class</td>\n",
       "      <td>positive</td>\n",
       "      <td>beautiful day not to got to my first class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788</th>\n",
       "      <td>I am so bored, i really don`t know what to do!</td>\n",
       "      <td>negative</td>\n",
       "      <td>bored, i really don`t know what to do!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>ack! I just read about your show  that`s horrible!!! No accounting for taste.</td>\n",
       "      <td>negative</td>\n",
       "      <td>horrible!!! no accounting for taste.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        text  \\\n",
       "3051                                                                           Happy Mothers` Day to my mom and all the mothers in the world   \n",
       "2242                      thanks for a nice blog post!  should however be given some creds since he has done at least half of the work on it   \n",
       "694                          It depends on your goals & how much you want to spend  Cannondale, Specialized and Cervelo are all good brands.   \n",
       "3101                                                                                                Oooh you just spoiled my teenage fantasy   \n",
       "2483                                                                 Driving home to trade cars  hopefully it makes it! http://myloc.me/21SL   \n",
       "1378                                                           Conference call with HP.  They gave me an invalid pass code so I can`t attend   \n",
       "3139                                                I`m the bird with broken wings, she`s the song i love to sing  you know who you are ****   \n",
       "1857  the day goes on and on...i think im gonna write a song about it! still thinking itï¿½s impossible for me to get a true friend  why????   \n",
       "2227                                                                               guns and roses baby! yay!  was hopin you would blip.fm it   \n",
       "1305                                                                                                                        my stomach hurts   \n",
       "2390                                                            oh mann... Me likey that!! But sadly I`m not bein auctioned on #twpp tonight   \n",
       "115                                                                         i always have those for my Champions League parties  Tis awesome   \n",
       "592                                                                  'You guys could have fun in a cardboard box'.. I miss you already, bro.   \n",
       "2351                                    I have been playing skate for two hours. Now i need to get out and actually skate. But it`s too late   \n",
       "3216                                                                                http://twitpic.com/4wgoq - The Beach was phenomenal 2day   \n",
       "3462                    maybe going to see the hannah montana movie todaaaay  i`ve seen it one time before, but can`t wait to see it again!!   \n",
       "1485                                                                           @_supernatural_ http://twitpic.com/66l83 - I really miss her.   \n",
       "3482                                                                                                                       Goodnight tweeps.   \n",
       "3048                                                                                Test another update... sorry for bothering all of u guys   \n",
       "2231                                                                                    Pulled in all directions and not knowing where to go   \n",
       "2618                                                                                                          I miss u like cotton candy  <3   \n",
       "1522          I hope you didn`t take that quote personally!! I`ve been throwing them out here and there all day-guess I shoulda waited a bit   \n",
       "886                                                                                        what a beautiful day not to got to my first class   \n",
       "2788                                                                                          I am so bored, i really don`t know what to do!   \n",
       "1561                                                           ack! I just read about your show  that`s horrible!!! No accounting for taste.   \n",
       "\n",
       "     sentiment  \\\n",
       "3051  positive   \n",
       "2242  positive   \n",
       "694   positive   \n",
       "3101  negative   \n",
       "2483  positive   \n",
       "1378  negative   \n",
       "3139  positive   \n",
       "1857  negative   \n",
       "2227  positive   \n",
       "1305  negative   \n",
       "2390  negative   \n",
       "115   positive   \n",
       "592   negative   \n",
       "2351  negative   \n",
       "3216  positive   \n",
       "3462  positive   \n",
       "1485  negative   \n",
       "3482  positive   \n",
       "3048  negative   \n",
       "2231  negative   \n",
       "2618  negative   \n",
       "1522  negative   \n",
       "886   positive   \n",
       "2788  negative   \n",
       "1561  negative   \n",
       "\n",
       "                                                                                                                              selected_text  \n",
       "3051                                                                          happy mothers` day to my mom and all the mothers in the world  \n",
       "2242                                                                                                                      thanks for a nice  \n",
       "694                          it depends on your goals & how much you want to spend cannondale, specialized and cervelo are all good brands.  \n",
       "3101                                                                                               oooh you just spoiled my teenage fantasy  \n",
       "2483                                                                                                                 hopefully it makes it!  \n",
       "1378                                                           conference call with hp. they gave me an invalid pass code so i can`t attend  \n",
       "3139                                                                                  i`m the bird with broken wings, she`s the song i love  \n",
       "1857  the day goes on and on...i think im gonna write a song about it! still thinking itï¿½s impossible for me to get a true friend why????  \n",
       "2227                                                                                                              guns and roses baby! yay!  \n",
       "1305                                                                                                                       my stomach hurts  \n",
       "2390                                                                                                   oh mann... me likey that!! but sadly  \n",
       "115                                                                                                                                 awesome  \n",
       "592                                                                                                                i miss you already, bro.  \n",
       "2351                                   i have been playing skate for two hours. now i need to get out and actually skate. but it`s too late  \n",
       "3216                                                                                                                        phenomenal 2day  \n",
       "3462                    maybe going to see the hannah montana movie todaaaay i`ve seen it one time before, but can`t wait to see it again!!  \n",
       "1485                                                                                                                              miss her.  \n",
       "3482                                                                                                                      goodnight tweeps.  \n",
       "3048                                                                                                      sorry for bothering all of u guys  \n",
       "2231                                                                                   pulled in all directions and not knowing where to go  \n",
       "2618                                                                                                                                 i miss  \n",
       "1522                                                                                                                                 i hope  \n",
       "886                                                                                              beautiful day not to got to my first class  \n",
       "2788                                                                                                 bored, i really don`t know what to do!  \n",
       "1561                                                                                                   horrible!!! no accounting for taste.  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_span.loc[test_df_span.sentiment!=\"neutral\"][[\"text\", \"sentiment\",\"selected_text\"]].sample(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
