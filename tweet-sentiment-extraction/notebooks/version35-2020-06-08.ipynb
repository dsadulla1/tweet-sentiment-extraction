{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V35\" # Better start stop indices No removal of samples, Using my tokenizer, Roberta weights, Sentiment(LeakyKFold), SpanNoNeutral(NoLeakKFlod) With Label Smoothing\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 16\n",
    "PREDICT_BATCH_SIZE = 512\n",
    "DROPOUT = 0.3\n",
    "LABEL_SMOOTHING_PARAM = 0.2\n",
    "\n",
    "RUN_ON_SAMPLE = False\n",
    "EXCLUDE_NEUTRAL_CLASS = True\n",
    "SENTIMENT_MAX_LR = 5e-4\n",
    "SENTIMENT_MIN_LR = 5e-6\n",
    "SENTIMENT_NUM_EPOCHS = [4, 2]\n",
    "MAX_LR = 5e-3 #3e-5\n",
    "MID_LR = 1e-4 #3e-5\n",
    "MIN_LR = 1e-6 #3e-5\n",
    "NUM_EPOCHS = [4, 4, 1]\n",
    "NUM_FOLDS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"../working/\"\n",
    "DATA_DIR = \"../input/tweet-sentiment-extraction/\"\n",
    "MODEL_DIR = \"../working/models/\"\n",
    "EXT_MODEL_DIR = \"../input/robertamodelobjects/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, Layer, LeakyReLU, Subtract\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    a = set(str1)\n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeded_value = 88888\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  8 17:02:44 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0', '2.9.0', '0.7.0']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "if not os.path.exists(MODEL_DIR+\"tokenizers\"):\n",
    "    os.mkdir(MODEL_DIR+\"tokenizers\")\n",
    "\n",
    "if not os.path.exists(MODEL_DIR+\"tokenizers/roberta_tokenizer\"):\n",
    "    os.mkdir(MODEL_DIR+\"tokenizers/roberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\"  target=\"_blank\"><h2 id=\"limiting_gpu_memory_growth\" data-text=\"Limiting GPU memory growth\" tabindex=\"0\">Limiting GPU memory growth</h2></a>\n",
    "<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\"><code translate=\"no\" dir=\"ltr\">CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the <code translate=\"no\" dir=\"ltr\">tf.config.experimental.set_visible_devices</code> method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it okay to exclude neutral text from models and force it later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVERAGE: 0.5334592552617378\n",
      "MEAN JACCARD: 0.9808316180492903\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>c68863089e</td>\n",
       "      <td>_tifullyTragic ... in London last night so give it a few hours &amp; I`m sure a few will appear</td>\n",
       "      <td>in London last night so give it a few hours &amp; I`m sure a few will appear</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2211</th>\n",
       "      <td>edd2aceb1c</td>\n",
       "      <td>who sings `I Remember`? i alwaysss hear it on Radio 1 but never catch the artist</td>\n",
       "      <td>who sings `I Remember`? i alwaysss hear it on Radio 1 but never catch the artist</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8286</th>\n",
       "      <td>ee53bf0e43</td>\n",
       "      <td>this time there is a theme and it is 'purple'</td>\n",
       "      <td>this time there is a theme and it is 'purple'</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4240</th>\n",
       "      <td>2736a522fa</td>\n",
       "      <td>why isnt everyone with you?</td>\n",
       "      <td>why isnt everyone with you?</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10069</th>\n",
       "      <td>8ba7a1720e</td>\n",
       "      <td>and by the way it`s short stack, not sway sway  sway sway baby is a song of theirs</td>\n",
       "      <td>and by the way it`s short stack, not sway sway  sway sway baby is a song of theirs</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID  \\\n",
       "4896   c68863089e   \n",
       "2211   edd2aceb1c   \n",
       "8286   ee53bf0e43   \n",
       "4240   2736a522fa   \n",
       "10069  8ba7a1720e   \n",
       "\n",
       "                                                                                              text  \\\n",
       "4896   _tifullyTragic ... in London last night so give it a few hours & I`m sure a few will appear   \n",
       "2211              who sings `I Remember`? i alwaysss hear it on Radio 1 but never catch the artist   \n",
       "8286                                                 this time there is a theme and it is 'purple'   \n",
       "4240                                                                   why isnt everyone with you?   \n",
       "10069           and by the way it`s short stack, not sway sway  sway sway baby is a song of theirs   \n",
       "\n",
       "                                                                            selected_text  \\\n",
       "4896             in London last night so give it a few hours & I`m sure a few will appear   \n",
       "2211     who sings `I Remember`? i alwaysss hear it on Radio 1 but never catch the artist   \n",
       "8286                                        this time there is a theme and it is 'purple'   \n",
       "4240                                                          why isnt everyone with you?   \n",
       "10069  and by the way it`s short stack, not sway sway  sway sway baby is a song of theirs   \n",
       "\n",
       "      sentiment  \n",
       "4896    neutral  \n",
       "2211    neutral  \n",
       "8286    neutral  \n",
       "4240    neutral  \n",
       "10069   neutral  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\")\n",
    "# cases where neutral's text and selected text columns are the same\n",
    "neutrals = df2.loc[df2.sentiment==\"neutral\"].copy()\n",
    "neutrals = neutrals.reset_index(drop=True)\n",
    "neutrals[\"text\"] = neutrals[\"text\"].astype(str)\n",
    "neutrals[\"selected_text\"] = neutrals[\"selected_text\"].astype(str)\n",
    "print(\"COVERAGE:\", np.sum(np.where((neutrals.text == neutrals.selected_text), 1, 0))/neutrals.shape[0])\n",
    "print(\"MEAN JACCARD:\", np.mean([jaccard(str1=i, str2=j) for i,j in zip(neutrals.selected_text,neutrals.text)]))\n",
    "\n",
    "neutrals.loc[neutrals.text != neutrals.selected_text].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  1\n",
      "selected_text  object  1\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27480, 'selected_text': 22463, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count        27481   \n",
      "unique       27481   \n",
      "top     b797972363   \n",
      "freq             1   \n",
      "\n",
      "                                                                                               text  \\\n",
      "count                                                                                         27480   \n",
      "unique                                                                                        27480   \n",
      "top     Power Outage, door to freezer propped open, 3G ice cream make for slippy floor  M-er F-er!!   \n",
      "freq                                                                                              1   \n",
      "\n",
      "       selected_text sentiment  \n",
      "count          27480     27481  \n",
      "unique         22463         3  \n",
      "top             good   neutral  \n",
      "freq             199     11118  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\")\n",
    "\n",
    "print(pd.concat((df.dtypes, df.isna().sum()), axis=1))\n",
    "print(df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df[i].nunique() for i in df.columns})\n",
    "print(df.describe())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count         3534   \n",
      "unique        3534   \n",
      "top     68a6cf591a   \n",
      "freq             1   \n",
      "\n",
      "                                                              text sentiment  \n",
      "count                                                         3534      3534  \n",
      "unique                                                        3534         3  \n",
      "top     Anybody else experiencing painful slowdowns with facebook?   neutral  \n",
      "freq                                                             1      1430  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(DATA_DIR+\"test.csv\", encoding=\"utf8\")\n",
    "print(pd.concat((test_df.dtypes, test_df.isna().sum()), axis=1))\n",
    "print(test_df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df[i].nunique() for i in test_df.columns})\n",
    "print(test_df.describe())\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>c77717b103</td>\n",
       "      <td>I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½</td>\n",
       "      <td>I love to!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>28dbada620</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "44   c77717b103   \n",
       "192  28dbada620   \n",
       "\n",
       "                                                                                                       text  \\\n",
       "44    I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½   \n",
       "192                                *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "                                                                selected_text  \\\n",
       "44                                                                 I love to!   \n",
       "192  *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "    sentiment  \n",
       "44   positive  \n",
       "192   neutral  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>7223fdccc2</td>\n",
       "      <td>tikcets are only ï¿½91...each...BUT I SO WANT TO GO</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>43ad351369</td>\n",
       "      <td>AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "145  7223fdccc2   \n",
       "618  43ad351369   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "145                                                                                             tikcets are only ï¿½91...each...BUT I SO WANT TO GO   \n",
       "618  AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s   \n",
       "\n",
       "    sentiment  \n",
       "145  positive  \n",
       "618  positive  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[test_df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"set\"], test_df[\"set\"] = \"train\", \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create smaller sample for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_ON_SAMPLE:\n",
    "    df = df.sample(500).reset_index(drop=True)\n",
    "    test_df = test_df.sample(500).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine datasets for pretraining using sentiment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31015, 3)\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat((df[[\"text\",\"set\",\"sentiment\"]],\n",
    "                  test_df[[\"text\",\"set\",\"sentiment\"]]), axis=0)\n",
    "data[\"text\"] = data[\"text\"].astype(str)\n",
    "data = data.sample(frac=1.0).reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment count in combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">test</th>\n",
       "      <th>negative</th>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">train</th>\n",
       "      <th>negative</th>\n",
       "      <td>7781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>11118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>8582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  text\n",
       "set   sentiment       \n",
       "test  negative    1001\n",
       "      neutral     1430\n",
       "      positive    1103\n",
       "train negative    7781\n",
       "      neutral    11118\n",
       "      positive    8582"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"set\",\"sentiment\"])[[\"text\"]].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file='../input/robertamodelobjects/vocab.json',\n",
    "                                             merges_file='../input/robertamodelobjects/merges.txt',                                         \n",
    "                                             add_prefix_space=True,\n",
    "                                             lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../input/robertamodelobjects/special_tokens_map.json\") as f:\n",
    "    special_tokens = json.load(f)\n",
    "\n",
    "tokenizer.add_special_tokens([i for i in special_tokens.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for sentiment detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_addspace(text:str) -> str:\n",
    "    text = text.lower()\n",
    "    text = \" \" + text.strip(\" \") + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text_mod\"] = data.apply(lambda x: trim_addspace(x.text), axis=1)\n",
    "data[\"text_mod\"] = \"<s>\"  + data[\"text_mod\"] + \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lookup = {\"positive\":2,\"neutral\":1,\"negative\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': (31015, 100), 'X_att': (31015, 100), 'Y': (31015,), 'VOCAB_SIZE': 50265, 'MAX_SEQ_LEN': 100}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentiment(text_series=data.text_mod.tolist(), sentiment_series=data.sentiment):\n",
    "\n",
    "    X_tokens = tokenizer.encode_batch(text_series)\n",
    "\n",
    "    X = [i.ids for i in X_tokens]\n",
    "    MAX_SEQ_LEN = max([len(i) for i in X])\n",
    "    X = pad_sequences(X, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "    X_att = [i.attention_mask for i in X_tokens]\n",
    "    X_att = pad_sequences(X_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "    Y = sentiment_series.apply(lambda x: sentiment_lookup[x]).values\n",
    "\n",
    "    VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "\n",
    "    print({\n",
    "        \"X\":X.shape,\n",
    "        \"X_att\":X_att.shape,\n",
    "        \"Y\":Y.shape,\n",
    "        \"VOCAB_SIZE\":VOCAB_SIZE,\n",
    "        \"MAX_SEQ_LEN\":MAX_SEQ_LEN\n",
    "    })\n",
    "    \n",
    "    return X_tokens, X, X_att, Y, VOCAB_SIZE, MAX_SEQ_LEN\n",
    "\n",
    "X_sent_tokens, X_sent, X_sent_att, Y_sent, VOCAB_SIZE, MAX_SEQ_LEN_SENT = preprocess_sentiment(**{\n",
    "    \"text_series\" : data.text.tolist(),\n",
    "    \"sentiment_series\" : data.sentiment\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  0\n",
      "selected_text  object  0\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27481, 'selected_text': 22464, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count        27481   \n",
      "unique       27481   \n",
      "top     b797972363   \n",
      "freq             1   \n",
      "\n",
      "                                                                                               text  \\\n",
      "count                                                                                         27481   \n",
      "unique                                                                                        27481   \n",
      "top     Power Outage, door to freezer propped open, 3G ice cream make for slippy floor  M-er F-er!!   \n",
      "freq                                                                                              1   \n",
      "\n",
      "       selected_text sentiment  \n",
      "count          27481     27481  \n",
      "unique         22464         3  \n",
      "top             good   neutral  \n",
      "freq             199     11118  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\").fillna('')\n",
    "\n",
    "print(pd.concat((df_span.dtypes, df_span.isna().sum()), axis=1))\n",
    "print(df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df_span[i].nunique() for i in df_span.columns})\n",
    "print(df_span.describe())\n",
    "df_span.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count         3534   \n",
      "unique        3534   \n",
      "top     68a6cf591a   \n",
      "freq             1   \n",
      "\n",
      "                                                              text sentiment  \n",
      "count                                                         3534      3534  \n",
      "unique                                                        3534         3  \n",
      "top     Anybody else experiencing painful slowdowns with facebook?   neutral  \n",
      "freq                                                             1      1430  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_span = pd.read_csv(DATA_DIR+\"test.csv\", encoding=\"utf8\").fillna('')\n",
    "print(pd.concat((test_df_span.dtypes, test_df_span.isna().sum()), axis=1))\n",
    "print(test_df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df_span[i].nunique() for i in test_df_span.columns})\n",
    "print(test_df_span.describe())\n",
    "test_df_span.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text\"] = df_span[\"text\"].astype(str)\n",
    "df_span[\"selected_text\"] = df_span[\"selected_text\"].astype(str)\n",
    "test_df_span[\"text\"] = test_df_span[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_addspace(text:str) -> str:\n",
    "    text = text.lower()\n",
    "    text = \" \" + text.strip(\" \") + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(text:str, selected_text:str) -> (str, str, int, int):\n",
    "    \n",
    "    text, selected_text = text.lower(), selected_text.lower()\n",
    "    \n",
    "    text = trim_addspace(text)\n",
    "    \n",
    "    substring_ = re.findall(pattern=\"\\\\s[^\\s]*?\"+re.escape(selected_text)+\"[^\\s]*?\\\\s\", string=text)[0]\n",
    "    \n",
    "    return pd.Series([text, \" \"+substring_.strip(\" \"), text.find(substring_), len(substring_) + text.find(substring_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[[\"text_mod\", \"selected_text_mod\", \"start\", \"stop\"]] = df_span[['text','selected_text']].apply(lambda x: find_indices(x.text, x.selected_text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'textID': '4eac33d1c0',\n",
       " 'text': ' wish we could come see u on Denver  husband lost his job and can`t afford it',\n",
       " 'selected_text': 'd lost',\n",
       " 'sentiment': 'negative',\n",
       " 'text_mod': ' wish we could come see u on denver  husband lost his job and can`t afford it ',\n",
       " 'selected_text_mod': ' husband lost',\n",
       " 'start': 36,\n",
       " 'stop': 50}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span.iloc[27476].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span['text_mod'] = test_df_span['text'].apply(trim_addspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'textID': {12154: 'adfbcc6806'},\n",
       " 'text': {12154: 'i wanna see `up` tonight, but no one will go with me. whhhyyy'},\n",
       " 'selected_text': {12154: 'but no one will go with me.'},\n",
       " 'sentiment': {12154: 'negative'},\n",
       " 'text_mod': {12154: ' i wanna see `up` tonight, but no one will go with me. whhhyyy '},\n",
       " 'selected_text_mod': {12154: ' but no one will go with me.'},\n",
       " 'start': {12154: 26},\n",
       " 'stop': {12154: 55}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span.loc[df_span.text_mod.str.contains(\"tonight, but no one will go\")].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"sentiment_code\"] = df_span[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments = df_span[\"sentiment_code\"].cat.codes.values\n",
    "\n",
    "test_df_span[\"sentiment_code\"] = test_df_span[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments_test = test_df_span[\"sentiment_code\"].cat.codes.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neutral': [7974], 'negative': [2430], 'positive': [1313]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{t:tokenizer.encode(\" \"+t).ids for t in df_span.sentiment.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text_mod\"] = \"<s>\" + df_span['text_mod'] + \"</s> </s> \" + df_span.sentiment + \" </s>\"\n",
    "test_df_span[\"text_mod\"] = \"<s>\" + test_df_span['text_mod'] + \"</s> </s> \" + test_df_span.sentiment + \" </s>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclusions for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCLUDE_NEUTRAL_CLASS: (16363, 9)\n"
     ]
    }
   ],
   "source": [
    "if EXCLUDE_NEUTRAL_CLASS:\n",
    "    df_span = df_span.loc[df_span.sentiment!=\"neutral\"].copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"EXCLUDE_NEUTRAL_CLASS:\", df_span.shape)\n",
    "\n",
    "\n",
    "if RUN_ON_SAMPLE:\n",
    "    df_span = df_span.sample(1000).copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"Train RUN_ON_SAMPLE\", df_span.shape)\n",
    "    test_df_span = test_df_span.sample(1000).copy()\n",
    "    test_df_span = test_df_span.reset_index(drop=True)\n",
    "    print(\"Test  RUN_ON_SAMPLE\", test_df_span.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_tokens = tokenizer.encode_batch(df_span.text_mod.tolist())\n",
    "Y_span_tokens = tokenizer.encode_batch(df_span.selected_text_mod.tolist())\n",
    "X_span_tokens_test = tokenizer.encode_batch(test_df_span.text_mod.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = [i.ids for i in X_span_tokens]\n",
    "Y_span = [i.ids for i in Y_span_tokens]\n",
    "X_span_test = [i.ids for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_att = [i.attention_mask for i in X_span_tokens]\n",
    "Y_span_att = [i.attention_mask for i in Y_span_tokens] # Useless\n",
    "X_span_att_test = [i.attention_mask for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN_SPAN = max([len(i) for i in X_span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremities(l_string, s_string, print_it=False):\n",
    "    len_l = len(l_string)\n",
    "    len_s = len(s_string)\n",
    "    \n",
    "    for i in range(len_l - len_s + 1):\n",
    "        if (i + len_s) <= len_l:\n",
    "            substring = l_string[i:i+len_s]\n",
    "            if substring == s_string:\n",
    "                if print_it:\n",
    "                    print(l_string)\n",
    "                    print(substring)\n",
    "                    print(i, i+len_s, substring)\n",
    "                \n",
    "                start_vector, end_vector = [0] * len_l, [0] * len_l\n",
    "                start_vector[i], end_vector[i+len_s-1] = 1, 1\n",
    "                \n",
    "                return (start_vector, end_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16363 \t: #Processed\n",
      "2 \t: # of Anomalies\n"
     ]
    }
   ],
   "source": [
    "Y_span_starts, Y_span_stops = [], []\n",
    "anomaly_idx, counter = [], 0\n",
    "for num, (i,j) in enumerate(zip(X_span_tokens, Y_span_tokens)):\n",
    "    x,y = i.ids, j.ids\n",
    "    try:\n",
    "        s,e = get_extremities(x, y)\n",
    "        Y_span_starts.append(s)\n",
    "        Y_span_stops.append(e)\n",
    "    except TypeError as t:\n",
    "        counter += 1\n",
    "        anomaly_idx.append(num)\n",
    "        Y_span_starts.append([0]*15)\n",
    "        Y_span_stops.append([0]*15)\n",
    "print(num + 1, \"\\t: #Processed\")\n",
    "\n",
    "print(counter,\"\\t: # of Anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Luck13 they are SO cute\n",
      "SO cute\n",
      "[['<s>', 0, 0, 0], ['Ġ_', 18134, 0, 0], ['luck', 20540, 0, 0], ['13', 1558, 0, 0], ['Ġthey', 51, 0, 0], ['Ġare', 32, 0, 0], ['Ġso', 98, 1, 0], ['Ġcute', 11962, 0, 1], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0], ['Ġpositive', 1313, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0]]\n",
      "[[98, 'Ġso'], [11962, 'Ġcute']]\n"
     ]
    }
   ],
   "source": [
    "check_idx = 758\n",
    "print(df_span.text[check_idx])\n",
    "print(df_span.selected_text[check_idx])\n",
    "print([[i,j,k,l] for i,j,k,l in zip(X_span_tokens[check_idx].tokens,\n",
    "                                    X_span_tokens[check_idx].ids,\n",
    "                                    Y_span_starts[check_idx],\n",
    "                                    Y_span_stops[check_idx])])\n",
    "print([[i,j] for i,j in zip(Y_span_tokens[check_idx].ids,\n",
    "                            Y_span_tokens[check_idx].tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "\n",
    "Y_span_starts = pad_sequences(Y_span_starts, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")#.argmax(axis=1)\n",
    "Y_span_stops = pad_sequences(Y_span_stops, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")#.argmax(axis=1)\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAX_SEQ_LEN_SPAN': 78,\n",
      " 'VOCAB_SIZE': 50265,\n",
      " 'X_span': (16363, 78),\n",
      " 'X_span_att': (16363, 78),\n",
      " 'X_span_att_test': (3534, 78),\n",
      " 'X_span_test': (3534, 78),\n",
      " 'Y_span': (16363, 78),\n",
      " 'Y_span_starts': (16363, 78),\n",
      " 'Y_span_stops': (16363, 78)}\n"
     ]
    }
   ],
   "source": [
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \"Y_span_starts\" : Y_span_starts.shape,\n",
    "    \"Y_span_stops\" : Y_span_stops.shape,\n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "    \"VOCAB_SIZE\":VOCAB_SIZE,\n",
    "    \"MAX_SEQ_LEN_SPAN\":MAX_SEQ_LEN_SPAN\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_flag = np.isin(Y_span_stops.argmax(axis=1),\n",
    "                    np.unique(Y_span_stops.argmax(axis=1),\n",
    "                              return_counts=True)[0][np.unique(Y_span_stops.argmax(axis=1),\n",
    "                                                               return_counts=True)[1]>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16359, 16363, 4)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(keep_flag), df_span.shape[0], df_span.shape[0] - sum(keep_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (16363, 78) \t: X  \n",
      " (16363, 78) \t: X_att  \n",
      " (16363, 78) \t: Y  \n",
      " (16363, 78) \t: Y_starts  \n",
      " (16363, 78) \t: Y_stops  \n",
      " (3534, 78) \t: X_test  \n",
      " (3534, 78) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = X_span[keep_flag]\n",
    "X_span_att = X_span_att[keep_flag]\n",
    "Y_span = Y_span[keep_flag]\n",
    "Y_span_starts = Y_span_starts[keep_flag]\n",
    "Y_span_stops = Y_span_stops[keep_flag]\n",
    "X_span_test = X_span_test\n",
    "X_span_att_test = X_span_att_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (16359, 78) \t: X  \n",
      " (16359, 78) \t: X_att  \n",
      " (16359, 78) \t: Y  \n",
      " (16359, 78) \t: Y_starts  \n",
      " (16359, 78) \t: Y_stops  \n",
      " (3534, 78) \t: X_test  \n",
      " (3534, 78) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_span_words = [tokenizer.decode(i) for i in Y_span]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 78, 100)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LEN = max(MAX_SEQ_LEN_SENT, MAX_SEQ_LEN_SPAN)\n",
    "MAX_SEQ_LEN, MAX_SEQ_LEN_SPAN, MAX_SEQ_LEN_SENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X_sent': (31015, 100),\n",
      " 'X_sent_att': (31015, 100),\n",
      " 'X_span': (16359, 100),\n",
      " 'X_span_att': (16359, 100),\n",
      " 'X_span_att_test': (3534, 100),\n",
      " 'X_span_test': (3534, 100),\n",
      " 'Y_span': (16359, 100)}\n"
     ]
    }
   ],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span_starts = pad_sequences(Y_span_starts, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span_stops = pad_sequences(Y_span_stops, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "X_sent = pad_sequences(X_sent, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_sent_att = pad_sequences(X_sent_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "    \n",
    "    \"X_sent\" : X_sent.shape,\n",
    "    \"X_sent_att\" : X_sent_att.shape,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_sequences = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"words\")\n",
    "    input_att_flags = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"att_flags\")\n",
    "    input_token_ids = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"token_ids\")\n",
    "    \n",
    "    config = RobertaConfig.from_pretrained(EXT_MODEL_DIR+'config.json')\n",
    "    roberta_model = TFRobertaModel.from_pretrained(EXT_MODEL_DIR+'tf_model.h5', config=config)\n",
    "    x = roberta_model(inputs=input_sequences, attention_mask=input_att_flags, token_type_ids=input_token_ids)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(DROPOUT)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    output_starts_0 = tf.keras.layers.Activation('softmax', name=\"starts_0\")(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(DROPOUT)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    output_stops_0 = tf.keras.layers.Activation('softmax', name=\"stops_0\")(x2)\n",
    "    \n",
    "    x3 = tf.keras.layers.Dropout(DROPOUT)(x[0]) \n",
    "    x3 = tf.keras.layers.Conv1D(768, 2,padding='same')(x3)\n",
    "    x3 = tf.keras.layers.LeakyReLU()(x3)\n",
    "    x3 = tf.keras.layers.Dense(1)(x3)\n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "    x3 = tf.keras.layers.Dense(3)(x3)\n",
    "    output_sentiment = tf.keras.layers.Activation('softmax', name=\"output_sentiments\")(x3)\n",
    "    \n",
    "    sentiment_model = Model([input_att_flags, input_sequences, input_token_ids], [output_sentiment])\n",
    "    \n",
    "    output_subtract = tf.keras.layers.Subtract()([output_starts_0, output_stops_0])\n",
    "    output_flat = concatenate([output_starts_0, output_stops_0, output_subtract, output_sentiment])\n",
    "    output_starts_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"starts_1\")(output_flat)\n",
    "    output_stops_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"stops_1\")(output_flat)\n",
    "    \n",
    "    span_detection_model = Model([input_att_flags, input_sequences, input_token_ids],\n",
    "                                 [output_starts_0, output_stops_0, output_starts_1, output_stops_1])\n",
    "    \n",
    "    return sentiment_model, span_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model, span_detection_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 100, 768)     0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 100, 768)     1180416     dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 100, 768)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100, 1)       769         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            303         flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_sentiments (Activation)  (None, 3)            0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 125,827,120\n",
      "Trainable params: 125,827,120\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 100, 768)     0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 100, 768)     0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 100, 768)     0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 100, 768)     1180416     dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 100, 768)     1180416     dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 100, 768)     1180416     dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 100, 768)     0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 100, 768)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 100, 768)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100, 1)       769         leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100, 1)       769         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100, 1)       769         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "starts_0 (Activation)           (None, 100)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "stops_0 (Activation)            (None, 100)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            303         flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 100)          0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_sentiments (Activation)  (None, 3)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 303)          0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "                                                                 subtract[0][0]                   \n",
      "                                                                 output_sentiments[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "starts_1 (Dense)                (None, 100)          30400       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "stops_1 (Dense)                 (None, 100)          30400       concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 128,250,290\n",
      "Trainable params: 128,250,290\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "span_detection_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_sentiment(x):\n",
    "    encoded_repr = tokenizer.encode_batch(x.tolist())\n",
    "\n",
    "    sample_text_ids = pad_sequences([i.ids for i in encoded_repr],\n",
    "                                    maxlen=MAX_SEQ_LEN,\n",
    "                                    padding=\"post\")\n",
    "    sample_text_att = pad_sequences([i.attention_mask for i in encoded_repr],\n",
    "                                    maxlen=MAX_SEQ_LEN,\n",
    "                                    padding=\"post\")\n",
    "    pred = sentiment_model.predict({\"att_flags\":sample_text_att,\n",
    "                                    \"words\":sample_text_ids,\n",
    "                                    \"token_ids\":np.zeros_like(sample_text_att)},\n",
    "                                  batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    res = pd.DataFrame({\"predicted_sentiment\":pred.argmax(axis=1)})\n",
    "    \n",
    "    return res.predicted_sentiment.apply(lambda x:[k for k,v in sentiment_lookup.items() if v==x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_results(data):\n",
    "    data[\"predicted_sentiment\"] = infer_sentiment(x=data.text)\n",
    "\n",
    "    print(classification_report(y_true=data.sentiment[data.index.isin(tr_index)],\n",
    "                                y_pred=data.predicted_sentiment[data.index.isin(tr_index)]))\n",
    "\n",
    "    print(classification_report(y_true=data.sentiment[data.index.isin(va_index)],\n",
    "                                y_pred=data.predicted_sentiment[data.index.isin(va_index)]))\n",
    "\n",
    "    print(confusion_matrix(y_true=data.sentiment,\n",
    "                     y_pred=data.predicted_sentiment,\n",
    "                     labels=['positive', 'neutral', 'negative']))\n",
    "\n",
    "    print(confusion_matrix(y_true=data.sentiment,\n",
    "                     y_pred=data.predicted_sentiment,\n",
    "                     labels=['positive', 'neutral', 'negative'],\n",
    "                     normalize=\"all\"))\n",
    "\n",
    "    data[\"set2\"] = np.where(data.index.isin(tr_index), \"train\", \"valid\")\n",
    "    print(data.groupby(\"set2\").apply(lambda x : accuracy_score(y_true=x.sentiment, y_pred=x.predicted_sentiment)))\n",
    "\n",
    "    print(data.groupby(\"set\").apply(lambda x : accuracy_score(y_true=x.sentiment, y_pred=x.predicted_sentiment)))\n",
    "\n",
    "    print(pd.concat({\n",
    "        \"accuracy\" : data.groupby([\"set\", \"set2\"]).apply(lambda x : accuracy_score(y_true=x.sentiment,\n",
    "                                                                                   y_pred=x.predicted_sentiment)),\n",
    "        \"count\" : data.groupby([\"set\", \"set2\"])[\"sentiment\"].count()\n",
    "    }, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,c = np.unique(Y_sent, return_counts=True)\n",
    "cw = class_weight.compute_class_weight('balanced', np.unique(Y_sent), Y_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_kf = KFold(n_splits=NUM_FOLDS,shuffle=True,random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mcp = ModelCheckpoint(filepath=RESULTS_DIR+\"Sentiment_\"+MODEL_PREFIX+\"BestCheckpoint.h5\", monitor='val_loss',\n",
    "                                verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "sentiment_csvl = CSVLogger(filename=RESULTS_DIR+\"Sentiment_\"+MODEL_PREFIX+\"_LossLogs.csv\",\n",
    "                           separator=\",\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ==================== FOLD# 0 ====================\n",
      "Train on 15507 samples, validate on 15508 samples\n",
      "Epoch 1/4\n",
      "15507/15507 [==============================] - 191s 12ms/sample - loss: 1.0447 - accuracy: 0.4570 - val_loss: 0.9220 - val_accuracy: 0.5402\n",
      "Epoch 2/4\n",
      "15507/15507 [==============================] - 174s 11ms/sample - loss: 0.9256 - accuracy: 0.5419 - val_loss: 0.8702 - val_accuracy: 0.5887\n",
      "Epoch 3/4\n",
      "15507/15507 [==============================] - 174s 11ms/sample - loss: 0.8845 - accuracy: 0.5726 - val_loss: 0.8189 - val_accuracy: 0.6216\n",
      "Epoch 4/4\n",
      "15507/15507 [==============================] - 172s 11ms/sample - loss: 0.8610 - accuracy: 0.5936 - val_loss: 0.8237 - val_accuracy: 0.6166\n",
      "Train on 15507 samples, validate on 15508 samples\n",
      "Epoch 1/2\n",
      "15507/15507 [==============================] - 319s 21ms/sample - loss: 0.7336 - accuracy: 0.6859 - val_loss: 0.6456 - val_accuracy: 0.7413\n",
      "Epoch 2/2\n",
      "15508/15508 [==============================] - 317s 20ms/sample - loss: 0.5996 - accuracy: 0.7494 - val_loss: 0.5222 - val_accuracy: 0.7888\n",
      "Epoch 2/2\n",
      "15508/15508 [==============================] - 299s 19ms/sample - loss: 0.5367 - accuracy: 0.7803 - val_loss: 0.5029 - val_accuracy: 0.7993\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.81      0.83      4386\n",
      "     neutral       0.77      0.84      0.80      6244\n",
      "    positive       0.89      0.82      0.86      4878\n",
      "\n",
      "    accuracy                           0.83     15508\n",
      "   macro avg       0.84      0.83      0.83     15508\n",
      "weighted avg       0.83      0.83      0.83     15508\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.77      0.80      4396\n",
      "     neutral       0.74      0.82      0.78      6304\n",
      "    positive       0.87      0.80      0.83      4807\n",
      "\n",
      "    accuracy                           0.80     15507\n",
      "   macro avg       0.81      0.80      0.80     15507\n",
      "weighted avg       0.80      0.80      0.80     15507\n",
      "\n",
      "[[ 7888  1684   113]\n",
      " [  933 10387  1228]\n",
      " [  137  1699  6946]]\n",
      "[[0.25432855 0.05429631 0.0036434 ]\n",
      " [0.03008222 0.33490247 0.03959374]\n",
      " [0.00441722 0.05477995 0.22395615]]\n",
      "set2\n",
      "train    0.827121\n",
      "valid    0.799252\n",
      "dtype: float64\n",
      "set\n",
      "test     0.817487\n",
      "train    0.812634\n",
      "dtype: float64\n",
      "             accuracy  count\n",
      "set   set2                  \n",
      "test  train  0.828555   1744\n",
      "      valid  0.806704   1790\n",
      "train train  0.826940  13764\n",
      "      valid  0.798280  13717\n"
     ]
    }
   ],
   "source": [
    "for num, (tr_index, va_index) in enumerate(sentiment_kf.split(X_sent, Y_sent)):\n",
    "    print(\"[INFO] ==================== FOLD#\", num, \"====================\")\n",
    "\n",
    "    sentiment_model.layers[3].trainable = False\n",
    "    adam = Adam(learning_rate=SENTIMENT_MAX_LR)\n",
    "    sentiment_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                            optimizer=adam,\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "    sent_history = sentiment_model.fit(x={\"att_flags\":X_sent_att[tr_index],\n",
    "                                          \"words\":X_sent[tr_index],\n",
    "                                          \"token_ids\":np.zeros_like(X_sent_att[tr_index])},\n",
    "                                       y={\"output_sentiments\":Y_sent[tr_index]},\n",
    "                                       shuffle=True,\n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       epochs=SENTIMENT_NUM_EPOCHS[0],\n",
    "                                       validation_data=({\"att_flags\":X_sent_att[va_index],\n",
    "                                                         \"words\":X_sent[va_index],\n",
    "                                                         \"token_ids\":np.zeros_like(X_sent_att[va_index])},\n",
    "                                                        {\"output_sentiments\":Y_sent[va_index]}),\n",
    "                                       verbose=1,\n",
    "                                       class_weight=cw,\n",
    "                                       callbacks=[sentiment_mcp, sentiment_csvl])\n",
    "\n",
    "    sentiment_model.layers[3].trainable = True\n",
    "    adam = Adam(learning_rate=SENTIMENT_MIN_LR)\n",
    "    sentiment_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                            optimizer=adam,\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "    sent_history_finetuned = sentiment_model.fit(x={\"att_flags\":X_sent_att[tr_index],\n",
    "                                                    \"words\":X_sent[tr_index],\n",
    "                                                    \"token_ids\":np.zeros_like(X_sent_att[tr_index])},\n",
    "                                                 y={\"output_sentiments\":Y_sent[tr_index]},\n",
    "                                                 shuffle=True,\n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                 epochs=SENTIMENT_NUM_EPOCHS[1],\n",
    "                                                 validation_data=({\"att_flags\":X_sent_att[va_index],\n",
    "                                                                   \"words\":X_sent[va_index],\n",
    "                                                                   \"token_ids\":np.zeros_like(X_sent_att[va_index])},\n",
    "                                                                  {\"output_sentiments\":Y_sent[va_index]}),\n",
    "                                                 verbose=1,\n",
    "                                                 class_weight=cw,\n",
    "                                                 callbacks=[sentiment_mcp, sentiment_csvl])\n",
    "    \n",
    "    get_sentiment_results(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'predicted_sentiment': 'negative',\n",
      "     'text': 'Going out to Miranda shopping centre to spend time with the '\n",
      "             'family, before going away for 2 weeks to Malaysia. Gonna miss '\n",
      "             'them!'},\n",
      " 1: {'predicted_sentiment': 'neutral',\n",
      "     'text': '  Now I need to find the Keynote one! At least I know where to '\n",
      "             'go! #ScreenCastsOnline'},\n",
      " 2: {'predicted_sentiment': 'positive',\n",
      "     'text': 'relaxing night at home with best people'},\n",
      " 3: {'predicted_sentiment': 'negative',\n",
      "     'text': 'Watching Ace of Cakes: LOST edition omfgggg'},\n",
      " 4: {'predicted_sentiment': 'negative',\n",
      "     'text': 'doneeee wheeee hahaaaaaaaa so tired and sleepy  peter u suck not '\n",
      "             'coming to my bday!'}}\n"
     ]
    }
   ],
   "source": [
    "sample_text = data.text.sample(5).tolist()\n",
    "\n",
    "encoded_repr = tokenizer.encode_batch(sample_text)\n",
    "\n",
    "sample_text_ids = pad_sequences([i.ids for i in encoded_repr],\n",
    "                                maxlen=MAX_SEQ_LEN,\n",
    "                                padding=\"post\")\n",
    "sample_text_att = pad_sequences([i.attention_mask for i in encoded_repr],\n",
    "                                maxlen=MAX_SEQ_LEN,\n",
    "                                padding=\"post\")\n",
    "pred = sentiment_model.predict({\"att_flags\":sample_text_att,\n",
    "                                \"words\":sample_text_ids,\n",
    "                                \"token_ids\":np.zeros_like(sample_text_att)})\n",
    "\n",
    "pprint({\n",
    "    num:{\n",
    "        \"text\":i,\n",
    "        \"predicted_sentiment\":[k for k,v in sentiment_lookup.items() if v==j][0]\n",
    "    } for num,(i,j) in enumerate(zip(sample_text, pred.argmax(axis=1)))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 2, 'neutral': 1, 'negative': 0}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>set</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_mod</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "      <th>set2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7520</th>\n",
       "      <td>i know its awful  but never fear i`ve got loads of funds so we can have a drunken catch up post-exams ja?</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>&lt;s&gt; i know its awful  but never fear i`ve got loads of funds so we can have a drunken catch up post-exams ja? &lt;/s&gt;</td>\n",
       "      <td>negative</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9871</th>\n",
       "      <td>ahh naa i dont like rain  nm really friend is comin over then goin 2 my grans</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>&lt;s&gt; ahh naa i dont like rain  nm really friend is comin over then goin 2 my grans &lt;/s&gt;</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13665</th>\n",
       "      <td>good news: finally finished my #EASactive workout that has been paused for 6 hours. bad news: my resistance band is torn</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>&lt;s&gt; good news: finally finished my #easactive workout that has been paused for 6 hours. bad news: my resistance band is torn &lt;/s&gt;</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24207</th>\n",
       "      <td>Not actually managed to purchase anything from the ovi store yet. Constant server error on check out</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>&lt;s&gt; not actually managed to purchase anything from the ovi store yet. constant server error on check out &lt;/s&gt;</td>\n",
       "      <td>negative</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4546</th>\n",
       "      <td>why not now you made me sad I thought you`d be jumping for joy</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>&lt;s&gt; why not now you made me sad i thought you`d be jumping for joy &lt;/s&gt;</td>\n",
       "      <td>negative</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           text  \\\n",
       "7520                  i know its awful  but never fear i`ve got loads of funds so we can have a drunken catch up post-exams ja?   \n",
       "9871                                              ahh naa i dont like rain  nm really friend is comin over then goin 2 my grans   \n",
       "13665  good news: finally finished my #EASactive workout that has been paused for 6 hours. bad news: my resistance band is torn   \n",
       "24207                      Not actually managed to purchase anything from the ovi store yet. Constant server error on check out   \n",
       "4546                                                             why not now you made me sad I thought you`d be jumping for joy   \n",
       "\n",
       "         set sentiment  \\\n",
       "7520   train   neutral   \n",
       "9871   train   neutral   \n",
       "13665  train   neutral   \n",
       "24207  train   neutral   \n",
       "4546   train   neutral   \n",
       "\n",
       "                                                                                                                                text_mod  \\\n",
       "7520                  <s> i know its awful  but never fear i`ve got loads of funds so we can have a drunken catch up post-exams ja? </s>   \n",
       "9871                                              <s> ahh naa i dont like rain  nm really friend is comin over then goin 2 my grans </s>   \n",
       "13665  <s> good news: finally finished my #easactive workout that has been paused for 6 hours. bad news: my resistance band is torn </s>   \n",
       "24207                      <s> not actually managed to purchase anything from the ovi store yet. constant server error on check out </s>   \n",
       "4546                                                             <s> why not now you made me sad i thought you`d be jumping for joy </s>   \n",
       "\n",
       "      predicted_sentiment   set2  \n",
       "7520             negative  valid  \n",
       "9871             negative  train  \n",
       "13665            negative  train  \n",
       "24207            negative  valid  \n",
       "4546             negative  valid  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[(data.sentiment == \"neutral\") & (data.predicted_sentiment == \"negative\")].sample(5) # most incorrect in this cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span Detection Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_FOLDS):\n",
    "    if os.path.exists(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\"):\n",
    "        os.remove(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(pred_dict):\n",
    "    print(\"[INFO] \",\"=\"*15,\"Validation for FOLD#\", num, \"=\"*15)\n",
    "    funcs = [accuracy_score, f1_score, precision_score, recall_score, confusion_matrix]\n",
    "    for f in funcs:\n",
    "        for data_set in [\"train\",\"valid\"]:\n",
    "            for var in [\"starts\", \"stops\"]:\n",
    "                if f in [accuracy_score]:\n",
    "                    res = f(**pred_dict[data_set][var])\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "                elif f in [confusion_matrix]:\n",
    "                    res = f(**pred_dict[data_set][var], labels=np.arange(MAX_SEQ_LEN))\n",
    "                    np.savetxt(X=res, fmt='%i', delimiter=\",\",\n",
    "                               fname=RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                    print(\"[INFO] \\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__, \"\\t||\", \n",
    "                          RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                else:\n",
    "                    res = f(**pred_dict[data_set][var], average=\"macro\")\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "        print(\"=======================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(string):\n",
    "    string = re.sub(pattern=\" (negative|positive|neutral)[ ]+$\", repl=\"\", string=string)\n",
    "    string = re.sub(pattern=\" +\", repl=\" \", string=string.strip())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><h2 id=\"finetuning\">Fine-tuning</h2></a>\n",
    "<p>Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.</p>\n",
    " <p>This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.</p>\n",
    " <p>It is critical to only do this step <em>after</em> the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.</p>\n",
    " <p>It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>\n",
    "\n",
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><p><strong>Important note about <code>compile()</code> and <code>trainable</code></strong></p></a>\n",
    "<p>Calling <code>compile()</code> on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the <code>trainable</code>\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until <code>compile</code> is called again. Hence, if you change any <code>trainable</code> value, make sure\n",
    " to call <code>compile()</code> again on your\n",
    "model for your changes to be taken into account.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_detection_model_bkup = span_detection_model\n",
    "span_detection_model.save_weights(filepath=RESULTS_DIR+\"FinalSentimentModel.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ==================== FOLD# 0 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 8179 samples, validate on 8180 samples\n",
      "Epoch 1/4\n",
      "8179/8179 [==============================] - 113s 14ms/sample - loss: 12.8387 - starts_0_loss: 2.9367 - stops_0_loss: 2.9193 - starts_1_loss: 3.3690 - stops_1_loss: 3.6117 - starts_0_accuracy: 0.3976 - stops_0_accuracy: 0.4175 - starts_1_accuracy: 0.3194 - stops_1_accuracy: 0.2750 - val_loss: 11.4714 - val_starts_0_loss: 2.7822 - val_stops_0_loss: 2.5326 - val_starts_1_loss: 3.0182 - val_stops_1_loss: 3.1393 - val_starts_0_accuracy: 0.4189 - val_stops_0_accuracy: 0.4884 - val_starts_1_accuracy: 0.3412 - val_stops_1_accuracy: 0.4688\n",
      "Epoch 2/4\n",
      "8179/8179 [==============================] - 98s 12ms/sample - loss: 11.7111 - starts_0_loss: 2.8703 - stops_0_loss: 2.8326 - starts_1_loss: 2.9516 - stops_1_loss: 3.0523 - starts_0_accuracy: 0.4217 - stops_0_accuracy: 0.4460 - starts_1_accuracy: 0.3695 - stops_1_accuracy: 0.4485 - val_loss: 11.0963 - val_starts_0_loss: 2.6517 - val_stops_0_loss: 2.7864 - val_starts_1_loss: 2.8273 - val_stops_1_loss: 2.8330 - val_starts_0_accuracy: 0.4726 - val_stops_0_accuracy: 0.4642 - val_starts_1_accuracy: 0.4270 - val_stops_1_accuracy: 0.4929\n",
      "Epoch 3/4\n",
      "8179/8179 [==============================] - 101s 12ms/sample - loss: 11.5094 - starts_0_loss: 2.9030 - stops_0_loss: 2.8420 - starts_1_loss: 2.8489 - stops_1_loss: 2.9179 - starts_0_accuracy: 0.4184 - stops_0_accuracy: 0.4482 - starts_1_accuracy: 0.4084 - stops_1_accuracy: 0.4662 - val_loss: 10.8845 - val_starts_0_loss: 2.6910 - val_stops_0_loss: 2.5814 - val_starts_1_loss: 2.7560 - val_stops_1_loss: 2.8572 - val_starts_0_accuracy: 0.4708 - val_stops_0_accuracy: 0.4698 - val_starts_1_accuracy: 0.4422 - val_stops_1_accuracy: 0.4971\n",
      "Epoch 4/4\n",
      "8179/8179 [==============================] - 101s 12ms/sample - loss: 11.3439 - starts_0_loss: 2.8852 - stops_0_loss: 2.8170 - starts_1_loss: 2.7915 - stops_1_loss: 2.8549 - starts_0_accuracy: 0.4212 - stops_0_accuracy: 0.4518 - starts_1_accuracy: 0.4247 - stops_1_accuracy: 0.4838 - val_loss: 10.7531 - val_starts_0_loss: 2.7848 - val_stops_0_loss: 2.5304 - val_starts_1_loss: 2.7091 - val_stops_1_loss: 2.7304 - val_starts_0_accuracy: 0.4707 - val_stops_0_accuracy: 0.4907 - val_starts_1_accuracy: 0.4767 - val_stops_1_accuracy: 0.5193\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 8179 samples, validate on 8180 samples\n",
      "Epoch 1/4\n",
      "8179/8179 [==============================] - 118s 14ms/sample - loss: 10.8233 - starts_0_loss: 2.7577 - stops_0_loss: 2.6343 - starts_1_loss: 2.7040 - stops_1_loss: 2.7258 - starts_0_accuracy: 0.4515 - stops_0_accuracy: 0.4829 - starts_1_accuracy: 0.4574 - stops_1_accuracy: 0.5257 - val_loss: 10.4785 - val_starts_0_loss: 2.5738 - val_stops_0_loss: 2.5150 - val_starts_1_loss: 2.6988 - val_stops_1_loss: 2.6931 - val_starts_0_accuracy: 0.4949 - val_stops_0_accuracy: 0.4961 - val_starts_1_accuracy: 0.4604 - val_stops_1_accuracy: 0.5279\n",
      "Epoch 2/4\n",
      "8179/8179 [==============================] - 99s 12ms/sample - loss: 10.6778 - starts_0_loss: 2.6809 - stops_0_loss: 2.6007 - starts_1_loss: 2.6913 - stops_1_loss: 2.7042 - starts_0_accuracy: 0.4623 - stops_0_accuracy: 0.4892 - starts_1_accuracy: 0.4584 - stops_1_accuracy: 0.5325 - val_loss: 10.4387 - val_starts_0_loss: 2.5675 - val_stops_0_loss: 2.4929 - val_starts_1_loss: 2.6925 - val_stops_1_loss: 2.6880 - val_starts_0_accuracy: 0.4990 - val_stops_0_accuracy: 0.5016 - val_starts_1_accuracy: 0.4628 - val_stops_1_accuracy: 0.5344\n",
      "Epoch 3/4\n",
      "8179/8179 [==============================] - 99s 12ms/sample - loss: 10.6509 - starts_0_loss: 2.6807 - stops_0_loss: 2.5805 - starts_1_loss: 2.6978 - stops_1_loss: 2.6914 - starts_0_accuracy: 0.4556 - stops_0_accuracy: 0.4953 - starts_1_accuracy: 0.4580 - stops_1_accuracy: 0.5348 - val_loss: 10.4382 - val_starts_0_loss: 2.5639 - val_stops_0_loss: 2.5013 - val_starts_1_loss: 2.6949 - val_stops_1_loss: 2.6801 - val_starts_0_accuracy: 0.5009 - val_stops_0_accuracy: 0.4945 - val_starts_1_accuracy: 0.4638 - val_stops_1_accuracy: 0.5290\n",
      "Epoch 4/4\n",
      "8179/8179 [==============================] - 100s 12ms/sample - loss: 10.6370 - starts_0_loss: 2.6676 - stops_0_loss: 2.5929 - starts_1_loss: 2.6858 - stops_1_loss: 2.6952 - starts_0_accuracy: 0.4668 - stops_0_accuracy: 0.4878 - starts_1_accuracy: 0.4600 - stops_1_accuracy: 0.5382 - val_loss: 10.4274 - val_starts_0_loss: 2.5672 - val_stops_0_loss: 2.4998 - val_starts_1_loss: 2.6868 - val_stops_1_loss: 2.6760 - val_starts_0_accuracy: 0.4994 - val_stops_0_accuracy: 0.4968 - val_starts_1_accuracy: 0.4674 - val_stops_1_accuracy: 0.5306\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 8179 samples, validate on 8180 samples\n",
      "8179/8179 [==============================] - 188s 23ms/sample - loss: 10.4676 - starts_0_loss: 2.6340 - stops_0_loss: 2.5410 - starts_1_loss: 2.6485 - stops_1_loss: 2.6408 - starts_0_accuracy: 0.4848 - stops_0_accuracy: 0.5081 - starts_1_accuracy: 0.4794 - stops_1_accuracy: 0.5564 - val_loss: 10.2928 - val_starts_0_loss: 2.5315 - val_stops_0_loss: 2.4634 - val_starts_1_loss: 2.6572 - val_stops_1_loss: 2.6422 - val_starts_0_accuracy: 0.5087 - val_stops_0_accuracy: 0.5088 - val_starts_1_accuracy: 0.4784 - val_stops_1_accuracy: 0.5425\n",
      "[INFO]  =============== Validation for FOLD# 0 ===============\n",
      "[INFO] 52.46 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 53.92 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 50.87 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 50.88 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 41.99 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 47.62 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 39.92 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 43.50 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 40.01 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 44.94 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 37.55 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 40.67 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 45.15 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 52.91 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 45.55 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 49.47 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../working/ConfusionMatrix_V35_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../working/ConfusionMatrix_V35_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../working/ConfusionMatrix_V35_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../working/ConfusionMatrix_V35_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (8179, 100) (8179, 100)\n",
      "[INFO] Prediction shape for validation data:  (8180, 100) (8180, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  6185 out of 8179\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  6191 out of 8180\n",
      "[INFO] Training Jaccard Score:  0.5403877637739355\n",
      "[INFO] Validation Jaccard Score:  0.532749916223115\n",
      "[INFO] Training for fold: 0 finished at Mon Jun  8 18:09:34 2020\n",
      "[INFO] ==================== FOLD# 1 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 8180 samples, validate on 8179 samples\n",
      "Epoch 1/4\n",
      "8180/8180 [==============================] - 114s 14ms/sample - loss: 12.8314 - starts_0_loss: 2.9358 - stops_0_loss: 2.9255 - starts_1_loss: 3.3602 - stops_1_loss: 3.6117 - starts_0_accuracy: 0.4093 - stops_0_accuracy: 0.4122 - starts_1_accuracy: 0.3241 - stops_1_accuracy: 0.2746 - val_loss: 11.4430 - val_starts_0_loss: 2.6553 - val_stops_0_loss: 2.6730 - val_starts_1_loss: 3.0241 - val_stops_1_loss: 3.0910 - val_starts_0_accuracy: 0.4607 - val_stops_0_accuracy: 0.4620 - val_starts_1_accuracy: 0.3431 - val_stops_1_accuracy: 0.4718\n",
      "Epoch 2/4\n",
      "8180/8180 [==============================] - 99s 12ms/sample - loss: 11.7626 - starts_0_loss: 2.8869 - stops_0_loss: 2.8481 - starts_1_loss: 2.9556 - stops_1_loss: 3.0691 - starts_0_accuracy: 0.4183 - stops_0_accuracy: 0.4435 - starts_1_accuracy: 0.3743 - stops_1_accuracy: 0.4440 - val_loss: 10.8939 - val_starts_0_loss: 2.6070 - val_stops_0_loss: 2.5526 - val_starts_1_loss: 2.8724 - val_stops_1_loss: 2.8629 - val_starts_0_accuracy: 0.4724 - val_stops_0_accuracy: 0.4804 - val_starts_1_accuracy: 0.3895 - val_stops_1_accuracy: 0.5106\n",
      "Epoch 3/4\n",
      "8180/8180 [==============================] - 100s 12ms/sample - loss: 11.4948 - starts_0_loss: 2.8968 - stops_0_loss: 2.8373 - starts_1_loss: 2.8432 - stops_1_loss: 2.9202 - starts_0_accuracy: 0.4214 - stops_0_accuracy: 0.4438 - starts_1_accuracy: 0.4132 - stops_1_accuracy: 0.4718 - val_loss: 10.8602 - val_starts_0_loss: 2.6658 - val_stops_0_loss: 2.6340 - val_starts_1_loss: 2.7867 - val_stops_1_loss: 2.7742 - val_starts_0_accuracy: 0.4617 - val_stops_0_accuracy: 0.4611 - val_starts_1_accuracy: 0.4403 - val_stops_1_accuracy: 0.5019\n",
      "Epoch 4/4\n",
      "8180/8180 [==============================] - 98s 12ms/sample - loss: 11.3800 - starts_0_loss: 2.8828 - stops_0_loss: 2.8353 - starts_1_loss: 2.7956 - stops_1_loss: 2.8671 - starts_0_accuracy: 0.4260 - stops_0_accuracy: 0.4467 - starts_1_accuracy: 0.4300 - stops_1_accuracy: 0.4779 - val_loss: 10.6167 - val_starts_0_loss: 2.6393 - val_stops_0_loss: 2.5399 - val_starts_1_loss: 2.7286 - val_stops_1_loss: 2.7095 - val_starts_0_accuracy: 0.4743 - val_stops_0_accuracy: 0.4952 - val_starts_1_accuracy: 0.4540 - val_stops_1_accuracy: 0.5289\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 8180 samples, validate on 8179 samples\n",
      "Epoch 1/4\n",
      "8180/8180 [==============================] - 115s 14ms/sample - loss: 10.8253 - starts_0_loss: 2.7215 - stops_0_loss: 2.6501 - starts_1_loss: 2.7076 - stops_1_loss: 2.7485 - starts_0_accuracy: 0.4581 - stops_0_accuracy: 0.4812 - starts_1_accuracy: 0.4535 - stops_1_accuracy: 0.5207 - val_loss: 10.5078 - val_starts_0_loss: 2.5813 - val_stops_0_loss: 2.5222 - val_starts_1_loss: 2.7186 - val_stops_1_loss: 2.6867 - val_starts_0_accuracy: 0.4869 - val_stops_0_accuracy: 0.4903 - val_starts_1_accuracy: 0.4532 - val_stops_1_accuracy: 0.5311\n",
      "Epoch 2/4\n",
      "8180/8180 [==============================] - 188s 23ms/sample - loss: 10.5022 - starts_0_loss: 2.6390 - stops_0_loss: 2.5505 - starts_1_loss: 2.6522 - stops_1_loss: 2.6597 - starts_0_accuracy: 0.4784 - stops_0_accuracy: 0.5004 - starts_1_accuracy: 0.4762 - stops_1_accuracy: 0.5478 - val_loss: 10.2859 - val_starts_0_loss: 2.5326 - val_stops_0_loss: 2.4495 - val_starts_1_loss: 2.6749 - val_stops_1_loss: 2.6304 - val_starts_0_accuracy: 0.4986 - val_stops_0_accuracy: 0.5190 - val_starts_1_accuracy: 0.4670 - val_stops_1_accuracy: 0.5506\n",
      "[INFO]  =============== Validation for FOLD# 1 ===============\n",
      "[INFO] 52.11 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 53.74 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 49.86 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 51.90 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 40.54 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 45.24 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 41.58 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 45.67 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 37.82 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 42.46 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 39.77 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 43.26 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 46.89 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 51.57 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 44.72 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 50.37 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../working/ConfusionMatrix_V35_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../working/ConfusionMatrix_V35_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../working/ConfusionMatrix_V35_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../working/ConfusionMatrix_V35_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (8180, 100) (8180, 100)\n",
      "[INFO] Prediction shape for validation data:  (8179, 100) (8179, 100)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  6292 out of 8180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  6306 out of 8179\n",
      "[INFO] Training Jaccard Score:  0.5327974569639415\n",
      "[INFO] Validation Jaccard Score:  0.5297035385868454\n",
      "[INFO] Training for fold: 1 finished at Mon Jun  8 18:27:40 2020\n",
      "Mon Jun  8 18:27:40 2020\n"
     ]
    }
   ],
   "source": [
    "for num, (t_index, v_index) in enumerate(span_kf.split(X_span, Y_span_stops)):\n",
    "    print(\"[INFO] ==================== FOLD#\", num, \"====================\")\n",
    "    \n",
    "    if num > 0:\n",
    "        del span_history\n",
    "        del span_detection_model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    \n",
    "    span_detection_model = span_detection_model_bkup\n",
    "    span_detection_model.load_weights(RESULTS_DIR+\"FinalSentimentModel.h5\")\n",
    "    \n",
    "    span_mcp = ModelCheckpoint(filepath=RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\", monitor='val_loss',\n",
    "                               verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "    \n",
    "    span_csvl = CSVLogger(filename=RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(num)+\".csv\",\n",
    "                          separator=\",\",\n",
    "                          append=True)\n",
    "    \n",
    "    print(\"[INFO] Training only the final layers at higher learning rates.\")\n",
    "    span_detection_model.layers[3].trainable = False\n",
    "    adam = Adam(learning_rate=MAX_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=LABEL_SMOOTHING_PARAM),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[0],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl])\n",
    "    \n",
    "    print(\"[INFO] Training only the final layers at lower learning rates.\")\n",
    "    adam = Adam(learning_rate=MID_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=LABEL_SMOOTHING_PARAM),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[1],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl])\n",
    "    \n",
    "    print(\"[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\")\n",
    "    span_detection_model.layers[3].trainable = True\n",
    "    adam = Adam(learning_rate=MIN_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=LABEL_SMOOTHING_PARAM),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[2],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl])\n",
    "    \n",
    "    # Loading best weights per fold\n",
    "    span_detection_model.load_weights(RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\")\n",
    "    \n",
    "    pred_train = span_detection_model.predict(x = {\"att_flags\":X_span_att[t_index],\n",
    "                                                   \"words\":X_span[t_index],\n",
    "                                                   \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                              batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "    pred_val = span_detection_model.predict(x = {\"att_flags\":X_span_att[v_index],\n",
    "                                                 \"words\":X_span[v_index],\n",
    "                                                 \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                            batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    pred_starts_train, pred_stops_train = (pred_train[0]/2.0 + pred_train[2]/2.0), (pred_train[1]/2.0 + pred_train[3]/2.0)\n",
    "    pred_starts_val, pred_stops_val = (pred_val[0]/2.0 + pred_val[2]/2.0), (pred_val[1]/2.0 + pred_val[3]/2.0)\n",
    "    \n",
    "    # Accumulate test results after training every fold\n",
    "    pred_test_fold = span_detection_model.predict(x = {\"att_flags\":X_span_att_test,\n",
    "                                                       \"words\":X_span_test,\n",
    "                                                       \"token_ids\":np.zeros_like(X_span_att_test)},\n",
    "                                                  batch_size=PREDICT_BATCH_SIZE)\n",
    "    if num==0:\n",
    "        pred_test = []\n",
    "        pred_test.append(pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test.append(pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    else:\n",
    "        pred_test[0] += (pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test[1] += (pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    \n",
    "    # Tabulate\n",
    "    preds = {\n",
    "        \"train\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_span_starts[t_index].argmax(axis=1),\n",
    "                \"y_pred\":pred_train[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_span_stops[t_index].argmax(axis=1),\n",
    "                \"y_pred\":pred_train[1].argmax(axis=1)\n",
    "            }\n",
    "        },\n",
    "        \"valid\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_span_starts[v_index].argmax(axis=1),\n",
    "                \"y_pred\":pred_val[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_span_stops[v_index].argmax(axis=1),\n",
    "                \"y_pred\":pred_val[1].argmax(axis=1)\n",
    "            }        \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print_metrics(pred_dict=preds)\n",
    "\n",
    "    print(\"[INFO] Prediction shape for training data: \", pred_starts_train.shape, pred_stops_train.shape)\n",
    "    print(\"[INFO] Prediction shape for validation data: \", pred_starts_val.shape, pred_stops_val.shape)\n",
    "\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for training data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_train.argmax(axis=1),\n",
    "                                  pred_stops_train.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_train.shape[0])\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for validation data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_val.argmax(axis=1),\n",
    "                                  pred_stops_val.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_val.shape[0],)\n",
    "\n",
    "    pred_words_train = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[t_index],\n",
    "                                                                                                                                   pred_starts_train.argmax(axis=1),\n",
    "                                                                                                                                   pred_stops_train.argmax(axis=1))]\n",
    "    \n",
    "    pred_words_val = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[v_index],\n",
    "                                                                                                                                 pred_starts_val.argmax(axis=1),\n",
    "                                                                                                                                 pred_stops_val.argmax(axis=1))]\n",
    "\n",
    "    print(\"[INFO] Training Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in t_index],\n",
    "                                                          pred_words_train)]))\n",
    "    print(\"[INFO] Validation Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in v_index],\n",
    "                                                          pred_words_val)]))\n",
    "    print(\"[INFO] Training for fold:\", num, \"finished at\", ctime(time()))\n",
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Prediction shape for testing data:  (3534, 100) (3534, 100)\n"
     ]
    }
   ],
   "source": [
    "pred_starts_test, pred_stops_test = pred_test[0]/NUM_FOLDS, pred_test[1]/NUM_FOLDS\n",
    "print(\"[INFO] Prediction shape for testing data: \", pred_starts_test.shape, pred_stops_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal predictions (StartIndex less than EndIndex) for testing data: 2924 out of 3534\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal predictions (StartIndex less than EndIndex) for testing data:\",\n",
    "      sum([s<e for s,e in zip(pred_starts_test.argmax(axis=1),\n",
    "                              pred_stops_test.argmax(axis=1))]), \n",
    "      \"out of\",\n",
    "      pred_starts_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words_test = [\n",
    "    post_process(tokenizer.decode(t[s:e+1])) if s<e else post_process(tokenizer.decode(t[e:])) for t,s,e in zip(X_span_test,\n",
    "                                                                                                              pred_starts_test.argmax(axis=1),\n",
    "                                                                                                              pred_stops_test.argmax(axis=1))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " yes! im down to 50% full on my dvr  i was at 98% like 3 days ago... lol i swear if i didnt have a dvr i would never watch tv   neutral \n",
      "25\n",
      "38\n",
      "i swear if i didnt have a dvr i would never watch tv\n"
     ]
    }
   ],
   "source": [
    "check_idx = 158\n",
    "#print([[t,i,j,k] for t,i,j,k in zip(tokenizer.decode(),X_test[check_idx],pred_starts_test[check_idx],pred_stops_test[check_idx])])\n",
    "print(tokenizer.decode(X_span_test[check_idx]))\n",
    "print(pred_starts_test.argmax(axis=1)[check_idx])\n",
    "print(pred_stops_test.argmax(axis=1)[check_idx])\n",
    "print(post_process(tokenizer.decode(X_span_test[check_idx][pred_starts_test.argmax(axis=1)[check_idx]:1+pred_stops_test.argmax(axis=1)[check_idx]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span['selected_text'] = pred_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[\"selected_text\"] = np.where(test_df_span[\"sentiment\"] == \"neutral\",\n",
    "                                         test_df_span[\"text\"],\n",
    "                                         test_df_span[\"selected_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[[\"textID\", \"selected_text\"]].to_csv(RESULTS_DIR+\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>Thank you so much phaoloo !!!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>thank you so much phaoloo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>Well, good luck then.</td>\n",
       "      <td>positive</td>\n",
       "      <td>good luck then.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>me because I might not have enough money for college!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>me because i might not have enough money for college!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>#warmfuzzies to you, my friend</td>\n",
       "      <td>positive</td>\n",
       "      <td>#warmfuzzies to you, my friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Welcome!</td>\n",
       "      <td>positive</td>\n",
       "      <td>welcome!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Hi all, just recovering from a party, looking forward to an exciting bank holiday around the diy shops...life cant get much better.surely</td>\n",
       "      <td>positive</td>\n",
       "      <td>exciting bank holiday around the diy shops...life cant get much better.surely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>`If I don`t believe in Him, why would He believe in me?` -Bring Me The Horizon  A chill goes down my spine whenever I hear that line.</td>\n",
       "      <td>negative</td>\n",
       "      <td>a chill goes down my spine whenever i hear that line.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Jus got back from a run up sunset blvd! My cuzin tried to kill me my legs are still movin and I`m sittin down!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>kill me my legs are still movin and i`m sittin down!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3316</th>\n",
       "      <td>Listening `Hallelujah` on Youtube. Leonard Cohen wins.  #xfactor</td>\n",
       "      <td>positive</td>\n",
       "      <td>listening `hallelujah` on youtube. leonard cohen wins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3211</th>\n",
       "      <td>hello  I`m up late playing on the internet. I love you!</td>\n",
       "      <td>positive</td>\n",
       "      <td>i love you!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>you guys didn`t say hi or answer my questions yesterday  but nice songs.</td>\n",
       "      <td>positive</td>\n",
       "      <td>nice songs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>Just found mj mouse flats at DJs. Of course there are none left in my size</td>\n",
       "      <td>negative</td>\n",
       "      <td>of course there are none left in my size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>is *ugh* what a miserable looking day. 54 degrees. Where did summer go?</td>\n",
       "      <td>negative</td>\n",
       "      <td>miserable looking day. 54 degrees. where did summer go?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>miss you captain</td>\n",
       "      <td>negative</td>\n",
       "      <td>miss you captain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>Lol - I could have written that - would be good to have 2 Mondays this week, then I might catch up x</td>\n",
       "      <td>positive</td>\n",
       "      <td>good to have 2 mondays this week, then i might catch up x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>s/s aus fshionwk- zimmermann,illionare,dhini + gail sorronda,ROMANCE WAS BORN,lisa ho,CASSETTE SOCIETY-loved the tutu, balmain ispired?</td>\n",
       "      <td>positive</td>\n",
       "      <td>loved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>hope u did alright on the final</td>\n",
       "      <td>positive</td>\n",
       "      <td>hope u did alright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>its at 3 am, im very tired but i can`t sleep  but i try it</td>\n",
       "      <td>negative</td>\n",
       "      <td>tired but i can`t sleep but i try it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Ur so cute..I`m a fan of Dream A Little Dream, This Kiss and appearances like in Dawson`s Creek  Make more flicks!</td>\n",
       "      <td>positive</td>\n",
       "      <td>ur so cute..i`m a fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>Listening to Busted - 3AM. I miss them</td>\n",
       "      <td>negative</td>\n",
       "      <td>i miss them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>_ aww thats too bad you lost it though</td>\n",
       "      <td>negative</td>\n",
       "      <td>thats too bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>Hello everyone im just sitting here rocking out to 9412 sam on the air The best classic rock station on twitter here</td>\n",
       "      <td>positive</td>\n",
       "      <td>best classic rock station on twitter here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429</th>\n",
       "      <td>HOW COME ITS SO HARD TO FIND A GUY WITH THE SAME PASSION FOR LOVE AS FOR A WOMAN?? AM I EVER GOING TO SEE THE DAY! IM lonely yall!</td>\n",
       "      <td>negative</td>\n",
       "      <td>lonely yall!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>My day on the computer is about to end. Dang. I hate not being allowed on the computer.</td>\n",
       "      <td>negative</td>\n",
       "      <td>i hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>I`m not ready for my baby to be 3 tomorrow  she is growing so fast....</td>\n",
       "      <td>positive</td>\n",
       "      <td>i`m not ready for my baby to be 3 tomorrow she is growing so fast....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "325                                                                                                              Thank you so much phaoloo !!!!   \n",
       "334                                                                                                                       Well, good luck then.   \n",
       "866                                                                                     me because I might not have enough money for college!!!   \n",
       "1484                                                                                                             #warmfuzzies to you, my friend   \n",
       "130                                                                                                                                    Welcome!   \n",
       "66    Hi all, just recovering from a party, looking forward to an exciting bank holiday around the diy shops...life cant get much better.surely   \n",
       "1474      `If I don`t believe in Him, why would He believe in me?` -Bring Me The Horizon  A chill goes down my spine whenever I hear that line.   \n",
       "321                             Jus got back from a run up sunset blvd! My cuzin tried to kill me my legs are still movin and I`m sittin down!!   \n",
       "3316                                                                           Listening `Hallelujah` on Youtube. Leonard Cohen wins.  #xfactor   \n",
       "3211                                                                                    hello  I`m up late playing on the internet. I love you!   \n",
       "18                                                                     you guys didn`t say hi or answer my questions yesterday  but nice songs.   \n",
       "389                                                                  Just found mj mouse flats at DJs. Of course there are none left in my size   \n",
       "2787                                                                    is *ugh* what a miserable looking day. 54 degrees. Where did summer go?   \n",
       "977                                                                                                                            miss you captain   \n",
       "406                                        Lol - I could have written that - would be good to have 2 Mondays this week, then I might catch up x   \n",
       "2276    s/s aus fshionwk- zimmermann,illionare,dhini + gail sorronda,ROMANCE WAS BORN,lisa ho,CASSETTE SOCIETY-loved the tutu, balmain ispired?   \n",
       "2215                                                                                                            hope u did alright on the final   \n",
       "3529                                                                                 its at 3 am, im very tired but i can`t sleep  but i try it   \n",
       "71                           Ur so cute..I`m a fan of Dream A Little Dream, This Kiss and appearances like in Dawson`s Creek  Make more flicks!   \n",
       "2009                                                                                                     Listening to Busted - 3AM. I miss them   \n",
       "934                                                                                                      _ aww thats too bad you lost it though   \n",
       "2217                       Hello everyone im just sitting here rocking out to 9412 sam on the air The best classic rock station on twitter here   \n",
       "3429         HOW COME ITS SO HARD TO FIND A GUY WITH THE SAME PASSION FOR LOVE AS FOR A WOMAN?? AM I EVER GOING TO SEE THE DAY! IM lonely yall!   \n",
       "2136                                                    My day on the computer is about to end. Dang. I hate not being allowed on the computer.   \n",
       "1389                                                                     I`m not ready for my baby to be 3 tomorrow  she is growing so fast....   \n",
       "\n",
       "     sentiment  \\\n",
       "325   positive   \n",
       "334   positive   \n",
       "866   negative   \n",
       "1484  positive   \n",
       "130   positive   \n",
       "66    positive   \n",
       "1474  negative   \n",
       "321   negative   \n",
       "3316  positive   \n",
       "3211  positive   \n",
       "18    positive   \n",
       "389   negative   \n",
       "2787  negative   \n",
       "977   negative   \n",
       "406   positive   \n",
       "2276  positive   \n",
       "2215  positive   \n",
       "3529  negative   \n",
       "71    positive   \n",
       "2009  negative   \n",
       "934   negative   \n",
       "2217  positive   \n",
       "3429  negative   \n",
       "2136  negative   \n",
       "1389  positive   \n",
       "\n",
       "                                                                      selected_text  \n",
       "325                                                       thank you so much phaoloo  \n",
       "334                                                                 good luck then.  \n",
       "866                         me because i might not have enough money for college!!!  \n",
       "1484                                                 #warmfuzzies to you, my friend  \n",
       "130                                                                        welcome!  \n",
       "66    exciting bank holiday around the diy shops...life cant get much better.surely  \n",
       "1474                          a chill goes down my spine whenever i hear that line.  \n",
       "321                           kill me my legs are still movin and i`m sittin down!!  \n",
       "3316                         listening `hallelujah` on youtube. leonard cohen wins.  \n",
       "3211                                                                    i love you!  \n",
       "18                                                                      nice songs.  \n",
       "389                                        of course there are none left in my size  \n",
       "2787                        miserable looking day. 54 degrees. where did summer go?  \n",
       "977                                                                miss you captain  \n",
       "406                       good to have 2 mondays this week, then i might catch up x  \n",
       "2276                                                                          loved  \n",
       "2215                                                             hope u did alright  \n",
       "3529                                           tired but i can`t sleep but i try it  \n",
       "71                                                            ur so cute..i`m a fan  \n",
       "2009                                                                    i miss them  \n",
       "934                                                                   thats too bad  \n",
       "2217                                      best classic rock station on twitter here  \n",
       "3429                                                                   lonely yall!  \n",
       "2136                                                                         i hate  \n",
       "1389          i`m not ready for my baby to be 3 tomorrow she is growing so fast....  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_span.loc[test_df_span.sentiment!=\"neutral\"][[\"text\", \"sentiment\",\"selected_text\"]].sample(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
