{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V22\"\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 16\n",
    "PREDICT_BATCH_SIZE = 512\n",
    "DROPOUT = 0.3\n",
    "\n",
    "RUN_ON_SAMPLE = False\n",
    "EXCLUDE_NEUTRAL_CLASS = True\n",
    "MAX_LR = 5e-3\n",
    "MID_LR = 1e-4\n",
    "MIN_LR = 1e-6\n",
    "NUM_EPOCHS = [10, 5, 2]\n",
    "NUM_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, MaxPooling1D, Layer, AdditiveAttention, Attention\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    a = set(str1)\n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(98765)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 18 16:28:54 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0', '2.8.0', '0.5.2']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\"  target=\"_blank\"><h2 id=\"limiting_gpu_memory_growth\" data-text=\"Limiting GPU memory growth\" tabindex=\"0\">Limiting GPU memory growth</h2></a>\n",
    "<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\"><code translate=\"no\" dir=\"ltr\">CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the <code translate=\"no\" dir=\"ltr\">tf.config.experimental.set_visible_devices</code> method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it okay to exclude neutral text from models and force it later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVERAGE: 0.5334592552617378\n",
      "MEAN JACCARD: 0.9808316180492903\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7094</th>\n",
       "      <td>588e531efe</td>\n",
       "      <td>Moooorning! Fancy a coffee?</td>\n",
       "      <td>Moooorning! Fancy a coffee?</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>dd01108dba</td>\n",
       "      <td>....... i`m wondering if you`re as awake as i am. ?</td>\n",
       "      <td>i`m wondering if you`re as awake as i am. ?</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>e1c569d4cf</td>\n",
       "      <td>Will certainly do that.</td>\n",
       "      <td>Will certainly do that.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>208cb66bdd</td>\n",
       "      <td>Crab boat? I have my mobile set to go off every night, to remind me to head to bed</td>\n",
       "      <td>Crab boat? I have my mobile set to go off every night, to remind me to head to bed</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10481</th>\n",
       "      <td>9a4f40c0b4</td>\n",
       "      <td>yes, yes it was</td>\n",
       "      <td>yes, yes it was</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID  \\\n",
       "7094   588e531efe   \n",
       "4148   dd01108dba   \n",
       "3430   e1c569d4cf   \n",
       "4714   208cb66bdd   \n",
       "10481  9a4f40c0b4   \n",
       "\n",
       "                                                                                      text  \\\n",
       "7094                                                           Moooorning! Fancy a coffee?   \n",
       "4148                                   ....... i`m wondering if you`re as awake as i am. ?   \n",
       "3430                                                               Will certainly do that.   \n",
       "4714    Crab boat? I have my mobile set to go off every night, to remind me to head to bed   \n",
       "10481                                                                      yes, yes it was   \n",
       "\n",
       "                                                                            selected_text  \\\n",
       "7094                                                          Moooorning! Fancy a coffee?   \n",
       "4148                                          i`m wondering if you`re as awake as i am. ?   \n",
       "3430                                                              Will certainly do that.   \n",
       "4714   Crab boat? I have my mobile set to go off every night, to remind me to head to bed   \n",
       "10481                                                                     yes, yes it was   \n",
       "\n",
       "      sentiment  \n",
       "7094    neutral  \n",
       "4148    neutral  \n",
       "3430    neutral  \n",
       "4714    neutral  \n",
       "10481   neutral  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"../data/train.csv\", encoding=\"utf8\")\n",
    "# cases where neutral's text and selected text columns are the same\n",
    "neutrals = df2.loc[df2.sentiment==\"neutral\"].copy()\n",
    "neutrals = neutrals.reset_index(drop=True)\n",
    "neutrals[\"text\"] = neutrals[\"text\"].astype(str)\n",
    "neutrals[\"selected_text\"] = neutrals[\"selected_text\"].astype(str)\n",
    "print(\"COVERAGE:\", np.sum(np.where((neutrals.text == neutrals.selected_text), 1, 0))/neutrals.shape[0])\n",
    "print(\"MEAN JACCARD:\", np.mean([jaccard(str1=i, str2=j) for i,j in zip(neutrals.selected_text,neutrals.text)]))\n",
    "\n",
    "neutrals.loc[neutrals.text != neutrals.selected_text].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  1\n",
      "selected_text  object  1\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27480, 'selected_text': 22463, 'sentiment': 3}\n",
      "            textID                                text selected_text sentiment\n",
      "count        27481                               27480         27480     27481\n",
      "unique       27481                               27480         22463         3\n",
      "top     05ed6c49d8   It`s on today`s menu, don`t worry          good   neutral\n",
      "freq             1                                   1           199     11118\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\", encoding=\"utf8\")\n",
    "\n",
    "print(pd.concat((df.dtypes, df.isna().sum()), axis=1))\n",
    "print(df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df[i].nunique() for i in df.columns})\n",
    "print(df.describe())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID                                                      text  \\\n",
      "count         3534                                                      3534   \n",
      "unique        3534                                                      3534   \n",
      "top     982b1c05d3   Some of yr coworkers hogging the jukebox @ Union Jacks.   \n",
      "freq             1                                                         1   \n",
      "\n",
      "       sentiment  \n",
      "count       3534  \n",
      "unique         3  \n",
      "top      neutral  \n",
      "freq        1430  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "print(pd.concat((test_df.dtypes, test_df.isna().sum()), axis=1))\n",
    "print(test_df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df[i].nunique() for i in test_df.columns})\n",
    "print(test_df.describe())\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>c77717b103</td>\n",
       "      <td>I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½</td>\n",
       "      <td>I love to!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>28dbada620</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "44   c77717b103   \n",
       "192  28dbada620   \n",
       "\n",
       "                                                                                                       text  \\\n",
       "44    I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½   \n",
       "192                                *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "                                                                selected_text  \\\n",
       "44                                                                 I love to!   \n",
       "192  *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "    sentiment  \n",
       "44   positive  \n",
       "192   neutral  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>7223fdccc2</td>\n",
       "      <td>tikcets are only ï¿½91...each...BUT I SO WANT TO GO</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>43ad351369</td>\n",
       "      <td>AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "145  7223fdccc2   \n",
       "618  43ad351369   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "145                                                                                             tikcets are only ï¿½91...each...BUT I SO WANT TO GO   \n",
       "618  AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s   \n",
       "\n",
       "    sentiment  \n",
       "145  positive  \n",
       "618  positive  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[test_df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment count in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>7781</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>11117</td>\n",
       "      <td>1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>8582</td>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text  text\n",
       "sentiment             \n",
       "negative    7781  1001\n",
       "neutral    11117  1430\n",
       "positive    8582  1103"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df.groupby(\"sentiment\")[[\"text\"]].count(), test_df.groupby(\"sentiment\")[[\"text\"]].count()], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ec1ca3933742>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# For traceability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"original_index\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"original_index\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# For traceability\n",
    "df[\"original_index\"] = df.index\n",
    "test_df[\"original_index\"] = test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_idxs = [513, 1060, 5543, 8410, 13605, 24778, 25258] + [18, 27, 32, 39, 48, 49, 64, 66, 84, 92, 102, 116, 129, 132, 134, 149, 160, 166, 168, 189, 196, 197, 210, 223, 247, 251, 260, 285, 295, 296, 297, 309, 349, 362, 366, 382, 396, 398, 406, 410, 425, 458, 460, 482, 492, 500, 504, 517, 533, 537, 543, 569, 573, 581, 604, 605, 608, 634, 636, 637, 639, 642, 665, 670, 674, 678, 685, 702, 707, 709, 710, 727, 737, 746, 751, 757, 768, 778, 787, 794, 804, 809, 853, 854, 856, 863, 871, 872, 912, 931, 937, 942, 950, 956, 957, 963, 992, 993, 999, 1001, 1011, 1012, 1036, 1039, 1057, 1062, 1076, 1077, 1083, 1096, 1105, 1116, 1117, 1122, 1134, 1137, 1150, 1159, 1199, 1200, 1209, 1217, 1264, 1271, 1280, 1283, 1298, 1300, 1303, 1319, 1327, 1342, 1360, 1363, 1365, 1372, 1374, 1376, 1382, 1393, 1416, 1417, 1420, 1447, 1454, 1468, 1472, 1513, 1515, 1518, 1531, 1548, 1558, 1567, 1580, 1588, 1605, 1623, 1648, 1649, 1657, 1659, 1661, 1665, 1672, 1674, 1678, 1683, 1693, 1696, 1713, 1723, 1728, 1739, 1747, 1754, 1798, 1806, 1808, 1814, 1822, 1828, 1830, 1835, 1850, 1857, 1860, 1866, 1888, 1901, 1902, 1929, 1938, 1940, 1987, 1994, 2003, 2004, 2008, 2011, 2023, 2054, 2056, 2065, 2068, 2083, 2086, 2094, 2106, 2113, 2124, 2125, 2136, 2139, 2145, 2168, 2180, 2181, 2187, 2213, 2225, 2227, 2230, 2239, 2273, 2279, 2286, 2288, 2298, 2323, 2324, 2335, 2343, 2359, 2364, 2371, 2385, 2388, 2392, 2398, 2401, 2404, 2410, 2415, 2421, 2430, 2438, 2439, 2440, 2444, 2445, 2448, 2470, 2478, 2489, 2499, 2505, 2515, 2520, 2559, 2574, 2576, 2589, 2598, 2600, 2640, 2643, 2650, 2671, 2679, 2696, 2699, 2706, 2716, 2718, 2731, 2743, 2749, 2750, 2785, 2787, 2789, 2805, 2809, 2825, 2838, 2856, 2888, 2898, 2903, 2923, 2925, 2931, 2933, 2942, 2957, 2961, 2986, 2995, 3034, 3068, 3075, 3077, 3080, 3088, 3089, 3096, 3097, 3101, 3121, 3147, 3160, 3161, 3176, 3182, 3187, 3211, 3225, 3234, 3257, 3263, 3267, 3281, 3290, 3295, 3317, 3319, 3325, 3333, 3359, 3364, 3369, 3374, 3390, 3399, 3400, 3411, 3424, 3473, 3475, 3476, 3479, 3492, 3499, 3504, 3509, 3517, 3519, 3532, 3553, 3578, 3588, 3590, 3593, 3598, 3605, 3606, 3607, 3622, 3634, 3669, 3672, 3678, 3682, 3685, 3694, 3708, 3720, 3729, 3750, 3755, 3784, 3794, 3798, 3801, 3806, 3807, 3813, 3826, 3834, 3872, 3886, 3895, 3903, 3925, 3937, 3946, 3962, 3969, 3998, 4031, 4047, 4053, 4074, 4091, 4093, 4100, 4108, 4113, 4119, 4143, 4148, 4163, 4164, 4183, 4196, 4254, 4260, 4265, 4307, 4308, 4309, 4312, 4318, 4325, 4327, 4328, 4329, 4344, 4347, 4369, 4371, 4377, 4384, 4402, 4404, 4405, 4407, 4412, 4421, 4422, 4437, 4455, 4457, 4471, 4473, 4479, 4483, 4486, 4514, 4532, 4537, 4542, 4543, 4545, 4550, 4554, 4563, 4575, 4576, 4591, 4595, 4631, 4636, 4637, 4639, 4654, 4655, 4704, 4706, 4714, 4718, 4724, 4729, 4735, 4747, 4749, 4750, 4764, 4800, 4814, 4841, 4842, 4844, 4864, 4866, 4869, 4875, 4891, 4908, 4918, 4924, 4928, 4933, 4943, 4946, 4947, 4956, 4968, 4980, 4994, 5009, 5011, 5038, 5056, 5079, 5083, 5101, 5133, 5167, 5173, 5174, 5188, 5189, 5196, 5197, 5213, 5218, 5229, 5241, 5245, 5258, 5279, 5290, 5294, 5297, 5308, 5317, 5331, 5342, 5343, 5353, 5355, 5356, 5358, 5379, 5385, 5386, 5399, 5418, 5433, 5436, 5450, 5456, 5466, 5489, 5504, 5509, 5510, 5530, 5542, 5560, 5583, 5603, 5607, 5618, 5620, 5624, 5626, 5678, 5697, 5712, 5725, 5751, 5757, 5775, 5785, 5799, 5822, 5832, 5836, 5844, 5895, 5902, 5904, 5913, 5918, 5938, 5939, 5961, 5984, 5990, 6025, 6031, 6033, 6060, 6077, 6084, 6090, 6113, 6131, 6134, 6151, 6152, 6176, 6203, 6229, 6230, 6248, 6261, 6277, 6289, 6296, 6304, 6308, 6317, 6319, 6321, 6335, 6338, 6343, 6353, 6356, 6360, 6377, 6382, 6395, 6404, 6425, 6433, 6468, 6474, 6475, 6476, 6477, 6510, 6528, 6540, 6574, 6580, 6582, 6593, 6609, 6627, 6628, 6633, 6635, 6650, 6656, 6657, 6670, 6673, 6676, 6677, 6679, 6686, 6702, 6724, 6740, 6745, 6759, 6801, 6804, 6830, 6833, 6834, 6842, 6859, 6871, 6875, 6878, 6880, 6885, 6896, 6934, 6939, 6948, 6950, 6958, 6970, 6979, 6987, 7010, 7017, 7024, 7040, 7099, 7107, 7112, 7136, 7147, 7149, 7158, 7184, 7225, 7227, 7251, 7274, 7288, 7305, 7306, 7308, 7331, 7344, 7355, 7373, 7407, 7408, 7409, 7410, 7423, 7436, 7438, 7442, 7444, 7450, 7469, 7473, 7479, 7487, 7492, 7506, 7513, 7521, 7528, 7530, 7546, 7567, 7581, 7582, 7597, 7599, 7602, 7632, 7635, 7636, 7642, 7662, 7663, 7701, 7752, 7758, 7765, 7769, 7778, 7780, 7788, 7801, 7816, 7818, 7831, 7836, 7837, 7849, 7893, 7895, 7896, 7922, 7927, 7945, 7948, 7968, 7997, 8003, 8005, 8010, 8030, 8049, 8051, 8052, 8059, 8060, 8085, 8094, 8142, 8143, 8151, 8153, 8161, 8169, 8170, 8196, 8208, 8212, 8215, 8224, 8235, 8241, 8249, 8250, 8276, 8282, 8285, 8286, 8288, 8289, 8299, 8314, 8338, 8343, 8345, 8349, 8373, 8379, 8390, 8397, 8399, 8407, 8427, 8428, 8432, 8439, 8442, 8444, 8484, 8493, 8497, 8500, 8507, 8510, 8534, 8546, 8559, 8569, 8579, 8588, 8594, 8604, 8617, 8620, 8624, 8631, 8638, 8661, 8671, 8674, 8687, 8691, 8696, 8706, 8720, 8729, 8749, 8756, 8758, 8769, 8774, 8780, 8800, 8803, 8809, 8815, 8827, 8830, 8875, 8902, 8906, 8910, 8917, 8919, 8922, 8951, 8954, 8968, 8985, 8986, 8991, 8999, 9014, 9034, 9050, 9057, 9063, 9070, 9088, 9098, 9112, 9113, 9122, 9127, 9140, 9155, 9157, 9169, 9179, 9190, 9216, 9223, 9226, 9264, 9272, 9281, 9297, 9300, 9303, 9322, 9354, 9361, 9374, 9377, 9381, 9385, 9400, 9403, 9419, 9422, 9425, 9428, 9436, 9442, 9449, 9454, 9456, 9458, 9470, 9482, 9496, 9500, 9535, 9539, 9541, 9554, 9556, 9560, 9565, 9574, 9594, 9605, 9610, 9631, 9632, 9660, 9689, 9691, 9696, 9708, 9710, 9712, 9725, 9726, 9734, 9742, 9753, 9770, 9789, 9799, 9801, 9812, 9817, 9836, 9839, 9850, 9852, 9881, 9882, 9891, 9906, 9912, 9920, 9922, 9928, 9976, 9980, 9984, 10002, 10004, 10007, 10029, 10039, 10042, 10050, 10052, 10057, 10067, 10070, 10092, 10093, 10123, 10131, 10138, 10145, 10151, 10164, 10167, 10170, 10179, 10184, 10185, 10190, 10197, 10200, 10211, 10218, 10219, 10222, 10228, 10245, 10247, 10255, 10259, 10276, 10293, 10298, 10299, 10306, 10319, 10327, 10342, 10348, 10377, 10379, 10449, 10465, 10469, 10492, 10504, 10505, 10508, 10521, 10528, 10529, 10530, 10570, 10573, 10577, 10578, 10597, 10611, 10639, 10651, 10666, 10672, 10675, 10690, 10706, 10711, 10738, 10742, 10747, 10749, 10779, 10782, 10783, 10795, 10813, 10849, 10852, 10853, 10884, 10885, 10888, 10892, 10901, 10914, 10918, 10919, 10933, 10935, 10956, 10966, 10968, 10970, 10981, 10986, 10996, 10999, 11005, 11032, 11038, 11051, 11055, 11060, 11067, 11097, 11102, 11155, 11157, 11159, 11162, 11167, 11176, 11187, 11215, 11222, 11228, 11230, 11257, 11263, 11265, 11274, 11286, 11289, 11297, 11300, 11308, 11323, 11337, 11339, 11341, 11347, 11349, 11353, 11371, 11381, 11386, 11426, 11431, 11448, 11449, 11474, 11480, 11500, 11507, 11553, 11562, 11582, 11588, 11594, 11612, 11616, 11618, 11636, 11643, 11650, 11658, 11666, 11679, 11683, 11693, 11695, 11698, 11699, 11706, 11716, 11723, 11730, 11732, 11741, 11745, 11780, 11794, 11798, 11808, 11812, 11828, 11829, 11837, 11840, 11841, 11844, 11849, 11853, 11854, 11855, 11861, 11862, 11872, 11875, 11878, 11886, 11890, 11899, 11905, 11912, 11938, 11963, 11967, 11973, 11980, 11985, 11999, 12026, 12027, 12029, 12039, 12054, 12067, 12115, 12124, 12134, 12136, 12138, 12150, 12187, 12205, 12206, 12224, 12242, 12258, 12269, 12277, 12283, 12295, 12305, 12314, 12322, 12331, 12333, 12334, 12350, 12355, 12356, 12360, 12370, 12372, 12389, 12395, 12396, 12397, 12405, 12410, 12416, 12429, 12439, 12440, 12442, 12446, 12474, 12482, 12486, 12507, 12516, 12521, 12522, 12526, 12527, 12537, 12550, 12563, 12576, 12585, 12586, 12587, 12598, 12606, 12630, 12631, 12635, 12639, 12650, 12657, 12662, 12669, 12687, 12703, 12718, 12724, 12732, 12736, 12745, 12767, 12787, 12803, 12804, 12808, 12818, 12830, 12843, 12855, 12856, 12870, 12884, 12903, 12914, 12920, 12923, 12924, 12927, 12936, 12972, 12977, 12998, 13003, 13004, 13009, 13044, 13068, 13069, 13074, 13080, 13095, 13098, 13124, 13130, 13153, 13154, 13176, 13219, 13222, 13227, 13237, 13267, 13274, 13293, 13301, 13304, 13307, 13330, 13333, 13354, 13365, 13373, 13379, 13381, 13389, 13397, 13422, 13430, 13433, 13445, 13466, 13473, 13476, 13482, 13490, 13493, 13528, 13529, 13535, 13543, 13558, 13559, 13586, 13637, 13643, 13646, 13665, 13674, 13678, 13679, 13704, 13707, 13710, 13718, 13727, 13733, 13747, 13762, 13772, 13785, 13796, 13803, 13812, 13814, 13817, 13827, 13844, 13846, 13847, 13848, 13854, 13855, 13861, 13864, 13873, 13899, 13907, 13936, 13939, 13946, 13949, 13954, 13964, 13965, 13974, 13975, 13978, 13980, 14002, 14006, 14007, 14030, 14043, 14044, 14057, 14058, 14060, 14073, 14091, 14109, 14110, 14112, 14117, 14131, 14132, 14159, 14172, 14173, 14176, 14184, 14194, 14197, 14200, 14204, 14207, 14213, 14230, 14257, 14273, 14275, 14287, 14299, 14301, 14307, 14308, 14347, 14362, 14365, 14381, 14386, 14400, 14416, 14428, 14436, 14446, 14457, 14458, 14462, 14487, 14499, 14509, 14532, 14548, 14551, 14571, 14596, 14598, 14611, 14621, 14639, 14640, 14648, 14662, 14669, 14671, 14676, 14680, 14687, 14689, 14691, 14718, 14731, 14747, 14779, 14787, 14825, 14839, 14844, 14847, 14855, 14870, 14880, 14891, 14908, 14946, 14959, 14960, 14967, 14971, 14986, 15007, 15010, 15016, 15044, 15049, 15055, 15056, 15112, 15131, 15158, 15165, 15174, 15177, 15196, 15203, 15206, 15207, 15213, 15217, 15233, 15236, 15253, 15259, 15286, 15296, 15307, 15308, 15321, 15323, 15327, 15339, 15363, 15372, 15374, 15379, 15380, 15383, 15385, 15392, 15410, 15449, 15452, 15462, 15464, 15465, 15477, 15483, 15503, 15505, 15528, 15532, 15548, 15562, 15566, 15571, 15605, 15610, 15622, 15651, 15660, 15666, 15674, 15709, 15721, 15731, 15735, 15736, 15741, 15767, 15774, 15785, 15792, 15793, 15811, 15851, 15879, 15883, 15884, 15912, 15926, 15927, 15931, 15940, 15959, 15965, 15985, 15998, 16020, 16027, 16031, 16032, 16044, 16070, 16100, 16117, 16129, 16136, 16137, 16140, 16149, 16152, 16175, 16186, 16201, 16241, 16261, 16269, 16272, 16279, 16290, 16308, 16319, 16324, 16325, 16327, 16338, 16341, 16345, 16352, 16354, 16371, 16372, 16385, 16391, 16416, 16423, 16448, 16455, 16468, 16470, 16477, 16481, 16484, 16491, 16493, 16500, 16509, 16510, 16516, 16541, 16559, 16577, 16589, 16632, 16638, 16642, 16643, 16649, 16650, 16654, 16665, 16677, 16684, 16704, 16720, 16726, 16734, 16765, 16774, 16775, 16777, 16780, 16797, 16798, 16799, 16803, 16807, 16809, 16830, 16832, 16861, 16863, 16868, 16876, 16889, 16890, 16902, 16915, 16927, 16939, 16949, 16962, 16968, 17002, 17010, 17013, 17021, 17023, 17029, 17062, 17066, 17084, 17093, 17094, 17102, 17152, 17160, 17167, 17173, 17183, 17221, 17239, 17250, 17267, 17277, 17278, 17282, 17296, 17307, 17315, 17333, 17336, 17344, 17346, 17348, 17354, 17365, 17374, 17385, 17387, 17397, 17401, 17404, 17412, 17417, 17432, 17436, 17444, 17452, 17467, 17469, 17501, 17502, 17513, 17530, 17531, 17547, 17550, 17555, 17573, 17577, 17578, 17586, 17600, 17602, 17611, 17627, 17640, 17645, 17648, 17649, 17652, 17667, 17676, 17682, 17721, 17724, 17729, 17731, 17751, 17754, 17762, 17770, 17772, 17781, 17802, 17820, 17823, 17833, 17848, 17857, 17858, 17865, 17867, 17887, 17893, 17909, 17924, 17926, 17945, 17971, 17977, 17986, 17990, 18003, 18011, 18042, 18046, 18052, 18077, 18086, 18091, 18099, 18103, 18123, 18128, 18131, 18142, 18153, 18165, 18167, 18173, 18181, 18206, 18226, 18252, 18263, 18276, 18277, 18284, 18314, 18319, 18327, 18330, 18342, 18352, 18355, 18362, 18364, 18375, 18383, 18398, 18403, 18432, 18447, 18453, 18465, 18467, 18485, 18486, 18492, 18505, 18508, 18510, 18515, 18536, 18540, 18548, 18550, 18557, 18563, 18570, 18574, 18576, 18616, 18622, 18627, 18639, 18649, 18655, 18673, 18681, 18712, 18745, 18746, 18758, 18773, 18778, 18782, 18826, 18832, 18834, 18842, 18851, 18853, 18858, 18862, 18867, 18872, 18877, 18908, 18910, 18912, 18913, 18919, 18925, 18929, 18930, 18933, 18936, 18944, 18974, 18975, 18997, 19009, 19011, 19014, 19016, 19028, 19057, 19098, 19106, 19114, 19130, 19133, 19147, 19157, 19162, 19166, 19173, 19179, 19198, 19213, 19225, 19230, 19239, 19241, 19266, 19274, 19279, 19283, 19285, 19316, 19327, 19335, 19352, 19358, 19362, 19366, 19377, 19379, 19405, 19423, 19428, 19431, 19437, 19444, 19456, 19469, 19475, 19476, 19481, 19486, 19492, 19506, 19507, 19536, 19537, 19562, 19586, 19589, 19622, 19634, 19651, 19663, 19679, 19685, 19690, 19697, 19707, 19712, 19714, 19715, 19717, 19724, 19734, 19744, 19754, 19769, 19779, 19792, 19807, 19830, 19850, 19863, 19875, 19903, 19904, 19908, 19963, 19985, 19988, 20003, 20012, 20014, 20018, 20040, 20042, 20054, 20078, 20079, 20083, 20087, 20089, 20114, 20121, 20122, 20138, 20155, 20162, 20186, 20199, 20216, 20222, 20234, 20238, 20254, 20285, 20299, 20302, 20305, 20337, 20345, 20364, 20365, 20374, 20382, 20394, 20403, 20440, 20450, 20451, 20476, 20506, 20508, 20516, 20527, 20536, 20540, 20541, 20542, 20557, 20566, 20584, 20608, 20611, 20613, 20618, 20627, 20644, 20656, 20663, 20674, 20682, 20690, 20699, 20707, 20711, 20737, 20751, 20765, 20771, 20774, 20778, 20779, 20789, 20795, 20798, 20834, 20851, 20865, 20867, 20869, 20872, 20883, 20892, 20894, 20895, 20896, 20901, 20919, 20924, 20933, 20937, 20938, 20946, 20947, 20955, 20957, 20958, 20965, 20966, 20970, 20974, 20991, 21001, 21013, 21018, 21020, 21025, 21114, 21142, 21205, 21210, 21227, 21230, 21236, 21253, 21262, 21294, 21300, 21316, 21331, 21339, 21349, 21356, 21365, 21369, 21376, 21399, 21410, 21413, 21418, 21420, 21435, 21447, 21452, 21455, 21468, 21470, 21474, 21486, 21495, 21501, 21517, 21530, 21539, 21546, 21547, 21553, 21556, 21564, 21569, 21579, 21593, 21595, 21599, 21603, 21632, 21643, 21690, 21713, 21726, 21732, 21737, 21738, 21744, 21755, 21773, 21782, 21800, 21807, 21829, 21839, 21848, 21876, 21893, 21923, 21926, 21931, 21958, 21967, 21970, 21973, 21982, 22002, 22029, 22031, 22035, 22044, 22051, 22068, 22073, 22099, 22117, 22121, 22126, 22145, 22146, 22149, 22161, 22166, 22170, 22177, 22205, 22210, 22231, 22233, 22234, 22244, 22246, 22251, 22272, 22280, 22299, 22317, 22321, 22327, 22350, 22355, 22363, 22365, 22378, 22383, 22387, 22391, 22393, 22395, 22403, 22406, 22418, 22423, 22428, 22437, 22444, 22451, 22453, 22460, 22480, 22491, 22510, 22530, 22536, 22554, 22557, 22561, 22562, 22564, 22571, 22588, 22593, 22606, 22607, 22610, 22611, 22642, 22666, 22717, 22729, 22738, 22744, 22745, 22763, 22768, 22769, 22774, 22782, 22789, 22796, 22800, 22836, 22841, 22843, 22858, 22864, 22886, 22889, 22897, 22925, 22928, 22938, 22944, 22945, 22948, 22950, 22956, 22983, 22986, 22988, 22993, 22994, 22997, 23028, 23030, 23039, 23047, 23081, 23091, 23097, 23105, 23108, 23111, 23113, 23133, 23145, 23150, 23161, 23172, 23176, 23199, 23200, 23202, 23204, 23205, 23211, 23212, 23226, 23227, 23231, 23236, 23239, 23263, 23265, 23266, 23269, 23290, 23311, 23315, 23320, 23323, 23340, 23352, 23361, 23372, 23375, 23376, 23381, 23386, 23390, 23401, 23415, 23418, 23428, 23429, 23453, 23467, 23479, 23505, 23508, 23527, 23528, 23535, 23537, 23541, 23553, 23555, 23557, 23559, 23563, 23566, 23581, 23585, 23596, 23598, 23604, 23605, 23617, 23630, 23643, 23658, 23669, 23673, 23680, 23683, 23690, 23691, 23701, 23707, 23714, 23720, 23725, 23733, 23746, 23750, 23751, 23769, 23774, 23784, 23809, 23818, 23831, 23839, 23842, 23857, 23924, 23930, 23944, 23945, 23947, 23959, 23971, 23975, 23987, 23989, 24002, 24003, 24009, 24019, 24026, 24039, 24041, 24046, 24062, 24071, 24077, 24079, 24087, 24096, 24119, 24121, 24137, 24150, 24169, 24181, 24194, 24195, 24199, 24200, 24203, 24210, 24223, 24258, 24272, 24274, 24291, 24293, 24300, 24305, 24318, 24339, 24378, 24380, 24383, 24392, 24402, 24442, 24468, 24476, 24480, 24488, 24490, 24495, 24502, 24504, 24505, 24516, 24562, 24572, 24594, 24595, 24597, 24600, 24606, 24609, 24622, 24623, 24629, 24631, 24632, 24633, 24657, 24662, 24664, 24666, 24682, 24697, 24709, 24753, 24758, 24766, 24833, 24840, 24851, 24858, 24869, 24886, 24909, 24915, 24916, 24929, 24945, 24951, 24966, 24972, 24980, 24996, 25012, 25017, 25043, 25059, 25067, 25104, 25121, 25125, 25127, 25133, 25150, 25191, 25210, 25224, 25228, 25240, 25243, 25255, 25264, 25267, 25272, 25282, 25284, 25290, 25292, 25293, 25334, 25338, 25350, 25365, 25368, 25380, 25389, 25391, 25400, 25413, 25422, 25440, 25444, 25446, 25456, 25460, 25472, 25486, 25495, 25499, 25509, 25512, 25528, 25530, 25532, 25547, 25559, 25591, 25593, 25597, 25601, 25614, 25616, 25638, 25672, 25691, 25698, 25712, 25713, 25721, 25722, 25725, 25732, 25753, 25760, 25775, 25794, 25813, 25819, 25828, 25833, 25901, 25908, 25947, 25983, 25987, 25989, 25996, 26008, 26017, 26025, 26030, 26039, 26047, 26054, 26058, 26077, 26088, 26092, 26112, 26183, 26185, 26190, 26201, 26204, 26237, 26239, 26241, 26248, 26251, 26252, 26256, 26263, 26268, 26282, 26314, 26331, 26346, 26358, 26360, 26364, 26377, 26428, 26434, 26460, 26481, 26485, 26497, 26532, 26533, 26538, 26561, 26567, 26585, 26588, 26595, 26596, 26618, 26625, 26633, 26643, 26651, 26677, 26687, 26715, 26721, 26732, 26733, 26746, 26760, 26762, 26769, 26779, 26781, 26822, 26828, 26830, 26870, 26882, 26883, 26889, 26892, 26894, 26914, 26927, 26932, 26944, 26953, 26957, 26963, 26967, 26973, 26976, 26978, 26985, 27030, 27038, 27044, 27048, 27052, 27067, 27111, 27115, 27121, 27142, 27144, 27153, 27167, 27205, 27209, 27217, 27218, 27229, 27233, 27240, 27250, 27280, 27302, 27332, 27339, 27349, 27362, 27376, 27386, 27396, 27401, 27413, 27426, 27429, 27456, 27470, 27474, 27476, 27477, 27480]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27481, 5)\n",
      "(24857, 5)\n",
      "(24856, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df = df[(~df.index.isin(anomalous_idxs))]\n",
    "print(df.shape)\n",
    "df = df[(~df.text.isna())]\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.copy()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment_code\"] = df[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments = df[\"sentiment_code\"].cat.codes.values\n",
    "\n",
    "test_df[\"sentiment_code\"] = test_df[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments_test = test_df[\"sentiment_code\"].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"selected_text\"] = df[\"selected_text\"].astype(str)\n",
    "test_df[\"text\"] = test_df[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_mod\"] = \"<s> \" + df.text.str.strip() + \" </s> \" + df.sentiment + \" </s>\"\n",
    "test_df[\"text_mod\"] = \"<s> \" + test_df.text.str.strip() + \" </s> \" + test_df.sentiment + \" </s>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCLUDE_NEUTRAL_CLASS: (14017, 7)\n"
     ]
    }
   ],
   "source": [
    "if EXCLUDE_NEUTRAL_CLASS:\n",
    "    df = df.loc[df.sentiment!=\"neutral\"].copy()\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(\"EXCLUDE_NEUTRAL_CLASS:\", df.shape)\n",
    "\n",
    "if RUN_ON_SAMPLE:\n",
    "    df = df.sample(3000).copy()\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(\"RUN_ON_SAMPLE\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../results/tokenizers/roberta_tokenizer/vocab.json',\n",
       " '../results/tokenizers/roberta_tokenizer/merges.txt',\n",
       " '../results/tokenizers/roberta_tokenizer/special_tokens_map.json',\n",
       " '../results/tokenizers/roberta_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"../results/tokenizers/roberta_tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=\"../results/tokenizers/roberta_tokenizer/vocab.json\",\n",
    "    merges_file=\"../results/tokenizers/roberta_tokenizer/merges.txt\",\n",
    "    add_prefix_space=True,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../results/tokenizers/roberta_tokenizer/special_tokens_map.json\") as f:\n",
    "    special_tokens = json.load(f)\n",
    "\n",
    "tokenizer.add_special_tokens([i for i in special_tokens.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens = tokenizer.encode_batch(df.text_mod.tolist())\n",
    "Y_tokens = tokenizer.encode_batch(df.selected_text.tolist())\n",
    "X_tokens_test = tokenizer.encode_batch(test_df.text_mod.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i.ids for i in X_tokens]\n",
    "Y = [i.ids for i in Y_tokens]\n",
    "X_test = [i.ids for i in X_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_att = [i.attention_mask for i in X_tokens]\n",
    "Y_att = [i.attention_mask for i in Y_tokens]\n",
    "X_att_test = [i.attention_mask for i in X_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265 76\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(i) for i in X])\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "print(VOCAB_SIZE, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremities(l_string, s_string, print_it=False):\n",
    "    len_l = len(l_string)\n",
    "    len_s = len(s_string)\n",
    "    \n",
    "    for i in range(len_l - len_s + 1):\n",
    "        if (i + len_s) <= len_l:\n",
    "            substring = l_string[i:i+len_s]\n",
    "            if substring == s_string:\n",
    "                if print_it:\n",
    "                    print(l_string)\n",
    "                    print(substring)\n",
    "                    print(i, i+len_s, substring)\n",
    "                \n",
    "                start_vector, end_vector = [0] * len_l, [0] * len_l\n",
    "                start_vector[i], end_vector[i+len_s-1] = 1, 1\n",
    "                \n",
    "                return (start_vector, end_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14017 \t: #Processed\n",
      "0 \t: # of Anomalies\n"
     ]
    }
   ],
   "source": [
    "Y_starts, Y_stops = [], []\n",
    "anomaly_idx, counter = [], 0\n",
    "for num, (i,j) in enumerate(zip(X_tokens, Y_tokens)):\n",
    "    x,y = i.ids, j.ids\n",
    "    #s,e = get_extremities(x, y)\n",
    "    try:\n",
    "        s,e = get_extremities(x, y)\n",
    "    except TypeError as t:\n",
    "        counter += 1\n",
    "        anomaly_idx.append(num)\n",
    "    Y_starts.append(s)\n",
    "    Y_stops.append(e)\n",
    "print(num + 1, \"\\t: #Processed\")\n",
    "\n",
    "print(counter,\"\\t: # of Anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if counter:\n",
    "    print(anomaly_idx, sep=\",\")\n",
    "    print(df[\"original_index\"].iloc[anomaly_idx].tolist())\n",
    "    df.iloc[anomaly_idx].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcfly gig last nightt omg it was amazin didnt sit down through the whole thing  mcfly did you see me and ma best mate we were in tutus\n",
      "it was amazin\n",
      "[['<s>', 0, 0, 0], ['Ġmc', 44355, 0, 0], ['fly', 19252, 0, 0], ['Ġgig', 10196, 0, 0], ['Ġlast', 94, 0, 0], ['Ġnight', 363, 0, 0], ['t', 90, 0, 0], ['Ġo', 1021, 0, 0], ['mg', 22984, 0, 0], ['Ġit', 24, 1, 0], ['Ġwas', 21, 0, 0], ['Ġamaz', 42402, 0, 0], ['in', 179, 0, 1], ['Ġdidnt', 46405, 0, 0], ['Ġsit', 2662, 0, 0], ['Ġdown', 159, 0, 0], ['Ġthrough', 149, 0, 0], ['Ġthe', 5, 0, 0], ['Ġwhole', 1086, 0, 0], ['Ġthing', 631, 0, 0], ['Ġ', 1437, 0, 0], ['Ġmc', 44355, 0, 0], ['fly', 19252, 0, 0], ['Ġdid', 222, 0, 0], ['Ġyou', 47, 0, 0], ['Ġsee', 192, 0, 0], ['Ġme', 162, 0, 0], ['Ġand', 8, 0, 0], ['Ġma', 9131, 0, 0], ['Ġbest', 275, 0, 0], ['Ġmate', 12563, 0, 0], ['Ġwe', 52, 0, 0], ['Ġwere', 58, 0, 0], ['Ġin', 11, 0, 0], ['Ġtut', 15511, 0, 0], ['us', 687, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0], ['Ġpositive', 1313, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0]]\n",
      "[[24, 'Ġit'], [21, 'Ġwas'], [42402, 'Ġamaz'], [179, 'in']]\n"
     ]
    }
   ],
   "source": [
    "check_idx = 156\n",
    "print(df.text[check_idx])\n",
    "print(df.selected_text[check_idx])\n",
    "print([[i,j,k,l] for i,j,k,l in zip(X_tokens[check_idx].tokens,\n",
    "                                    X_tokens[check_idx].ids,\n",
    "                                    Y_starts[check_idx],\n",
    "                                    Y_stops[check_idx])])\n",
    "print([[i,j] for i,j in zip(Y_tokens[check_idx].ids,\n",
    "                            Y_tokens[check_idx].tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X, maxlen=max_len, padding=\"post\")\n",
    "X_att = pad_sequences(X_att, maxlen=max_len, padding=\"post\")\n",
    "Y = pad_sequences(Y, maxlen=max_len, padding=\"post\")\n",
    "Y_starts = pad_sequences(Y_starts, maxlen=max_len, padding=\"post\").argmax(axis=1)\n",
    "Y_stops = pad_sequences(Y_stops, maxlen=max_len, padding=\"post\").argmax(axis=1)\n",
    "\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding=\"post\")\n",
    "X_att_test = pad_sequences(X_att_test, maxlen=max_len, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_flag = np.isin(Y_stops, np.unique(Y_stops, return_counts=True)[0][np.unique(Y_stops, return_counts=True)[1]>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (14017, 76) \t: X  \n",
      " (14017, 76) \t: X_att  \n",
      " (14017, 76) \t: Y  \n",
      " (14017,) \t: Y_starts  \n",
      " (14017,) \t: Y_stops  \n",
      " (3534, 76) \t: X_test  \n",
      " (3534, 76) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X.shape, \"\\t: X \", \"\\n\",\n",
    "     X_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[keep_flag]\n",
    "X_att = X_att[keep_flag]\n",
    "Y = Y[keep_flag]\n",
    "Y_starts = Y_starts[keep_flag]\n",
    "Y_stops = Y_stops[keep_flag]\n",
    "X_test = X_test\n",
    "X_att_test = X_att_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (14015, 76) \t: X  \n",
      " (14015, 76) \t: X_att  \n",
      " (14015, 76) \t: Y  \n",
      " (14015,) \t: Y_starts  \n",
      " (14015,) \t: Y_stops  \n",
      " (3534, 76) \t: X_test  \n",
      " (3534, 76) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X.shape, \"\\t: X \", \"\\n\",\n",
    "     X_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sss = StratifiedShuffleSplit(n_splits=NUM_FOLDS, train_size=TRAIN_SPLIT_RATIO, random_state=0)\n",
    "#sss.get_n_splits(X, Y_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "sss.get_n_splits(X, Y_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_words = [tokenizer.decode(i) for i in Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=6a6L_9USZxg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_FOLDS):\n",
    "    if os.path.exists(\"../results/\"+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\"):\n",
    "        os.remove(\"../results/\"+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(pred_dict):\n",
    "    print(\"[INFO] \",\"=\"*15,\"Validation for FOLD#\", num, \"=\"*15)\n",
    "    funcs = [accuracy_score, f1_score, precision_score, recall_score, confusion_matrix]\n",
    "    for f in funcs:\n",
    "        for data_set in [\"train\",\"valid\"]:\n",
    "            for var in [\"starts\", \"stops\"]:\n",
    "                if f in [accuracy_score]:\n",
    "                    res = f(**pred_dict[data_set][var])\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "                elif f in [confusion_matrix]:\n",
    "                    res = f(**pred_dict[data_set][var], labels=np.arange(max_len))\n",
    "                    np.savetxt(X=res, fmt='%i', delimiter=\",\",\n",
    "                               fname=\"../results/ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                    print(\"[INFO] \\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__, \"\\t||\", \n",
    "                          \"../results/ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                else:\n",
    "                    res = f(**pred_dict[data_set][var], average=\"macro\")\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "        print(\"=======================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(string):\n",
    "    string = re.sub(pattern=\" (negative|positive|neutral)[ ]+$\", repl=\"\", string=string)\n",
    "    string = re.sub(pattern=\" +\", repl=\" \", string=string.strip())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_sequences = Input((max_len), dtype=tf.int32, name=\"words\")\n",
    "    input_att_flags = Input((max_len), dtype=tf.int32, name=\"att_flags\")\n",
    "    input_token_ids = Input((max_len), dtype=tf.int32, name=\"token_ids\")\n",
    "\n",
    "    config = RobertaConfig.from_pretrained('roberta-base')\n",
    "    roberta_model = TFRobertaModel.from_pretrained('roberta-base', config=config)\n",
    "    x = roberta_model(inputs=input_sequences, attention_mask=input_att_flags, token_type_ids=input_token_ids)\n",
    "    \n",
    "    x_conv = Reshape((max_len, 768, 1))(x[0])\n",
    "    x_conv = Conv2D(filters=2, kernel_size=(1,768), data_format=\"channels_last\")(x_conv)\n",
    "    x_conv = BatchNormalization()(x_conv)\n",
    "    x_conv = Dropout(DROPOUT)(x_conv)\n",
    "    x_conv = Flatten()(x_conv)\n",
    "    \n",
    "    x_lstm = Bidirectional(LSTM(2, activation='relu', return_sequences=False))(x[0])\n",
    "    x_lstm = BatchNormalization()(x_lstm)\n",
    "    x_lstm = Dropout(DROPOUT)(x_lstm)\n",
    "    x_lstm = Flatten()(x_lstm)\n",
    "    \n",
    "    x_dense = TimeDistributed(Dense(2, activation='relu'))(x[0])\n",
    "    x_dense = BatchNormalization()(x_dense)\n",
    "    x_dense = Dropout(DROPOUT)(x_dense)\n",
    "    x_dense = Flatten()(x_dense)\n",
    "    \n",
    "    x_flat = concatenate([x_conv, x_lstm, x_dense])\n",
    "    \n",
    "    output_starts_0 = Dense(max_len, activation='softmax', name=\"starts_0\")(x_flat)\n",
    "    output_stops_0 = Dense(max_len, activation='softmax', name=\"stops_0\")(x_flat)\n",
    "    \n",
    "    # How do we enforce start index is less than stop index?\n",
    "    output_subtract = tf.keras.layers.Subtract()([output_starts_0, output_stops_0])\n",
    "    output_flat = concatenate([output_starts_0, output_stops_0, output_subtract])\n",
    "    output_starts_1 = Dense(max_len, activation='softmax', name=\"starts_1\")(output_flat)\n",
    "    output_stops_1 = Dense(max_len, activation='softmax', name=\"stops_1\")(output_flat)\n",
    "    \n",
    "    model = Model([input_att_flags, input_sequences, input_token_ids],\n",
    "                  [output_starts_0, output_stops_0, output_starts_1, output_stops_1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 76)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 76)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 76)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 76, 768), (N 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 76, 768, 1)   0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 76, 1, 2)     1538        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 4)            12336       tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 76, 2)        1538        tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 76, 1, 2)     8           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4)            16          bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 76, 2)        8           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 76, 1, 2)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 4)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 76, 2)        0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 152)          0           dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4)            0           dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 152)          0           dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 308)          0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "starts_0 (Dense)                (None, 76)           23484       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "stops_0 (Dense)                 (None, 76)           23484       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 76)           0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 228)          0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "                                                                 subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "starts_1 (Dense)                (None, 76)           17404       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "stops_1 (Dense)                 (None, 76)           17404       concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 124,742,852\n",
      "Trainable params: 124,742,836\n",
      "Non-trainable params: 16\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><h2 id=\"finetuning\">Fine-tuning</h2></a>\n",
    "<p>Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.</p>\n",
    " <p>This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.</p>\n",
    " <p>It is critical to only do this step <em>after</em> the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.</p>\n",
    " <p>It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>\n",
    "\n",
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><p><strong>Important note about <code>compile()</code> and <code>trainable</code></strong></p></a>\n",
    "<p>Calling <code>compile()</code> on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the <code>trainable</code>\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until <code>compile</code> is called again. Hence, if you change any <code>trainable</code> value, make sure\n",
    " to call <code>compile()</code> again on your\n",
    "model for your changes to be taken into account.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ==================== FOLD# 0 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 2803 samples, validate on 11212 samples\n",
      "Epoch 1/10\n",
      "2803/2803 [==============================] - 162s 58ms/sample - loss: 13.2398 - starts_0_loss: 2.9181 - stops_0_loss: 3.3234 - starts_1_loss: 3.3520 - stops_1_loss: 3.6649 - starts_0_accuracy: 0.2954 - stops_0_accuracy: 0.1195 - starts_1_accuracy: 0.3154 - stops_1_accuracy: 0.0599 - val_loss: 11.2262 - val_starts_0_loss: 2.5268 - val_stops_0_loss: 2.6741 - val_starts_1_loss: 2.7239 - val_stops_1_loss: 3.3013 - val_starts_0_accuracy: 0.3563 - val_stops_0_accuracy: 0.2434 - val_starts_1_accuracy: 0.3551 - val_stops_1_accuracy: 0.0684\n",
      "Epoch 2/10\n",
      "2803/2803 [==============================] - 143s 51ms/sample - loss: 10.4848 - starts_0_loss: 2.2832 - stops_0_loss: 2.4907 - starts_1_loss: 2.6045 - stops_1_loss: 3.1157 - starts_0_accuracy: 0.3425 - stops_0_accuracy: 0.2868 - starts_1_accuracy: 0.3446 - stops_1_accuracy: 0.1541 - val_loss: 9.4856 - val_starts_0_loss: 2.1001 - val_stops_0_loss: 2.0921 - val_starts_1_loss: 2.4270 - val_stops_1_loss: 2.8663 - val_starts_0_accuracy: 0.3743 - val_stops_0_accuracy: 0.3956 - val_starts_1_accuracy: 0.3551 - val_stops_1_accuracy: 0.2890\n",
      "Epoch 3/10\n",
      "2803/2803 [==============================] - 143s 51ms/sample - loss: 9.3586 - starts_0_loss: 2.0728 - stops_0_loss: 2.1891 - starts_1_loss: 2.3499 - stops_1_loss: 2.7419 - starts_0_accuracy: 0.3739 - stops_0_accuracy: 0.3824 - starts_1_accuracy: 0.3464 - stops_1_accuracy: 0.3164 - val_loss: 8.9765 - val_starts_0_loss: 2.0472 - val_stops_0_loss: 2.0276 - val_starts_1_loss: 2.2849 - val_stops_1_loss: 2.6170 - val_starts_0_accuracy: 0.3918 - val_stops_0_accuracy: 0.4069 - val_starts_1_accuracy: 0.3589 - val_stops_1_accuracy: 0.3479\n",
      "Epoch 4/10\n",
      "2803/2803 [==============================] - 144s 51ms/sample - loss: 8.7477 - starts_0_loss: 2.0010 - stops_0_loss: 2.0469 - starts_1_loss: 2.2074 - stops_1_loss: 2.4785 - starts_0_accuracy: 0.3946 - stops_0_accuracy: 0.4238 - starts_1_accuracy: 0.3596 - stops_1_accuracy: 0.3789 - val_loss: 8.3574 - val_starts_0_loss: 1.9177 - val_stops_0_loss: 1.8946 - val_starts_1_loss: 2.1472 - val_stops_1_loss: 2.3980 - val_starts_0_accuracy: 0.4011 - val_stops_0_accuracy: 0.4291 - val_starts_1_accuracy: 0.3703 - val_stops_1_accuracy: 0.3949\n",
      "Epoch 5/10\n",
      "2803/2803 [==============================] - 142s 51ms/sample - loss: 8.1964 - starts_0_loss: 1.8949 - stops_0_loss: 1.9377 - starts_1_loss: 2.0784 - stops_1_loss: 2.2992 - starts_0_accuracy: 0.4231 - stops_0_accuracy: 0.4488 - starts_1_accuracy: 0.3874 - stops_1_accuracy: 0.4196 - val_loss: 8.2043 - val_starts_0_loss: 1.8904 - val_stops_0_loss: 1.9067 - val_starts_1_loss: 2.1038 - val_stops_1_loss: 2.3034 - val_starts_0_accuracy: 0.4124 - val_stops_0_accuracy: 0.4435 - val_starts_1_accuracy: 0.3821 - val_stops_1_accuracy: 0.4180\n",
      "Epoch 6/10\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 7.8996 - starts_0_loss: 1.8521 - stops_0_loss: 1.8658 - starts_1_loss: 2.0036 - stops_1_loss: 2.1775 - starts_0_accuracy: 0.4377 - stops_0_accuracy: 0.4609 - starts_1_accuracy: 0.4042 - stops_1_accuracy: 0.4509 - val_loss: 8.1236 - val_starts_0_loss: 1.9305 - val_stops_0_loss: 1.8782 - val_starts_1_loss: 2.0899 - val_stops_1_loss: 2.2251 - val_starts_0_accuracy: 0.4066 - val_stops_0_accuracy: 0.4531 - val_starts_1_accuracy: 0.3872 - val_stops_1_accuracy: 0.4294\n",
      "Epoch 7/10\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 7.6473 - starts_0_loss: 1.7767 - stops_0_loss: 1.8376 - starts_1_loss: 1.9124 - stops_1_loss: 2.1095 - starts_0_accuracy: 0.4470 - stops_0_accuracy: 0.4677 - starts_1_accuracy: 0.4356 - stops_1_accuracy: 0.4527 - val_loss: 7.9592 - val_starts_0_loss: 1.8744 - val_stops_0_loss: 1.8687 - val_starts_1_loss: 2.0322 - val_stops_1_loss: 2.1839 - val_starts_0_accuracy: 0.4176 - val_stops_0_accuracy: 0.4470 - val_starts_1_accuracy: 0.3996 - val_stops_1_accuracy: 0.4381\n",
      "Epoch 8/10\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 7.4177 - starts_0_loss: 1.7414 - stops_0_loss: 1.7744 - starts_1_loss: 1.8753 - stops_1_loss: 2.0232 - starts_0_accuracy: 0.4691 - stops_0_accuracy: 0.4870 - starts_1_accuracy: 0.4410 - stops_1_accuracy: 0.4688 - val_loss: 7.9854 - val_starts_0_loss: 1.8990 - val_stops_0_loss: 1.8650 - val_starts_1_loss: 2.0258 - val_stops_1_loss: 2.1955 - val_starts_0_accuracy: 0.4178 - val_stops_0_accuracy: 0.4500 - val_starts_1_accuracy: 0.4014 - val_stops_1_accuracy: 0.4336\n",
      "Epoch 9/10\n",
      "2803/2803 [==============================] - 141s 50ms/sample - loss: 7.2273 - starts_0_loss: 1.7118 - stops_0_loss: 1.7383 - starts_1_loss: 1.8189 - stops_1_loss: 1.9635 - starts_0_accuracy: 0.4620 - stops_0_accuracy: 0.5016 - starts_1_accuracy: 0.4527 - stops_1_accuracy: 0.4756 - val_loss: 8.0698 - val_starts_0_loss: 1.9135 - val_stops_0_loss: 1.9359 - val_starts_1_loss: 2.0291 - val_stops_1_loss: 2.1915 - val_starts_0_accuracy: 0.4127 - val_stops_0_accuracy: 0.4403 - val_starts_1_accuracy: 0.4024 - val_stops_1_accuracy: 0.4286\n",
      "Epoch 10/10\n",
      "2803/2803 [==============================] - 139s 50ms/sample - loss: 6.9882 - starts_0_loss: 1.6368 - stops_0_loss: 1.6904 - starts_1_loss: 1.7502 - stops_1_loss: 1.8982 - starts_0_accuracy: 0.4859 - stops_0_accuracy: 0.5087 - starts_1_accuracy: 0.4813 - stops_1_accuracy: 0.5034 - val_loss: 8.0727 - val_starts_0_loss: 1.9183 - val_stops_0_loss: 1.8608 - val_starts_1_loss: 2.0874 - val_stops_1_loss: 2.2060 - val_starts_0_accuracy: 0.4051 - val_stops_0_accuracy: 0.4535 - val_starts_1_accuracy: 0.3980 - val_stops_1_accuracy: 0.4321\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 2803 samples, validate on 11212 samples\n",
      "Epoch 1/5\n",
      "2803/2803 [==============================] - 239s 85ms/sample - loss: 6.5862 - starts_0_loss: 1.5326 - stops_0_loss: 1.5929 - starts_1_loss: 1.6647 - stops_1_loss: 1.8010 - starts_0_accuracy: 0.5198 - stops_0_accuracy: 0.5291 - starts_1_accuracy: 0.5005 - stops_1_accuracy: 0.5273 - val_loss: 7.7826 - val_starts_0_loss: 1.8334 - val_stops_0_loss: 1.8426 - val_starts_1_loss: 1.9885 - val_stops_1_loss: 2.1180 - val_starts_0_accuracy: 0.4188 - val_stops_0_accuracy: 0.4500 - val_starts_1_accuracy: 0.4086 - val_stops_1_accuracy: 0.4429\n",
      "Epoch 2/5\n",
      "2803/2803 [==============================] - 141s 50ms/sample - loss: 6.5648 - starts_0_loss: 1.5406 - stops_0_loss: 1.5711 - starts_1_loss: 1.6752 - stops_1_loss: 1.7676 - starts_0_accuracy: 0.5066 - stops_0_accuracy: 0.5359 - starts_1_accuracy: 0.4923 - stops_1_accuracy: 0.5319 - val_loss: 7.7548 - val_starts_0_loss: 1.8303 - val_stops_0_loss: 1.8431 - val_starts_1_loss: 1.9730 - val_stops_1_loss: 2.1082 - val_starts_0_accuracy: 0.4219 - val_stops_0_accuracy: 0.4497 - val_starts_1_accuracy: 0.4128 - val_stops_1_accuracy: 0.4440\n",
      "Epoch 3/5\n",
      "2803/2803 [==============================] - 139s 50ms/sample - loss: 6.5469 - starts_0_loss: 1.5414 - stops_0_loss: 1.5518 - starts_1_loss: 1.6877 - stops_1_loss: 1.7653 - starts_0_accuracy: 0.5041 - stops_0_accuracy: 0.5344 - starts_1_accuracy: 0.4952 - stops_1_accuracy: 0.5355 - val_loss: 7.7455 - val_starts_0_loss: 1.8289 - val_stops_0_loss: 1.8419 - val_starts_1_loss: 1.9684 - val_stops_1_loss: 2.1063 - val_starts_0_accuracy: 0.4237 - val_stops_0_accuracy: 0.4503 - val_starts_1_accuracy: 0.4130 - val_stops_1_accuracy: 0.4458\n",
      "Epoch 4/5\n",
      "2803/2803 [==============================] - 142s 51ms/sample - loss: 6.5056 - starts_0_loss: 1.5251 - stops_0_loss: 1.5619 - starts_1_loss: 1.6643 - stops_1_loss: 1.7625 - starts_0_accuracy: 0.5177 - stops_0_accuracy: 0.5380 - starts_1_accuracy: 0.5034 - stops_1_accuracy: 0.5387 - val_loss: 7.7406 - val_starts_0_loss: 1.8275 - val_stops_0_loss: 1.8378 - val_starts_1_loss: 1.9684 - val_stops_1_loss: 2.1069 - val_starts_0_accuracy: 0.4241 - val_stops_0_accuracy: 0.4511 - val_starts_1_accuracy: 0.4143 - val_stops_1_accuracy: 0.4466\n",
      "Epoch 5/5\n",
      "2803/2803 [==============================] - 141s 50ms/sample - loss: 6.5163 - starts_0_loss: 1.5349 - stops_0_loss: 1.5484 - starts_1_loss: 1.6806 - stops_1_loss: 1.7484 - starts_0_accuracy: 0.5048 - stops_0_accuracy: 0.5384 - starts_1_accuracy: 0.4873 - stops_1_accuracy: 0.5480 - val_loss: 7.7352 - val_starts_0_loss: 1.8264 - val_stops_0_loss: 1.8362 - val_starts_1_loss: 1.9665 - val_stops_1_loss: 2.1060 - val_starts_0_accuracy: 0.4252 - val_stops_0_accuracy: 0.4511 - val_starts_1_accuracy: 0.4148 - val_stops_1_accuracy: 0.4466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 2803 samples, validate on 11212 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2803/2803 [==============================] - 199s 71ms/sample - loss: 6.4054 - starts_0_loss: 1.5258 - stops_0_loss: 1.5167 - starts_1_loss: 1.6458 - stops_1_loss: 1.7350 - starts_0_accuracy: 0.5144 - stops_0_accuracy: 0.5419 - starts_1_accuracy: 0.5037 - stops_1_accuracy: 0.5405 - val_loss: 7.7084 - val_starts_0_loss: 1.8192 - val_stops_0_loss: 1.8354 - val_starts_1_loss: 1.9583 - val_stops_1_loss: 2.0955 - val_starts_0_accuracy: 0.4276 - val_stops_0_accuracy: 0.4540 - val_starts_1_accuracy: 0.4159 - val_stops_1_accuracy: 0.4486\n",
      "Epoch 2/2\n",
      "2803/2803 [==============================] - 173s 62ms/sample - loss: 6.3406 - starts_0_loss: 1.4679 - stops_0_loss: 1.5156 - starts_1_loss: 1.6077 - stops_1_loss: 1.7335 - starts_0_accuracy: 0.5309 - stops_0_accuracy: 0.5430 - starts_1_accuracy: 0.5109 - stops_1_accuracy: 0.5423 - val_loss: 7.6565 - val_starts_0_loss: 1.8093 - val_stops_0_loss: 1.8129 - val_starts_1_loss: 1.9457 - val_stops_1_loss: 2.0886 - val_starts_0_accuracy: 0.4315 - val_stops_0_accuracy: 0.4575 - val_starts_1_accuracy: 0.4194 - val_stops_1_accuracy: 0.4537\n",
      "[INFO]  =============== Validation for FOLD# 0 ===============\n",
      "[INFO] 64.79 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 67.14 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 43.15 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 45.75 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 71.36 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 74.37 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 19.31 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 35.62 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 75.52 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 72.19 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 24.30 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 34.34 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 69.78 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 77.62 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 18.16 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 37.95 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_train_stops.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (2803, 76) (2803, 76)\n",
      "[INFO] Prediction shape for validation data:  (11212, 76) (11212, 76)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  2334 out of 2803\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  9822 out of 11212\n",
      "[INFO] Training Jaccard Score:  0.5672471509136416\n",
      "[INFO] Validation Jaccard Score:  0.5049976616119369\n",
      "[INFO] Training for fold: 0 finished at Mon May 18 17:14:16 2020\n",
      "[INFO] ==================== FOLD# 1 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 2803 samples, validate on 11212 samples\n",
      "Epoch 1/10\n",
      "2803/2803 [==============================] - 161s 57ms/sample - loss: 12.9240 - starts_0_loss: 2.8275 - stops_0_loss: 3.1596 - starts_1_loss: 3.2964 - stops_1_loss: 3.6347 - starts_0_accuracy: 0.3254 - stops_0_accuracy: 0.1548 - starts_1_accuracy: 0.3293 - stops_1_accuracy: 0.0639 - val_loss: 11.2284 - val_starts_0_loss: 2.5422 - val_stops_0_loss: 2.6764 - val_starts_1_loss: 2.7155 - val_stops_1_loss: 3.2943 - val_starts_0_accuracy: 0.3530 - val_stops_0_accuracy: 0.2350 - val_starts_1_accuracy: 0.3533 - val_stops_1_accuracy: 0.0695\n",
      "Epoch 2/10\n",
      "2803/2803 [==============================] - 141s 50ms/sample - loss: 10.6369 - starts_0_loss: 2.3077 - stops_0_loss: 2.5843 - starts_1_loss: 2.6038 - stops_1_loss: 3.1385 - starts_0_accuracy: 0.3585 - stops_0_accuracy: 0.2733 - starts_1_accuracy: 0.3518 - stops_1_accuracy: 0.1345 - val_loss: 9.8804 - val_starts_0_loss: 2.1809 - val_stops_0_loss: 2.2778 - val_starts_1_loss: 2.4597 - val_stops_1_loss: 2.9621 - val_starts_0_accuracy: 0.3694 - val_stops_0_accuracy: 0.3396 - val_starts_1_accuracy: 0.3533 - val_stops_1_accuracy: 0.2200\n",
      "Epoch 3/10\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 9.6835 - starts_0_loss: 2.1134 - stops_0_loss: 2.3523 - starts_1_loss: 2.3901 - stops_1_loss: 2.8256 - starts_0_accuracy: 0.3910 - stops_0_accuracy: 0.3396 - starts_1_accuracy: 0.3539 - stops_1_accuracy: 0.2686 - val_loss: 9.2026 - val_starts_0_loss: 2.0963 - val_stops_0_loss: 2.1355 - val_starts_1_loss: 2.3086 - val_stops_1_loss: 2.6623 - val_starts_0_accuracy: 0.3620 - val_stops_0_accuracy: 0.3968 - val_starts_1_accuracy: 0.3595 - val_stops_1_accuracy: 0.3618\n",
      "Epoch 4/10\n",
      "2803/2803 [==============================] - 141s 50ms/sample - loss: 8.9591 - starts_0_loss: 1.9866 - stops_0_loss: 2.1754 - starts_1_loss: 2.2219 - stops_1_loss: 2.5734 - starts_0_accuracy: 0.4046 - stops_0_accuracy: 0.3903 - starts_1_accuracy: 0.3653 - stops_1_accuracy: 0.3282 - val_loss: 8.5207 - val_starts_0_loss: 1.9238 - val_stops_0_loss: 2.0091 - val_starts_1_loss: 2.1568 - val_stops_1_loss: 2.4309 - val_starts_0_accuracy: 0.3974 - val_stops_0_accuracy: 0.4217 - val_starts_1_accuracy: 0.3686 - val_stops_1_accuracy: 0.3953\n",
      "Epoch 5/10\n",
      "2803/2803 [==============================] - 141s 50ms/sample - loss: 8.6033 - starts_0_loss: 1.9372 - stops_0_loss: 2.1304 - starts_1_loss: 2.1214 - stops_1_loss: 2.4225 - starts_0_accuracy: 0.4217 - stops_0_accuracy: 0.4049 - starts_1_accuracy: 0.3846 - stops_1_accuracy: 0.3735 - val_loss: 8.3823 - val_starts_0_loss: 1.9806 - val_stops_0_loss: 1.9625 - val_starts_1_loss: 2.1302 - val_stops_1_loss: 2.3092 - val_starts_0_accuracy: 0.3848 - val_stops_0_accuracy: 0.4361 - val_starts_1_accuracy: 0.3828 - val_stops_1_accuracy: 0.4300\n",
      "Epoch 6/10\n",
      "2803/2803 [==============================] - 204s 73ms/sample - loss: 8.3401 - starts_0_loss: 1.8678 - stops_0_loss: 2.0918 - starts_1_loss: 2.0401 - stops_1_loss: 2.3439 - starts_0_accuracy: 0.4224 - stops_0_accuracy: 0.4185 - starts_1_accuracy: 0.4078 - stops_1_accuracy: 0.3910 - val_loss: 8.1005 - val_starts_0_loss: 1.8531 - val_stops_0_loss: 1.9221 - val_starts_1_loss: 2.0332 - val_stops_1_loss: 2.2920 - val_starts_0_accuracy: 0.4114 - val_stops_0_accuracy: 0.4315 - val_starts_1_accuracy: 0.3906 - val_stops_1_accuracy: 0.4064\n",
      "Epoch 7/10\n",
      "2803/2803 [==============================] - 145s 52ms/sample - loss: 8.0845 - starts_0_loss: 1.8535 - stops_0_loss: 2.0113 - starts_1_loss: 1.9767 - stops_1_loss: 2.2373 - starts_0_accuracy: 0.4388 - stops_0_accuracy: 0.4335 - starts_1_accuracy: 0.4156 - stops_1_accuracy: 0.4085 - val_loss: 7.9712 - val_starts_0_loss: 1.8798 - val_stops_0_loss: 1.8765 - val_starts_1_loss: 2.0178 - val_stops_1_loss: 2.1974 - val_starts_0_accuracy: 0.4035 - val_stops_0_accuracy: 0.4443 - val_starts_1_accuracy: 0.3937 - val_stops_1_accuracy: 0.4276\n",
      "Epoch 8/10\n",
      "2803/2803 [==============================] - 142s 51ms/sample - loss: 7.8750 - starts_0_loss: 1.8427 - stops_0_loss: 1.9378 - starts_1_loss: 1.9516 - stops_1_loss: 2.1539 - starts_0_accuracy: 0.4338 - stops_0_accuracy: 0.4563 - starts_1_accuracy: 0.4199 - stops_1_accuracy: 0.4435 - val_loss: 7.9774 - val_starts_0_loss: 1.8893 - val_stops_0_loss: 1.8974 - val_starts_1_loss: 2.0147 - val_stops_1_loss: 2.1761 - val_starts_0_accuracy: 0.4065 - val_stops_0_accuracy: 0.4478 - val_starts_1_accuracy: 0.3948 - val_stops_1_accuracy: 0.4403\n",
      "Epoch 9/10\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 7.8235 - starts_0_loss: 1.8343 - stops_0_loss: 1.9302 - starts_1_loss: 1.9327 - stops_1_loss: 2.1305 - starts_0_accuracy: 0.4352 - stops_0_accuracy: 0.4463 - starts_1_accuracy: 0.4295 - stops_1_accuracy: 0.4313 - val_loss: 7.9010 - val_starts_0_loss: 1.8366 - val_stops_0_loss: 1.8829 - val_starts_1_loss: 2.0005 - val_stops_1_loss: 2.1812 - val_starts_0_accuracy: 0.4173 - val_stops_0_accuracy: 0.4506 - val_starts_1_accuracy: 0.3962 - val_stops_1_accuracy: 0.4380\n",
      "Epoch 10/10\n",
      "2803/2803 [==============================] - 141s 50ms/sample - loss: 7.5970 - starts_0_loss: 1.7943 - stops_0_loss: 1.8745 - starts_1_loss: 1.8685 - stops_1_loss: 2.0694 - starts_0_accuracy: 0.4513 - stops_0_accuracy: 0.4709 - starts_1_accuracy: 0.4427 - stops_1_accuracy: 0.4460 - val_loss: 7.8592 - val_starts_0_loss: 1.8530 - val_stops_0_loss: 1.8832 - val_starts_1_loss: 1.9923 - val_stops_1_loss: 2.1308 - val_starts_0_accuracy: 0.4159 - val_stops_0_accuracy: 0.4468 - val_starts_1_accuracy: 0.3975 - val_stops_1_accuracy: 0.4445\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 2803 samples, validate on 11212 samples\n",
      "Epoch 1/5\n",
      "2803/2803 [==============================] - 160s 57ms/sample - loss: 7.2259 - starts_0_loss: 1.6970 - stops_0_loss: 1.7571 - starts_1_loss: 1.8248 - stops_1_loss: 1.9605 - starts_0_accuracy: 0.4759 - stops_0_accuracy: 0.5041 - starts_1_accuracy: 0.4584 - stops_1_accuracy: 0.4873 - val_loss: 7.7824 - val_starts_0_loss: 1.8361 - val_stops_0_loss: 1.8498 - val_starts_1_loss: 1.9762 - val_stops_1_loss: 2.1204 - val_starts_0_accuracy: 0.4226 - val_stops_0_accuracy: 0.4480 - val_starts_1_accuracy: 0.4016 - val_stops_1_accuracy: 0.4491\n",
      "Epoch 2/5\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 7.3562 - starts_0_loss: 1.7097 - stops_0_loss: 1.8024 - starts_1_loss: 1.8323 - stops_1_loss: 2.0116 - starts_0_accuracy: 0.4773 - stops_0_accuracy: 0.4813 - starts_1_accuracy: 0.4509 - stops_1_accuracy: 0.4663 - val_loss: 7.7743 - val_starts_0_loss: 1.8363 - val_stops_0_loss: 1.8487 - val_starts_1_loss: 1.9713 - val_stops_1_loss: 2.1180 - val_starts_0_accuracy: 0.4224 - val_stops_0_accuracy: 0.4498 - val_starts_1_accuracy: 0.4035 - val_stops_1_accuracy: 0.4498\n",
      "Epoch 3/5\n",
      "2803/2803 [==============================] - 180s 64ms/sample - loss: 7.2825 - starts_0_loss: 1.6919 - stops_0_loss: 1.7868 - starts_1_loss: 1.8234 - stops_1_loss: 1.9843 - starts_0_accuracy: 0.4766 - stops_0_accuracy: 0.4938 - starts_1_accuracy: 0.4509 - stops_1_accuracy: 0.4823 - val_loss: 7.7666 - val_starts_0_loss: 1.8328 - val_stops_0_loss: 1.8479 - val_starts_1_loss: 1.9693 - val_stops_1_loss: 2.1167 - val_starts_0_accuracy: 0.4225 - val_stops_0_accuracy: 0.4506 - val_starts_1_accuracy: 0.4053 - val_stops_1_accuracy: 0.4501\n",
      "Epoch 4/5\n",
      "2803/2803 [==============================] - 141s 50ms/sample - loss: 7.3220 - starts_0_loss: 1.6886 - stops_0_loss: 1.7974 - starts_1_loss: 1.8378 - stops_1_loss: 1.9972 - starts_0_accuracy: 0.4734 - stops_0_accuracy: 0.4777 - starts_1_accuracy: 0.4406 - stops_1_accuracy: 0.4649 - val_loss: 7.7507 - val_starts_0_loss: 1.8277 - val_stops_0_loss: 1.8409 - val_starts_1_loss: 1.9674 - val_stops_1_loss: 2.1148 - val_starts_0_accuracy: 0.4221 - val_stops_0_accuracy: 0.4523 - val_starts_1_accuracy: 0.4047 - val_stops_1_accuracy: 0.4508\n",
      "Epoch 5/5\n",
      "2803/2803 [==============================] - 144s 51ms/sample - loss: 7.1644 - starts_0_loss: 1.6606 - stops_0_loss: 1.7413 - starts_1_loss: 1.7942 - stops_1_loss: 1.9575 - starts_0_accuracy: 0.4699 - stops_0_accuracy: 0.4916 - starts_1_accuracy: 0.4592 - stops_1_accuracy: 0.4770 - val_loss: 7.7378 - val_starts_0_loss: 1.8250 - val_stops_0_loss: 1.8364 - val_starts_1_loss: 1.9647 - val_stops_1_loss: 2.1118 - val_starts_0_accuracy: 0.4212 - val_stops_0_accuracy: 0.4525 - val_starts_1_accuracy: 0.4061 - val_stops_1_accuracy: 0.4511\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 2803 samples, validate on 11212 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2803/2803 [==============================] - 195s 70ms/sample - loss: 7.1107 - starts_0_loss: 1.6602 - stops_0_loss: 1.7307 - starts_1_loss: 1.7742 - stops_1_loss: 1.9302 - starts_0_accuracy: 0.4877 - stops_0_accuracy: 0.4898 - starts_1_accuracy: 0.4752 - stops_1_accuracy: 0.4784 - val_loss: 7.6679 - val_starts_0_loss: 1.8088 - val_stops_0_loss: 1.8181 - val_starts_1_loss: 1.9487 - val_stops_1_loss: 2.0923 - val_starts_0_accuracy: 0.4281 - val_stops_0_accuracy: 0.4587 - val_starts_1_accuracy: 0.4098 - val_stops_1_accuracy: 0.4541\n",
      "Epoch 2/2\n",
      "2803/2803 [==============================] - 173s 62ms/sample - loss: 7.0576 - starts_0_loss: 1.6405 - stops_0_loss: 1.7086 - starts_1_loss: 1.7859 - stops_1_loss: 1.9281 - starts_0_accuracy: 0.4877 - stops_0_accuracy: 0.5077 - starts_1_accuracy: 0.4620 - stops_1_accuracy: 0.4902 - val_loss: 7.5986 - val_starts_0_loss: 1.7841 - val_stops_0_loss: 1.8064 - val_starts_1_loss: 1.9303 - val_stops_1_loss: 2.0779 - val_starts_0_accuracy: 0.4333 - val_stops_0_accuracy: 0.4618 - val_starts_1_accuracy: 0.4124 - val_stops_1_accuracy: 0.4583\n",
      "[INFO]  =============== Validation for FOLD# 1 ===============\n",
      "[INFO] 60.36 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 61.61 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 43.33 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 46.18 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 67.75 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 69.49 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 20.93 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 36.29 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 72.24 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 66.78 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 23.98 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 35.49 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 65.62 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 73.56 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 19.88 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 39.02 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_train_stops.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (2803, 76) (2803, 76)\n",
      "[INFO] Prediction shape for validation data:  (11212, 76) (11212, 76)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  2355 out of 2803\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  9665 out of 11212\n",
      "[INFO] Training Jaccard Score:  0.5513565178732759\n",
      "[INFO] Validation Jaccard Score:  0.509665691301428\n",
      "[INFO] Training for fold: 1 finished at Mon May 18 17:59:50 2020\n",
      "[INFO] ==================== FOLD# 2 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 2803 samples, validate on 11212 samples\n",
      "Epoch 1/10\n",
      "2803/2803 [==============================] - 160s 57ms/sample - loss: 13.1139 - starts_0_loss: 2.8929 - stops_0_loss: 3.2805 - starts_1_loss: 3.2934 - stops_1_loss: 3.6480 - starts_0_accuracy: 0.3247 - stops_0_accuracy: 0.1520 - starts_1_accuracy: 0.3443 - stops_1_accuracy: 0.0706 - val_loss: 10.9519 - val_starts_0_loss: 2.4624 - val_stops_0_loss: 2.4449 - val_starts_1_loss: 2.7492 - val_stops_1_loss: 3.2954 - val_starts_0_accuracy: 0.3519 - val_stops_0_accuracy: 0.3270 - val_starts_1_accuracy: 0.3497 - val_stops_1_accuracy: 0.0769\n",
      "Epoch 2/10\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 9.8896 - starts_0_loss: 2.1361 - stops_0_loss: 2.1984 - starts_1_loss: 2.5120 - stops_1_loss: 3.0408 - starts_0_accuracy: 0.3846 - stops_0_accuracy: 0.3618 - starts_1_accuracy: 0.3660 - stops_1_accuracy: 0.2233 - val_loss: 9.1672 - val_starts_0_loss: 1.9654 - val_stops_0_loss: 1.9225 - val_starts_1_loss: 2.4377 - val_stops_1_loss: 2.8420 - val_starts_0_accuracy: 0.3940 - val_stops_0_accuracy: 0.4413 - val_starts_1_accuracy: 0.3497 - val_stops_1_accuracy: 0.3404\n",
      "Epoch 3/10\n",
      "2803/2803 [==============================] - 138s 49ms/sample - loss: 8.6683 - starts_0_loss: 1.8704 - stops_0_loss: 1.9376 - starts_1_loss: 2.2352 - stops_1_loss: 2.6154 - starts_0_accuracy: 0.4317 - stops_0_accuracy: 0.4306 - starts_1_accuracy: 0.3771 - stops_1_accuracy: 0.3832 - val_loss: 8.5454 - val_starts_0_loss: 1.9362 - val_stops_0_loss: 1.9047 - val_starts_1_loss: 2.2356 - val_stops_1_loss: 2.4697 - val_starts_0_accuracy: 0.4092 - val_stops_0_accuracy: 0.4424 - val_starts_1_accuracy: 0.3593 - val_stops_1_accuracy: 0.4041\n",
      "Epoch 4/10\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 7.9927 - starts_0_loss: 1.7760 - stops_0_loss: 1.8274 - starts_1_loss: 2.0471 - stops_1_loss: 2.3345 - starts_0_accuracy: 0.4545 - stops_0_accuracy: 0.4702 - starts_1_accuracy: 0.4096 - stops_1_accuracy: 0.4370 - val_loss: 8.4328 - val_starts_0_loss: 1.9797 - val_stops_0_loss: 1.9502 - val_starts_1_loss: 2.1426 - val_stops_1_loss: 2.3607 - val_starts_0_accuracy: 0.4097 - val_stops_0_accuracy: 0.4352 - val_starts_1_accuracy: 0.3776 - val_stops_1_accuracy: 0.4014\n",
      "Epoch 5/10\n",
      "2803/2803 [==============================] - 142s 51ms/sample - loss: 7.5823 - starts_0_loss: 1.7147 - stops_0_loss: 1.7634 - starts_1_loss: 1.9429 - stops_1_loss: 2.1735 - starts_0_accuracy: 0.4713 - stops_0_accuracy: 0.4920 - starts_1_accuracy: 0.4335 - stops_1_accuracy: 0.4549 - val_loss: 8.1597 - val_starts_0_loss: 1.8738 - val_stops_0_loss: 2.0224 - val_starts_1_loss: 2.0363 - val_stops_1_loss: 2.2279 - val_starts_0_accuracy: 0.4242 - val_stops_0_accuracy: 0.4358 - val_starts_1_accuracy: 0.3940 - val_stops_1_accuracy: 0.4261\n",
      "Epoch 6/10\n",
      "2803/2803 [==============================] - 139s 50ms/sample - loss: 7.2569 - starts_0_loss: 1.6822 - stops_0_loss: 1.6982 - starts_1_loss: 1.8461 - stops_1_loss: 2.0358 - starts_0_accuracy: 0.4781 - stops_0_accuracy: 0.5130 - starts_1_accuracy: 0.4427 - stops_1_accuracy: 0.4884 - val_loss: 8.0272 - val_starts_0_loss: 1.8978 - val_stops_0_loss: 1.9095 - val_starts_1_loss: 2.0137 - val_stops_1_loss: 2.2069 - val_starts_0_accuracy: 0.4196 - val_stops_0_accuracy: 0.4459 - val_starts_1_accuracy: 0.4014 - val_stops_1_accuracy: 0.4289\n",
      "Epoch 7/10\n",
      "2803/2803 [==============================] - 139s 50ms/sample - loss: 6.9442 - starts_0_loss: 1.5899 - stops_0_loss: 1.6342 - starts_1_loss: 1.7494 - stops_1_loss: 1.9554 - starts_0_accuracy: 0.4980 - stops_0_accuracy: 0.5184 - starts_1_accuracy: 0.4670 - stops_1_accuracy: 0.5009 - val_loss: 8.0392 - val_starts_0_loss: 1.9055 - val_stops_0_loss: 1.9163 - val_starts_1_loss: 2.0247 - val_stops_1_loss: 2.1933 - val_starts_0_accuracy: 0.4207 - val_stops_0_accuracy: 0.4436 - val_starts_1_accuracy: 0.4048 - val_stops_1_accuracy: 0.4356\n",
      "Epoch 8/10\n",
      "2803/2803 [==============================] - 139s 49ms/sample - loss: 6.7109 - starts_0_loss: 1.5684 - stops_0_loss: 1.5942 - starts_1_loss: 1.6893 - stops_1_loss: 1.8659 - starts_0_accuracy: 0.4934 - stops_0_accuracy: 0.5230 - starts_1_accuracy: 0.4859 - stops_1_accuracy: 0.5130 - val_loss: 8.0148 - val_starts_0_loss: 1.8879 - val_stops_0_loss: 2.0328 - val_starts_1_loss: 1.9294 - val_stops_1_loss: 2.1653 - val_starts_0_accuracy: 0.4286 - val_stops_0_accuracy: 0.4383 - val_starts_1_accuracy: 0.4256 - val_stops_1_accuracy: 0.4326\n",
      "Epoch 9/10\n",
      "2803/2803 [==============================] - 139s 50ms/sample - loss: 6.5894 - starts_0_loss: 1.5364 - stops_0_loss: 1.5597 - starts_1_loss: 1.6456 - stops_1_loss: 1.8287 - starts_0_accuracy: 0.5120 - stops_0_accuracy: 0.5259 - starts_1_accuracy: 0.4923 - stops_1_accuracy: 0.5177 - val_loss: 7.9698 - val_starts_0_loss: 1.8825 - val_stops_0_loss: 1.9719 - val_starts_1_loss: 1.9689 - val_stops_1_loss: 2.1472 - val_starts_0_accuracy: 0.4371 - val_stops_0_accuracy: 0.4447 - val_starts_1_accuracy: 0.4253 - val_stops_1_accuracy: 0.4459\n",
      "Epoch 10/10\n",
      "2803/2803 [==============================] - 141s 50ms/sample - loss: 6.4315 - starts_0_loss: 1.5255 - stops_0_loss: 1.5379 - starts_1_loss: 1.6151 - stops_1_loss: 1.7561 - starts_0_accuracy: 0.5273 - stops_0_accuracy: 0.5469 - starts_1_accuracy: 0.5216 - stops_1_accuracy: 0.5491 - val_loss: 7.8652 - val_starts_0_loss: 1.8485 - val_stops_0_loss: 1.9216 - val_starts_1_loss: 1.9433 - val_stops_1_loss: 2.1526 - val_starts_0_accuracy: 0.4322 - val_stops_0_accuracy: 0.4483 - val_starts_1_accuracy: 0.4233 - val_stops_1_accuracy: 0.4427\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 2803 samples, validate on 11212 samples\n",
      "Epoch 1/5\n",
      "2803/2803 [==============================] - 159s 57ms/sample - loss: 6.0006 - starts_0_loss: 1.4131 - stops_0_loss: 1.4138 - starts_1_loss: 1.5116 - stops_1_loss: 1.6582 - starts_0_accuracy: 0.5426 - stops_0_accuracy: 0.5776 - starts_1_accuracy: 0.5366 - stops_1_accuracy: 0.5751 - val_loss: 7.7638 - val_starts_0_loss: 1.8160 - val_stops_0_loss: 1.9026 - val_starts_1_loss: 1.9155 - val_stops_1_loss: 2.1306 - val_starts_0_accuracy: 0.4389 - val_stops_0_accuracy: 0.4542 - val_starts_1_accuracy: 0.4313 - val_stops_1_accuracy: 0.4489\n",
      "Epoch 2/5\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 5.9607 - starts_0_loss: 1.4019 - stops_0_loss: 1.3852 - starts_1_loss: 1.5242 - stops_1_loss: 1.6397 - starts_0_accuracy: 0.5433 - stops_0_accuracy: 0.5854 - starts_1_accuracy: 0.5287 - stops_1_accuracy: 0.5812 - val_loss: 7.7503 - val_starts_0_loss: 1.8119 - val_stops_0_loss: 1.8995 - val_starts_1_loss: 1.9150 - val_stops_1_loss: 2.1246 - val_starts_0_accuracy: 0.4395 - val_stops_0_accuracy: 0.4544 - val_starts_1_accuracy: 0.4318 - val_stops_1_accuracy: 0.4482\n",
      "Epoch 3/5\n",
      "2803/2803 [==============================] - 139s 49ms/sample - loss: 5.9185 - starts_0_loss: 1.3862 - stops_0_loss: 1.4007 - starts_1_loss: 1.5048 - stops_1_loss: 1.6610 - starts_0_accuracy: 0.5658 - stops_0_accuracy: 0.5858 - starts_1_accuracy: 0.5483 - stops_1_accuracy: 0.5708 - val_loss: 7.7383 - val_starts_0_loss: 1.8098 - val_stops_0_loss: 1.8934 - val_starts_1_loss: 1.9149 - val_stops_1_loss: 2.1210 - val_starts_0_accuracy: 0.4387 - val_stops_0_accuracy: 0.4567 - val_starts_1_accuracy: 0.4316 - val_stops_1_accuracy: 0.4493\n",
      "Epoch 4/5\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 5.8808 - starts_0_loss: 1.3817 - stops_0_loss: 1.3747 - starts_1_loss: 1.4894 - stops_1_loss: 1.6237 - starts_0_accuracy: 0.5430 - stops_0_accuracy: 0.5894 - starts_1_accuracy: 0.5387 - stops_1_accuracy: 0.5862 - val_loss: 7.7300 - val_starts_0_loss: 1.8068 - val_stops_0_loss: 1.8921 - val_starts_1_loss: 1.9133 - val_stops_1_loss: 2.1185 - val_starts_0_accuracy: 0.4387 - val_stops_0_accuracy: 0.4560 - val_starts_1_accuracy: 0.4319 - val_stops_1_accuracy: 0.4504\n",
      "Epoch 5/5\n",
      "2803/2803 [==============================] - 139s 50ms/sample - loss: 5.8141 - starts_0_loss: 1.3663 - stops_0_loss: 1.3683 - starts_1_loss: 1.4693 - stops_1_loss: 1.6151 - starts_0_accuracy: 0.5505 - stops_0_accuracy: 0.5908 - starts_1_accuracy: 0.5498 - stops_1_accuracy: 0.5901 - val_loss: 7.7370 - val_starts_0_loss: 1.8100 - val_stops_0_loss: 1.8929 - val_starts_1_loss: 1.9170 - val_stops_1_loss: 2.1179 - val_starts_0_accuracy: 0.4393 - val_stops_0_accuracy: 0.4580 - val_starts_1_accuracy: 0.4305 - val_stops_1_accuracy: 0.4514\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 2803 samples, validate on 11212 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2803/2803 [==============================] - 194s 69ms/sample - loss: 5.8434 - starts_0_loss: 1.3706 - stops_0_loss: 1.3711 - starts_1_loss: 1.4869 - stops_1_loss: 1.6089 - starts_0_accuracy: 0.5637 - stops_0_accuracy: 0.5822 - starts_1_accuracy: 0.5473 - stops_1_accuracy: 0.5794 - val_loss: 7.6959 - val_starts_0_loss: 1.8019 - val_stops_0_loss: 1.8964 - val_starts_1_loss: 1.8985 - val_stops_1_loss: 2.0999 - val_starts_0_accuracy: 0.4442 - val_stops_0_accuracy: 0.4555 - val_starts_1_accuracy: 0.4364 - val_stops_1_accuracy: 0.4567\n",
      "Epoch 2/2\n",
      "2803/2803 [==============================] - 172s 61ms/sample - loss: 5.7109 - starts_0_loss: 1.3558 - stops_0_loss: 1.3141 - starts_1_loss: 1.4864 - stops_1_loss: 1.5739 - starts_0_accuracy: 0.5615 - stops_0_accuracy: 0.5915 - starts_1_accuracy: 0.5462 - stops_1_accuracy: 0.5851 - val_loss: 7.6780 - val_starts_0_loss: 1.7965 - val_stops_0_loss: 1.8917 - val_starts_1_loss: 1.8946 - val_stops_1_loss: 2.0959 - val_starts_0_accuracy: 0.4465 - val_stops_0_accuracy: 0.4575 - val_starts_1_accuracy: 0.4339 - val_stops_1_accuracy: 0.4572\n",
      "[INFO]  =============== Validation for FOLD# 2 ===============\n",
      "[INFO] 68.39 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 71.60 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 44.65 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 45.75 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 79.61 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 79.92 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 23.07 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 35.87 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 79.63 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 78.35 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 26.63 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 35.52 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 80.41 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 82.14 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 22.58 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 37.50 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_train_stops.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (2803, 76) (2803, 76)\n",
      "[INFO] Prediction shape for validation data:  (11212, 76) (11212, 76)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  2205 out of 2803\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  9318 out of 11212\n",
      "[INFO] Training Jaccard Score:  0.5858062880508488\n",
      "[INFO] Validation Jaccard Score:  0.5142671219770434\n",
      "[INFO] Training for fold: 2 finished at Mon May 18 18:43:08 2020\n",
      "[INFO] ==================== FOLD# 3 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 2803 samples, validate on 11212 samples\n",
      "Epoch 1/10\n",
      "2803/2803 [==============================] - 157s 56ms/sample - loss: 13.1167 - starts_0_loss: 2.9385 - stops_0_loss: 3.1726 - starts_1_loss: 3.3312 - stops_1_loss: 3.6656 - starts_0_accuracy: 0.2997 - stops_0_accuracy: 0.1798 - starts_1_accuracy: 0.3343 - stops_1_accuracy: 0.0660 - val_loss: 11.0131 - val_starts_0_loss: 2.4221 - val_stops_0_loss: 2.5173 - val_starts_1_loss: 2.7626 - val_stops_1_loss: 3.3111 - val_starts_0_accuracy: 0.3557 - val_stops_0_accuracy: 0.3286 - val_starts_1_accuracy: 0.3536 - val_stops_1_accuracy: 0.1016\n",
      "Epoch 2/10\n",
      "2803/2803 [==============================] - 139s 50ms/sample - loss: 10.0256 - starts_0_loss: 2.1471 - stops_0_loss: 2.2702 - starts_1_loss: 2.5660 - stops_1_loss: 3.0548 - starts_0_accuracy: 0.3767 - stops_0_accuracy: 0.3618 - starts_1_accuracy: 0.3507 - stops_1_accuracy: 0.2212 - val_loss: 9.2143 - val_starts_0_loss: 2.0343 - val_stops_0_loss: 2.0037 - val_starts_1_loss: 2.3931 - val_stops_1_loss: 2.7832 - val_starts_0_accuracy: 0.3989 - val_stops_0_accuracy: 0.4123 - val_starts_1_accuracy: 0.3537 - val_stops_1_accuracy: 0.3347\n",
      "Epoch 3/10\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 8.8521 - starts_0_loss: 1.9187 - stops_0_loss: 2.0088 - starts_1_loss: 2.2904 - stops_1_loss: 2.6462 - starts_0_accuracy: 0.4206 - stops_0_accuracy: 0.4278 - starts_1_accuracy: 0.3639 - stops_1_accuracy: 0.3721 - val_loss: 8.6453 - val_starts_0_loss: 1.9729 - val_stops_0_loss: 1.9153 - val_starts_1_loss: 2.2486 - val_stops_1_loss: 2.5082 - val_starts_0_accuracy: 0.4041 - val_stops_0_accuracy: 0.4394 - val_starts_1_accuracy: 0.3601 - val_stops_1_accuracy: 0.3932\n",
      "Epoch 4/10\n",
      "2803/2803 [==============================] - 142s 51ms/sample - loss: 8.2438 - starts_0_loss: 1.7887 - stops_0_loss: 1.9297 - starts_1_loss: 2.1083 - stops_1_loss: 2.4083 - starts_0_accuracy: 0.4445 - stops_0_accuracy: 0.4517 - starts_1_accuracy: 0.3964 - stops_1_accuracy: 0.4053 - val_loss: 8.7956 - val_starts_0_loss: 2.0780 - val_stops_0_loss: 1.9412 - val_starts_1_loss: 2.2712 - val_stops_1_loss: 2.5051 - val_starts_0_accuracy: 0.3993 - val_stops_0_accuracy: 0.4394 - val_starts_1_accuracy: 0.3651 - val_stops_1_accuracy: 0.3773\n",
      "Epoch 5/10\n",
      "2803/2803 [==============================] - 143s 51ms/sample - loss: 7.8170 - starts_0_loss: 1.7354 - stops_0_loss: 1.8568 - starts_1_loss: 1.9890 - stops_1_loss: 2.2475 - starts_0_accuracy: 0.4731 - stops_0_accuracy: 0.4649 - starts_1_accuracy: 0.4174 - stops_1_accuracy: 0.4413 - val_loss: 8.1263 - val_starts_0_loss: 1.8654 - val_stops_0_loss: 1.9567 - val_starts_1_loss: 2.0456 - val_stops_1_loss: 2.2584 - val_starts_0_accuracy: 0.4248 - val_stops_0_accuracy: 0.4413 - val_starts_1_accuracy: 0.4006 - val_stops_1_accuracy: 0.4312\n",
      "Epoch 6/10\n",
      "2803/2803 [==============================] - 141s 50ms/sample - loss: 7.4545 - starts_0_loss: 1.6960 - stops_0_loss: 1.7431 - starts_1_loss: 1.9056 - stops_1_loss: 2.1214 - starts_0_accuracy: 0.4670 - stops_0_accuracy: 0.4806 - starts_1_accuracy: 0.4388 - stops_1_accuracy: 0.4574 - val_loss: 8.1624 - val_starts_0_loss: 1.9535 - val_stops_0_loss: 2.0207 - val_starts_1_loss: 2.0027 - val_stops_1_loss: 2.1853 - val_starts_0_accuracy: 0.4147 - val_stops_0_accuracy: 0.4391 - val_starts_1_accuracy: 0.4150 - val_stops_1_accuracy: 0.4398\n",
      "Epoch 7/10\n",
      "2803/2803 [==============================] - 141s 50ms/sample - loss: 7.2036 - starts_0_loss: 1.6520 - stops_0_loss: 1.7140 - starts_1_loss: 1.8397 - stops_1_loss: 2.0122 - starts_0_accuracy: 0.4741 - stops_0_accuracy: 0.5016 - starts_1_accuracy: 0.4549 - stops_1_accuracy: 0.4923 - val_loss: 7.9359 - val_starts_0_loss: 1.8426 - val_stops_0_loss: 1.8984 - val_starts_1_loss: 1.9956 - val_stops_1_loss: 2.1994 - val_starts_0_accuracy: 0.4337 - val_stops_0_accuracy: 0.4577 - val_starts_1_accuracy: 0.4116 - val_stops_1_accuracy: 0.4458\n",
      "Epoch 8/10\n",
      "2803/2803 [==============================] - 141s 50ms/sample - loss: 6.9674 - starts_0_loss: 1.5971 - stops_0_loss: 1.6804 - starts_1_loss: 1.7520 - stops_1_loss: 1.9479 - starts_0_accuracy: 0.5059 - stops_0_accuracy: 0.5059 - starts_1_accuracy: 0.4856 - stops_1_accuracy: 0.4977 - val_loss: 8.1684 - val_starts_0_loss: 1.9486 - val_stops_0_loss: 1.9662 - val_starts_1_loss: 2.0283 - val_stops_1_loss: 2.2249 - val_starts_0_accuracy: 0.4245 - val_stops_0_accuracy: 0.4480 - val_starts_1_accuracy: 0.4092 - val_stops_1_accuracy: 0.4365\n",
      "Epoch 9/10\n",
      "2803/2803 [==============================] - 141s 50ms/sample - loss: 6.7646 - starts_0_loss: 1.5493 - stops_0_loss: 1.6311 - starts_1_loss: 1.6851 - stops_1_loss: 1.8830 - starts_0_accuracy: 0.5109 - stops_0_accuracy: 0.5123 - starts_1_accuracy: 0.4959 - stops_1_accuracy: 0.5077 - val_loss: 8.0440 - val_starts_0_loss: 1.9064 - val_stops_0_loss: 2.0646 - val_starts_1_loss: 1.9341 - val_stops_1_loss: 2.1390 - val_starts_0_accuracy: 0.4262 - val_stops_0_accuracy: 0.4443 - val_starts_1_accuracy: 0.4320 - val_stops_1_accuracy: 0.4460\n",
      "Epoch 10/10\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 6.6514 - starts_0_loss: 1.5495 - stops_0_loss: 1.6176 - starts_1_loss: 1.6729 - stops_1_loss: 1.8341 - starts_0_accuracy: 0.5059 - stops_0_accuracy: 0.5252 - starts_1_accuracy: 0.4980 - stops_1_accuracy: 0.5248 - val_loss: 7.9128 - val_starts_0_loss: 1.8833 - val_stops_0_loss: 1.9458 - val_starts_1_loss: 1.9467 - val_stops_1_loss: 2.1368 - val_starts_0_accuracy: 0.4299 - val_stops_0_accuracy: 0.4481 - val_starts_1_accuracy: 0.4262 - val_stops_1_accuracy: 0.4503\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 2803 samples, validate on 11212 samples\n",
      "Epoch 1/5\n",
      "2803/2803 [==============================] - 159s 57ms/sample - loss: 6.1724 - starts_0_loss: 1.4335 - stops_0_loss: 1.4462 - starts_1_loss: 1.5763 - stops_1_loss: 1.7058 - starts_0_accuracy: 0.5391 - stops_0_accuracy: 0.5694 - starts_1_accuracy: 0.5252 - stops_1_accuracy: 0.5587 - val_loss: 7.8299 - val_starts_0_loss: 1.8411 - val_stops_0_loss: 1.9128 - val_starts_1_loss: 1.9460 - val_stops_1_loss: 2.1298 - val_starts_0_accuracy: 0.4394 - val_stops_0_accuracy: 0.4490 - val_starts_1_accuracy: 0.4291 - val_stops_1_accuracy: 0.4520\n",
      "Epoch 2/5\n",
      "2803/2803 [==============================] - 141s 50ms/sample - loss: 6.0563 - starts_0_loss: 1.4316 - stops_0_loss: 1.4128 - starts_1_loss: 1.5579 - stops_1_loss: 1.6695 - starts_0_accuracy: 0.5487 - stops_0_accuracy: 0.5697 - starts_1_accuracy: 0.5323 - stops_1_accuracy: 0.5730 - val_loss: 7.8135 - val_starts_0_loss: 1.8340 - val_stops_0_loss: 1.9153 - val_starts_1_loss: 1.9404 - val_stops_1_loss: 2.1235 - val_starts_0_accuracy: 0.4400 - val_stops_0_accuracy: 0.4496 - val_starts_1_accuracy: 0.4302 - val_stops_1_accuracy: 0.4545\n",
      "Epoch 3/5\n",
      "2803/2803 [==============================] - 144s 51ms/sample - loss: 6.0067 - starts_0_loss: 1.4023 - stops_0_loss: 1.4077 - starts_1_loss: 1.5436 - stops_1_loss: 1.6589 - starts_0_accuracy: 0.5530 - stops_0_accuracy: 0.5726 - starts_1_accuracy: 0.5369 - stops_1_accuracy: 0.5719 - val_loss: 7.8066 - val_starts_0_loss: 1.8329 - val_stops_0_loss: 1.9151 - val_starts_1_loss: 1.9378 - val_stops_1_loss: 2.1205 - val_starts_0_accuracy: 0.4410 - val_stops_0_accuracy: 0.4514 - val_starts_1_accuracy: 0.4300 - val_stops_1_accuracy: 0.4559\n",
      "Epoch 4/5\n",
      "2803/2803 [==============================] - 142s 51ms/sample - loss: 6.0116 - starts_0_loss: 1.4089 - stops_0_loss: 1.3988 - starts_1_loss: 1.5500 - stops_1_loss: 1.6535 - starts_0_accuracy: 0.5444 - stops_0_accuracy: 0.5765 - starts_1_accuracy: 0.5298 - stops_1_accuracy: 0.5744 - val_loss: 7.7979 - val_starts_0_loss: 1.8283 - val_stops_0_loss: 1.9229 - val_starts_1_loss: 1.9312 - val_stops_1_loss: 2.1153 - val_starts_0_accuracy: 0.4417 - val_stops_0_accuracy: 0.4493 - val_starts_1_accuracy: 0.4328 - val_stops_1_accuracy: 0.4543\n",
      "Epoch 5/5\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 5.9743 - starts_0_loss: 1.3688 - stops_0_loss: 1.4153 - starts_1_loss: 1.5306 - stops_1_loss: 1.6463 - starts_0_accuracy: 0.5530 - stops_0_accuracy: 0.5776 - starts_1_accuracy: 0.5398 - stops_1_accuracy: 0.5712 - val_loss: 7.7853 - val_starts_0_loss: 1.8260 - val_stops_0_loss: 1.9167 - val_starts_1_loss: 1.9270 - val_stops_1_loss: 2.1154 - val_starts_0_accuracy: 0.4427 - val_stops_0_accuracy: 0.4515 - val_starts_1_accuracy: 0.4329 - val_stops_1_accuracy: 0.4559\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 2803 samples, validate on 11212 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2803/2803 [==============================] - 199s 71ms/sample - loss: 5.9511 - starts_0_loss: 1.4014 - stops_0_loss: 1.3826 - starts_1_loss: 1.5329 - stops_1_loss: 1.6474 - starts_0_accuracy: 0.5573 - stops_0_accuracy: 0.5876 - starts_1_accuracy: 0.5430 - stops_1_accuracy: 0.5833 - val_loss: 7.7046 - val_starts_0_loss: 1.7995 - val_stops_0_loss: 1.8854 - val_starts_1_loss: 1.9106 - val_stops_1_loss: 2.1087 - val_starts_0_accuracy: 0.4478 - val_stops_0_accuracy: 0.4587 - val_starts_1_accuracy: 0.4359 - val_stops_1_accuracy: 0.4596\n",
      "Epoch 2/2\n",
      "2803/2803 [==============================] - 172s 61ms/sample - loss: 5.9471 - starts_0_loss: 1.3889 - stops_0_loss: 1.3840 - starts_1_loss: 1.5148 - stops_1_loss: 1.6466 - starts_0_accuracy: 0.5519 - stops_0_accuracy: 0.5769 - starts_1_accuracy: 0.5355 - stops_1_accuracy: 0.5772 - val_loss: 7.7016 - val_starts_0_loss: 1.8042 - val_stops_0_loss: 1.9044 - val_starts_1_loss: 1.8946 - val_stops_1_loss: 2.0981 - val_starts_0_accuracy: 0.4498 - val_stops_0_accuracy: 0.4583 - val_starts_1_accuracy: 0.4390 - val_stops_1_accuracy: 0.4599\n",
      "[INFO]  =============== Validation for FOLD# 3 ===============\n",
      "[INFO] 68.89 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 70.57 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 44.98 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 45.83 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 78.99 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 79.22 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 23.10 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 36.39 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 81.88 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 77.30 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 26.73 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 36.03 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 78.13 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 81.85 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 21.99 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 38.02 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_train_starts.csv\n",
      "[INFO] \t||"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (2803, 76) (2803, 76)\n",
      "[INFO] Prediction shape for validation data:  (11212, 76) (11212, 76)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  2232 out of 2803\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  9372 out of 11212\n",
      "[INFO] Training Jaccard Score:  0.584879385716618\n",
      "[INFO] Validation Jaccard Score:  0.512965961949882\n",
      "[INFO] Training for fold: 3 finished at Mon May 18 19:26:46 2020\n",
      "[INFO] ==================== FOLD# 4 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 2803 samples, validate on 11212 samples\n",
      "Epoch 1/10\n",
      "2803/2803 [==============================] - 159s 57ms/sample - loss: 13.2181 - starts_0_loss: 3.0118 - stops_0_loss: 3.2031 - starts_1_loss: 3.3324 - stops_1_loss: 3.6501 - starts_0_accuracy: 0.2893 - stops_0_accuracy: 0.1773 - starts_1_accuracy: 0.3261 - stops_1_accuracy: 0.0746 - val_loss: 10.9239 - val_starts_0_loss: 2.3970 - val_stops_0_loss: 2.4573 - val_starts_1_loss: 2.7624 - val_stops_1_loss: 3.3075 - val_starts_0_accuracy: 0.3531 - val_stops_0_accuracy: 0.3250 - val_starts_1_accuracy: 0.3532 - val_stops_1_accuracy: 0.1145\n",
      "Epoch 2/10\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 10.0100 - starts_0_loss: 2.1950 - stops_0_loss: 2.2117 - starts_1_loss: 2.5730 - stops_1_loss: 3.0341 - starts_0_accuracy: 0.3685 - stops_0_accuracy: 0.3607 - starts_1_accuracy: 0.3521 - stops_1_accuracy: 0.2340 - val_loss: 9.1125 - val_starts_0_loss: 1.9763 - val_stops_0_loss: 1.9955 - val_starts_1_loss: 2.4144 - val_stops_1_loss: 2.7264 - val_starts_0_accuracy: 0.3875 - val_stops_0_accuracy: 0.4221 - val_starts_1_accuracy: 0.3534 - val_stops_1_accuracy: 0.3584\n",
      "Epoch 3/10\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 8.8455 - starts_0_loss: 1.9566 - stops_0_loss: 1.9671 - starts_1_loss: 2.3138 - stops_1_loss: 2.6171 - starts_0_accuracy: 0.4146 - stops_0_accuracy: 0.4413 - starts_1_accuracy: 0.3553 - stops_1_accuracy: 0.3839 - val_loss: 8.4319 - val_starts_0_loss: 1.8838 - val_stops_0_loss: 1.9308 - val_starts_1_loss: 2.2118 - val_stops_1_loss: 2.4055 - val_starts_0_accuracy: 0.4127 - val_stops_0_accuracy: 0.4451 - val_starts_1_accuracy: 0.3633 - val_stops_1_accuracy: 0.4212\n",
      "Epoch 4/10\n",
      "2803/2803 [==============================] - 139s 50ms/sample - loss: 8.1121 - starts_0_loss: 1.8327 - stops_0_loss: 1.8002 - starts_1_loss: 2.1311 - stops_1_loss: 2.3269 - starts_0_accuracy: 0.4388 - stops_0_accuracy: 0.4806 - starts_1_accuracy: 0.3785 - stops_1_accuracy: 0.4406 - val_loss: 8.1464 - val_starts_0_loss: 1.8677 - val_stops_0_loss: 1.9118 - val_starts_1_loss: 2.1052 - val_stops_1_loss: 2.2615 - val_starts_0_accuracy: 0.4154 - val_stops_0_accuracy: 0.4458 - val_starts_1_accuracy: 0.3855 - val_stops_1_accuracy: 0.4359\n",
      "Epoch 5/10\n",
      "2803/2803 [==============================] - 139s 49ms/sample - loss: 7.6521 - starts_0_loss: 1.7386 - stops_0_loss: 1.7543 - starts_1_loss: 2.0032 - stops_1_loss: 2.1470 - starts_0_accuracy: 0.4748 - stops_0_accuracy: 0.4905 - starts_1_accuracy: 0.4196 - stops_1_accuracy: 0.4706 - val_loss: 8.0492 - val_starts_0_loss: 1.8694 - val_stops_0_loss: 1.8886 - val_starts_1_loss: 2.0586 - val_stops_1_loss: 2.2326 - val_starts_0_accuracy: 0.4264 - val_stops_0_accuracy: 0.4609 - val_starts_1_accuracy: 0.3983 - val_stops_1_accuracy: 0.4396\n",
      "Epoch 6/10\n",
      "2803/2803 [==============================] - 139s 49ms/sample - loss: 7.3529 - starts_0_loss: 1.7207 - stops_0_loss: 1.6689 - starts_1_loss: 1.9340 - stops_1_loss: 2.0414 - starts_0_accuracy: 0.4709 - stops_0_accuracy: 0.5077 - starts_1_accuracy: 0.4249 - stops_1_accuracy: 0.4895 - val_loss: 7.9675 - val_starts_0_loss: 1.8513 - val_stops_0_loss: 1.9266 - val_starts_1_loss: 2.0034 - val_stops_1_loss: 2.1862 - val_starts_0_accuracy: 0.4272 - val_stops_0_accuracy: 0.4544 - val_starts_1_accuracy: 0.4096 - val_stops_1_accuracy: 0.4424\n",
      "Epoch 7/10\n",
      "2803/2803 [==============================] - 139s 50ms/sample - loss: 7.0609 - starts_0_loss: 1.6283 - stops_0_loss: 1.6501 - starts_1_loss: 1.8150 - stops_1_loss: 1.9663 - starts_0_accuracy: 0.4927 - stops_0_accuracy: 0.5087 - starts_1_accuracy: 0.4702 - stops_1_accuracy: 0.5030 - val_loss: 7.9214 - val_starts_0_loss: 1.8443 - val_stops_0_loss: 1.9508 - val_starts_1_loss: 1.9863 - val_stops_1_loss: 2.1404 - val_starts_0_accuracy: 0.4319 - val_stops_0_accuracy: 0.4467 - val_starts_1_accuracy: 0.4138 - val_stops_1_accuracy: 0.4477\n",
      "Epoch 8/10\n",
      "2803/2803 [==============================] - 139s 50ms/sample - loss: 6.9967 - starts_0_loss: 1.6208 - stops_0_loss: 1.6595 - starts_1_loss: 1.7849 - stops_1_loss: 1.9350 - starts_0_accuracy: 0.4930 - stops_0_accuracy: 0.5159 - starts_1_accuracy: 0.4677 - stops_1_accuracy: 0.5030 - val_loss: 7.8045 - val_starts_0_loss: 1.8094 - val_stops_0_loss: 1.8906 - val_starts_1_loss: 1.9451 - val_stops_1_loss: 2.1594 - val_starts_0_accuracy: 0.4427 - val_stops_0_accuracy: 0.4568 - val_starts_1_accuracy: 0.4261 - val_stops_1_accuracy: 0.4420\n",
      "Epoch 9/10\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 6.7183 - starts_0_loss: 1.5863 - stops_0_loss: 1.5759 - starts_1_loss: 1.7187 - stops_1_loss: 1.8403 - starts_0_accuracy: 0.5030 - stops_0_accuracy: 0.5366 - starts_1_accuracy: 0.4831 - stops_1_accuracy: 0.5237 - val_loss: 7.8906 - val_starts_0_loss: 1.8659 - val_stops_0_loss: 1.9452 - val_starts_1_loss: 1.9517 - val_stops_1_loss: 2.1280 - val_starts_0_accuracy: 0.4262 - val_stops_0_accuracy: 0.4479 - val_starts_1_accuracy: 0.4148 - val_stops_1_accuracy: 0.4474\n",
      "Epoch 10/10\n",
      "2803/2803 [==============================] - 140s 50ms/sample - loss: 6.4199 - starts_0_loss: 1.5054 - stops_0_loss: 1.5293 - starts_1_loss: 1.6447 - stops_1_loss: 1.7557 - starts_0_accuracy: 0.5244 - stops_0_accuracy: 0.5376 - starts_1_accuracy: 0.5105 - stops_1_accuracy: 0.5309 - val_loss: 7.8304 - val_starts_0_loss: 1.8582 - val_stops_0_loss: 1.8975 - val_starts_1_loss: 1.9460 - val_stops_1_loss: 2.1287 - val_starts_0_accuracy: 0.4397 - val_stops_0_accuracy: 0.4576 - val_starts_1_accuracy: 0.4268 - val_stops_1_accuracy: 0.4531\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 2803 samples, validate on 11212 samples\n",
      "Epoch 1/5\n",
      "2803/2803 [==============================] - 161s 57ms/sample - loss: 6.1119 - starts_0_loss: 1.4343 - stops_0_loss: 1.4478 - starts_1_loss: 1.5662 - stops_1_loss: 1.6687 - starts_0_accuracy: 0.5462 - stops_0_accuracy: 0.5701 - starts_1_accuracy: 0.5269 - stops_1_accuracy: 0.5680 - val_loss: 7.8189 - val_starts_0_loss: 1.8587 - val_stops_0_loss: 1.9242 - val_starts_1_loss: 1.9245 - val_stops_1_loss: 2.1115 - val_starts_0_accuracy: 0.4399 - val_stops_0_accuracy: 0.4574 - val_starts_1_accuracy: 0.4333 - val_stops_1_accuracy: 0.4566\n",
      "Epoch 2/5\n",
      "2803/2803 [==============================] - 141s 50ms/sample - loss: 6.0017 - starts_0_loss: 1.4072 - stops_0_loss: 1.4174 - starts_1_loss: 1.5318 - stops_1_loss: 1.6328 - starts_0_accuracy: 0.5569 - stops_0_accuracy: 0.5808 - starts_1_accuracy: 0.5419 - stops_1_accuracy: 0.5808 - val_loss: 7.7988 - val_starts_0_loss: 1.8526 - val_stops_0_loss: 1.9150 - val_starts_1_loss: 1.9248 - val_stops_1_loss: 2.1065 - val_starts_0_accuracy: 0.4396 - val_stops_0_accuracy: 0.4585 - val_starts_1_accuracy: 0.4317 - val_stops_1_accuracy: 0.4560\n",
      "Epoch 3/5\n",
      "2803/2803 [==============================] - 145s 52ms/sample - loss: 6.0655 - starts_0_loss: 1.4383 - stops_0_loss: 1.4236 - starts_1_loss: 1.5543 - stops_1_loss: 1.6418 - starts_0_accuracy: 0.5391 - stops_0_accuracy: 0.5726 - starts_1_accuracy: 0.5401 - stops_1_accuracy: 0.5722 - val_loss: 7.7847 - val_starts_0_loss: 1.8489 - val_stops_0_loss: 1.9125 - val_starts_1_loss: 1.9201 - val_stops_1_loss: 2.1032 - val_starts_0_accuracy: 0.4388 - val_stops_0_accuracy: 0.4589 - val_starts_1_accuracy: 0.4326 - val_stops_1_accuracy: 0.4557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "2803/2803 [==============================] - 142s 51ms/sample - loss: 6.0871 - starts_0_loss: 1.4514 - stops_0_loss: 1.4196 - starts_1_loss: 1.5681 - stops_1_loss: 1.6327 - starts_0_accuracy: 0.5273 - stops_0_accuracy: 0.5758 - starts_1_accuracy: 0.5241 - stops_1_accuracy: 0.5769 - val_loss: 7.7797 - val_starts_0_loss: 1.8475 - val_stops_0_loss: 1.9068 - val_starts_1_loss: 1.9223 - val_stops_1_loss: 2.1031 - val_starts_0_accuracy: 0.4401 - val_stops_0_accuracy: 0.4600 - val_starts_1_accuracy: 0.4310 - val_stops_1_accuracy: 0.4561\n",
      "Epoch 5/5\n",
      "2803/2803 [==============================] - 143s 51ms/sample - loss: 5.9546 - starts_0_loss: 1.4184 - stops_0_loss: 1.3815 - starts_1_loss: 1.5284 - stops_1_loss: 1.6120 - starts_0_accuracy: 0.5512 - stops_0_accuracy: 0.5790 - starts_1_accuracy: 0.5444 - stops_1_accuracy: 0.5726 - val_loss: 7.7638 - val_starts_0_loss: 1.8433 - val_stops_0_loss: 1.8991 - val_starts_1_loss: 1.9214 - val_stops_1_loss: 2.1000 - val_starts_0_accuracy: 0.4398 - val_stops_0_accuracy: 0.4597 - val_starts_1_accuracy: 0.4304 - val_stops_1_accuracy: 0.4565\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 2803 samples, validate on 11212 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2803/2803 [==============================] - 198s 71ms/sample - loss: 5.8859 - starts_0_loss: 1.4096 - stops_0_loss: 1.3760 - starts_1_loss: 1.5339 - stops_1_loss: 1.5894 - starts_0_accuracy: 0.5558 - stops_0_accuracy: 0.6036 - starts_1_accuracy: 0.5466 - stops_1_accuracy: 0.5976 - val_loss: 7.7225 - val_starts_0_loss: 1.8314 - val_stops_0_loss: 1.9025 - val_starts_1_loss: 1.8977 - val_stops_1_loss: 2.0909 - val_starts_0_accuracy: 0.4409 - val_stops_0_accuracy: 0.4622 - val_starts_1_accuracy: 0.4392 - val_stops_1_accuracy: 0.4587\n",
      "Epoch 2/2\n",
      "2803/2803 [==============================] - 175s 62ms/sample - loss: 5.7822 - starts_0_loss: 1.3708 - stops_0_loss: 1.3405 - starts_1_loss: 1.5121 - stops_1_loss: 1.5602 - starts_0_accuracy: 0.5555 - stops_0_accuracy: 0.5979 - starts_1_accuracy: 0.5426 - stops_1_accuracy: 0.5944 - val_loss: 7.7108 - val_starts_0_loss: 1.8268 - val_stops_0_loss: 1.9028 - val_starts_1_loss: 1.8915 - val_stops_1_loss: 2.0897 - val_starts_0_accuracy: 0.4430 - val_stops_0_accuracy: 0.4624 - val_starts_1_accuracy: 0.4386 - val_stops_1_accuracy: 0.4608\n",
      "[INFO]  =============== Validation for FOLD# 4 ===============\n",
      "[INFO] 68.78 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 71.10 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 44.30 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 46.24 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 77.72 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 78.87 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 23.05 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 36.55 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 78.60 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 77.23 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 26.08 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 37.40 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 77.65 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 81.44 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 22.59 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 37.67 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_train_starts.csv\n",
      "[INFO] \t||"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V22_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (2803, 76) (2803, 76)\n",
      "[INFO] Prediction shape for validation data:  (11212, 76) (11212, 76)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  2128 out of 2803\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  8894 out of 11212\n",
      "[INFO] Training Jaccard Score:  0.576284372122289\n",
      "[INFO] Validation Jaccard Score:  0.5138265943405196\n",
      "[INFO] Training for fold: 4 finished at Mon May 18 20:10:20 2020\n",
      "Mon May 18 20:10:20 2020\n"
     ]
    }
   ],
   "source": [
    "for num, (v_index, t_index) in enumerate(sss.split(X, Y_stops)): # NOTICE the swap of val and train indices, on purpose for faster training\n",
    "    print(\"[INFO] ==================== FOLD#\", num, \"====================\")\n",
    "    \n",
    "    if num > 0:\n",
    "        del history\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    \n",
    "    model = build_model()\n",
    "    \n",
    "    mcp = ModelCheckpoint(filepath=\"../results/\"+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\", monitor='val_loss',\n",
    "                          verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "    \n",
    "    csvl = CSVLogger(filename=\"../results/\"+MODEL_PREFIX+\"_LossLogs_\"+str(num)+\".csv\",\n",
    "                     separator=\",\",\n",
    "                     append=True)\n",
    "    \n",
    "    print(\"[INFO] Training only the final layers at higher learning rates.\")\n",
    "    model.layers[3].trainable = False\n",
    "    adam = Adam(learning_rate=MAX_LR)\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'],\n",
    "                  loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    history = model.fit(x={\"att_flags\":X_att[t_index],\n",
    "                           \"words\":X[t_index],\n",
    "                           \"token_ids\":np.zeros_like(X_att[t_index])},\n",
    "                        y={\"starts_0\":Y_starts[t_index],\n",
    "                           \"stops_0\":Y_stops[t_index], \n",
    "                           \"starts_1\":Y_starts[t_index],\n",
    "                           \"stops_1\":Y_stops[t_index]},\n",
    "                        shuffle=True,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS[0],\n",
    "                        validation_data=({\"att_flags\":X_att[v_index],\n",
    "                                          \"words\":X[v_index],\n",
    "                                          \"token_ids\":np.zeros_like(X_att[v_index])},\n",
    "                                         {\"starts_0\":Y_starts[v_index],\n",
    "                                          \"stops_0\":Y_stops[v_index], \n",
    "                                          \"starts_1\":Y_starts[v_index],\n",
    "                                          \"stops_1\":Y_stops[v_index]}),\n",
    "                        verbose=1,\n",
    "                        callbacks=[mcp, csvl])\n",
    "    \n",
    "    print(\"[INFO] Training only the final layers at lower learning rates.\")\n",
    "    adam = Adam(learning_rate=MID_LR)\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'],\n",
    "                  loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    history = model.fit(x={\"att_flags\":X_att[t_index],\n",
    "                           \"words\":X[t_index],\n",
    "                           \"token_ids\":np.zeros_like(X_att[t_index])},\n",
    "                        y={\"starts_0\":Y_starts[t_index],\n",
    "                           \"stops_0\":Y_stops[t_index], \n",
    "                           \"starts_1\":Y_starts[t_index],\n",
    "                           \"stops_1\":Y_stops[t_index]},\n",
    "                        shuffle=True,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS[1],\n",
    "                        validation_data=({\"att_flags\":X_att[v_index],\n",
    "                                          \"words\":X[v_index],\n",
    "                                          \"token_ids\":np.zeros_like(X_att[v_index])},\n",
    "                                         {\"starts_0\":Y_starts[v_index],\n",
    "                                          \"stops_0\":Y_stops[v_index], \n",
    "                                          \"starts_1\":Y_starts[v_index],\n",
    "                                          \"stops_1\":Y_stops[v_index]}),\n",
    "                        verbose=1,\n",
    "                        callbacks=[mcp, csvl])\n",
    "    \n",
    "    print(\"[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\")\n",
    "    model.layers[3].trainable = True\n",
    "    adam = Adam(learning_rate=MIN_LR)\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'],\n",
    "                  loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    history = model.fit(x={\"att_flags\":X_att[t_index],\n",
    "                           \"words\":X[t_index],\n",
    "                           \"token_ids\":np.zeros_like(X_att[t_index])},\n",
    "                        y={\"starts_0\":Y_starts[t_index],\n",
    "                           \"stops_0\":Y_stops[t_index], \n",
    "                           \"starts_1\":Y_starts[t_index],\n",
    "                           \"stops_1\":Y_stops[t_index]},\n",
    "                        shuffle=True,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS[2],\n",
    "                        validation_data=({\"att_flags\":X_att[v_index],\n",
    "                                          \"words\":X[v_index],\n",
    "                                          \"token_ids\":np.zeros_like(X_att[v_index])},\n",
    "                                         {\"starts_0\":Y_starts[v_index],\n",
    "                                          \"stops_0\":Y_stops[v_index], \n",
    "                                          \"starts_1\":Y_starts[v_index],\n",
    "                                          \"stops_1\":Y_stops[v_index]}),\n",
    "                        verbose=1,\n",
    "                        callbacks=[mcp, csvl])\n",
    "    \n",
    "    #model = build_model()\n",
    "    #model.load_weights(\"../results/\"+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\")  \n",
    "    \n",
    "    pred_train = model.predict(x = {\"att_flags\":X_att[t_index],\n",
    "                                    \"words\":X[t_index],\n",
    "                                    \"token_ids\":np.zeros_like(X_att[t_index])},\n",
    "                               batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "    pred_val = model.predict(x = {\"att_flags\":X_att[v_index],\n",
    "                                  \"words\":X[v_index],\n",
    "                                  \"token_ids\":np.zeros_like(X_att[v_index])},\n",
    "                             batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    pred_starts_train, pred_stops_train = (pred_train[0]/2.0 + pred_train[2]/2.0), (pred_train[1]/2.0 + pred_train[3]/2.0)\n",
    "    pred_starts_val, pred_stops_val = (pred_val[0]/2.0 + pred_val[2]/2.0), (pred_val[1]/2.0 + pred_val[3]/2.0)\n",
    "    \n",
    "    # Accumulate test results after training every fold\n",
    "    pred_test_fold = model.predict(x = {\"att_flags\":X_att_test,\n",
    "                                        \"words\":X_test,\n",
    "                                        \"token_ids\":np.zeros_like(X_att_test)},\n",
    "                                   batch_size=PREDICT_BATCH_SIZE)\n",
    "    if num==0:\n",
    "        pred_test = []\n",
    "        pred_test.append(pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test.append(pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    else:\n",
    "        pred_test[0] += (pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test[1] += (pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    \n",
    "    # Tabulate\n",
    "    preds = {\n",
    "        \"train\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_starts[t_index],\n",
    "                \"y_pred\":pred_train[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_stops[t_index],\n",
    "                \"y_pred\":pred_train[1].argmax(axis=1)\n",
    "            }\n",
    "        },\n",
    "        \"valid\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_starts[v_index],\n",
    "                \"y_pred\":pred_val[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_stops[v_index],\n",
    "                \"y_pred\":pred_val[1].argmax(axis=1)\n",
    "            }        \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print_metrics(pred_dict=preds)\n",
    "\n",
    "    print(\"[INFO] Prediction shape for training data: \", pred_starts_train.shape, pred_stops_train.shape)\n",
    "    print(\"[INFO] Prediction shape for validation data: \", pred_starts_val.shape, pred_stops_val.shape)\n",
    "\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for training data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_train.argmax(axis=1),\n",
    "                                  pred_stops_train.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_train.shape[0])\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for validation data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_val.argmax(axis=1),\n",
    "                                  pred_stops_val.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_val.shape[0],)\n",
    "\n",
    "    pred_words_train = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X[t_index],\n",
    "                                                                                                                                   pred_starts_train.argmax(axis=1),\n",
    "                                                                                                                                   pred_stops_train.argmax(axis=1))]\n",
    "    \n",
    "    pred_words_val = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X[v_index],\n",
    "                                                                                                                                 pred_starts_val.argmax(axis=1),\n",
    "                                                                                                                                 pred_stops_val.argmax(axis=1))]\n",
    "\n",
    "    print(\"[INFO] Training Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_words) if n in t_index],\n",
    "                                                          pred_words_train)]))\n",
    "    print(\"[INFO] Validation Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_words) if n in v_index],\n",
    "                                                          pred_words_val)]))\n",
    "    print(\"[INFO] Training for fold:\", num, \"finished at\", ctime(time()))\n",
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Prediction shape for testing data:  (3534, 76) (3534, 76)\n"
     ]
    }
   ],
   "source": [
    "pred_starts_test, pred_stops_test = pred_test[0]/NUM_FOLDS, pred_test[1]/NUM_FOLDS\n",
    "print(\"[INFO] Prediction shape for testing data: \", pred_starts_test.shape, pred_stops_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal predictions (StartIndex less than EndIndex) for testing data: 3141 out of 3534\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal predictions (StartIndex less than EndIndex) for testing data:\",\n",
    "      sum([s<e for s,e in zip(pred_starts_test.argmax(axis=1),\n",
    "                              pred_stops_test.argmax(axis=1))]), \n",
    "      \"out of\",\n",
    "      pred_starts_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words_test = [\n",
    "    post_process(tokenizer.decode(t[s:e+1])) if s<e else post_process(tokenizer.decode(t[e:])) for t,s,e in zip(X_test,\n",
    "                                                                                                              pred_starts_test.argmax(axis=1),\n",
    "                                                                                                              pred_stops_test.argmax(axis=1))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " movie then sleep! today was good day  [{h!--d3ff}]  positive \n",
      "7\n",
      "8\n",
      "good day\n"
     ]
    }
   ],
   "source": [
    "check_idx = 753\n",
    "#print([[t,i,j,k] for t,i,j,k in zip(tokenizer.decode(),X_test[check_idx],pred_starts_test[check_idx],pred_stops_test[check_idx])])\n",
    "print(tokenizer.decode(X_test[check_idx]))\n",
    "print(pred_starts_test.argmax(axis=1)[check_idx])\n",
    "print(pred_stops_test.argmax(axis=1)[check_idx])\n",
    "print(post_process(tokenizer.decode(X_test[check_idx][pred_starts_test.argmax(axis=1)[check_idx]:1+pred_stops_test.argmax(axis=1)[check_idx]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['selected_text'] = pred_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"selected_text\"] = np.where(test_df[\"sentiment\"] == \"neutral\", test_df[\"text\"], test_df[\"selected_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>the voter paint just peeled off my skin. or maybe my skin just peeled off with the voter paint</td>\n",
       "      <td>negative</td>\n",
       "      <td>the voter paint just peeled off my skin. or maybe my skin just peeled off with the voter paint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>why has it been so long since i have talked to you let alone seen you?? it makes me sad</td>\n",
       "      <td>negative</td>\n",
       "      <td>it makes me sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>I`m gonna cry    I went bad at my History test ! I really hate History  !</td>\n",
       "      <td>negative</td>\n",
       "      <td>i`m gonna cry i went bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>umm get ready, help my mum, give GJ his presents when he gets up (could be a while), wait for people to arrive, then party</td>\n",
       "      <td>positive</td>\n",
       "      <td>umm get ready, help my mum, give gj his presents when he gets up (could be a while), wait for people to arrive, then party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>i think there`s something wrong with the video, it only loads the first 40-ish seconds</td>\n",
       "      <td>negative</td>\n",
       "      <td>wrong with the video, it only loads the first 40-ish seconds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>_za thanks man. I can`t wait</td>\n",
       "      <td>positive</td>\n",
       "      <td>_za thanks man. i can`t wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>omgoodness FINALLY back from dinner with the fam bam!</td>\n",
       "      <td>positive</td>\n",
       "      <td>omgoodness finally back from dinner with the fam bam!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>Thankfully that face only shows up for photoshoots</td>\n",
       "      <td>positive</td>\n",
       "      <td>thankfully that face only shows up for photoshoots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>Not only has it broken but it`s Monday!!Does this mean Monday can be fix`d?  ? http://blip.fm/~5jgio</td>\n",
       "      <td>negative</td>\n",
       "      <td>broken but it`s monday!!does this mean monday can be fix`d? ? http://blip.fm/~5jgio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>not happy</td>\n",
       "      <td>negative</td>\n",
       "      <td>not happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>Never realized how good #Techmeme is until I actually read it (duh!) As an avid Tech Crunch reader, this is much better...</td>\n",
       "      <td>positive</td>\n",
       "      <td>never realized how good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2737</th>\n",
       "      <td>nice night, should be golfing</td>\n",
       "      <td>positive</td>\n",
       "      <td>nice night, should be golfing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>All alone in this old house again.  Thanks for the net which keeps me alive and kicking! Whoever invented the net, i wanna kiss your hair!</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks for the net which keeps me alive and kicking! whoever invented the net, i wanna kiss your hair!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>wishes happy mothers` day to all moms out there.</td>\n",
       "      <td>positive</td>\n",
       "      <td>wishes happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>#SanctuarySunday  thanks for joining on #SanctuarySunday, follow more Sanctuary people to keep up to date on the tweets!</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks for joining on #sanctuarysunday, follow more sanctuary people to keep up to date on the tweets!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>Really sick</td>\n",
       "      <td>negative</td>\n",
       "      <td>really sick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>Man i am being boring today not tweeting  How are you guys?</td>\n",
       "      <td>negative</td>\n",
       "      <td>man i am being boring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>i would but he`s premiering avatar footage and can`t do it then. also, we better hang out.</td>\n",
       "      <td>positive</td>\n",
       "      <td>i would but he`s premiering avatar footage and can`t do it then. also, we better hang out.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>http://twitpic.com/4u5h8 - leon looks supa` fly on that mini couch</td>\n",
       "      <td>positive</td>\n",
       "      <td>- leon looks supa` fly on that mini couch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>Sites that republish my blog`s feed often end up higher than my blog on Google search results</td>\n",
       "      <td>negative</td>\n",
       "      <td>sites that republish my blog`s feed often end up higher than my blog on google search results</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>Birthday tomorrow. Doing jack **** all weekend</td>\n",
       "      <td>negative</td>\n",
       "      <td>birthday tomorrow. doing jack **** all weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>KILL IT JASMINE! Haven`t talked to you for ages.</td>\n",
       "      <td>negative</td>\n",
       "      <td>kill it jasmine! haven`t talked to you for ages.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>Hey!  It`s easy...Just type what you`re doing!  Just like facebook, but much simpler.  Have a good day!</td>\n",
       "      <td>positive</td>\n",
       "      <td>hey! it`s easy...just type what you`re doing! just like facebook, but much simpler. have a good day!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>I miss u like cotton candy  &lt;3</td>\n",
       "      <td>negative</td>\n",
       "      <td>miss u like cotton candy &lt;3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>oh no! my fun weekend with friends is gone! my mother has made a family weekend of it!  ****</td>\n",
       "      <td>negative</td>\n",
       "      <td>oh no! my fun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            text  \\\n",
       "2309                                              the voter paint just peeled off my skin. or maybe my skin just peeled off with the voter paint   \n",
       "745                                                      why has it been so long since i have talked to you let alone seen you?? it makes me sad   \n",
       "1106                                                                   I`m gonna cry    I went bad at my History test ! I really hate History  !   \n",
       "1199                  umm get ready, help my mum, give GJ his presents when he gets up (could be a while), wait for people to arrive, then party   \n",
       "2462                                                      i think there`s something wrong with the video, it only loads the first 40-ish seconds   \n",
       "1446                                                                                                                _za thanks man. I can`t wait   \n",
       "1770                                                                                       omgoodness FINALLY back from dinner with the fam bam!   \n",
       "552                                                                                           Thankfully that face only shows up for photoshoots   \n",
       "681                                         Not only has it broken but it`s Monday!!Does this mean Monday can be fix`d?  ? http://blip.fm/~5jgio   \n",
       "1581                                                                                                                                   not happy   \n",
       "2283                  Never realized how good #Techmeme is until I actually read it (duh!) As an avid Tech Crunch reader, this is much better...   \n",
       "2737                                                                                                               nice night, should be golfing   \n",
       "3530  All alone in this old house again.  Thanks for the net which keeps me alive and kicking! Whoever invented the net, i wanna kiss your hair!   \n",
       "2095                                                                                            wishes happy mothers` day to all moms out there.   \n",
       "289                     #SanctuarySunday  thanks for joining on #SanctuarySunday, follow more Sanctuary people to keep up to date on the tweets!   \n",
       "774                                                                                                                                  Really sick   \n",
       "1981                                                                                 Man i am being boring today not tweeting  How are you guys?   \n",
       "1078                                                  i would but he`s premiering avatar footage and can`t do it then. also, we better hang out.   \n",
       "2151                                                                          http://twitpic.com/4u5h8 - leon looks supa` fly on that mini couch   \n",
       "644                                                Sites that republish my blog`s feed often end up higher than my blog on Google search results   \n",
       "1792                                                                                              Birthday tomorrow. Doing jack **** all weekend   \n",
       "572                                                                                             KILL IT JASMINE! Haven`t talked to you for ages.   \n",
       "3385                                     Hey!  It`s easy...Just type what you`re doing!  Just like facebook, but much simpler.  Have a good day!   \n",
       "2618                                                                                                              I miss u like cotton candy  <3   \n",
       "880                                                 oh no! my fun weekend with friends is gone! my mother has made a family weekend of it!  ****   \n",
       "\n",
       "     sentiment  \\\n",
       "2309  negative   \n",
       "745   negative   \n",
       "1106  negative   \n",
       "1199  positive   \n",
       "2462  negative   \n",
       "1446  positive   \n",
       "1770  positive   \n",
       "552   positive   \n",
       "681   negative   \n",
       "1581  negative   \n",
       "2283  positive   \n",
       "2737  positive   \n",
       "3530  positive   \n",
       "2095  positive   \n",
       "289   positive   \n",
       "774   negative   \n",
       "1981  negative   \n",
       "1078  positive   \n",
       "2151  positive   \n",
       "644   negative   \n",
       "1792  negative   \n",
       "572   negative   \n",
       "3385  positive   \n",
       "2618  negative   \n",
       "880   negative   \n",
       "\n",
       "                                                                                                                   selected_text  \n",
       "2309                              the voter paint just peeled off my skin. or maybe my skin just peeled off with the voter paint  \n",
       "745                                                                                                              it makes me sad  \n",
       "1106                                                                                                    i`m gonna cry i went bad  \n",
       "1199  umm get ready, help my mum, give gj his presents when he gets up (could be a while), wait for people to arrive, then party  \n",
       "2462                                                                wrong with the video, it only loads the first 40-ish seconds  \n",
       "1446                                                                                                _za thanks man. i can`t wait  \n",
       "1770                                                                       omgoodness finally back from dinner with the fam bam!  \n",
       "552                                                                           thankfully that face only shows up for photoshoots  \n",
       "681                                          broken but it`s monday!!does this mean monday can be fix`d? ? http://blip.fm/~5jgio  \n",
       "1581                                                                                                                   not happy  \n",
       "2283                                                                                                     never realized how good  \n",
       "2737                                                                                               nice night, should be golfing  \n",
       "3530                      thanks for the net which keeps me alive and kicking! whoever invented the net, i wanna kiss your hair!  \n",
       "2095                                                                                                                wishes happy  \n",
       "289                       thanks for joining on #sanctuarysunday, follow more sanctuary people to keep up to date on the tweets!  \n",
       "774                                                                                                                  really sick  \n",
       "1981                                                                                                       man i am being boring  \n",
       "1078                                  i would but he`s premiering avatar footage and can`t do it then. also, we better hang out.  \n",
       "2151                                                                                   - leon looks supa` fly on that mini couch  \n",
       "644                                sites that republish my blog`s feed often end up higher than my blog on google search results  \n",
       "1792                                                                              birthday tomorrow. doing jack **** all weekend  \n",
       "572                                                                             kill it jasmine! haven`t talked to you for ages.  \n",
       "3385                        hey! it`s easy...just type what you`re doing! just like facebook, but much simpler. have a good day!  \n",
       "2618                                                                                                 miss u like cotton candy <3  \n",
       "880                                                                                                                oh no! my fun  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[test_df.sentiment!=\"neutral\"][[\"text\", \"sentiment\",\"selected_text\"]].sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[[\"textID\", \"selected_text\"]].to_csv(\"../results/submission_\"+MODEL_PREFIX+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 18 20:10:20 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
