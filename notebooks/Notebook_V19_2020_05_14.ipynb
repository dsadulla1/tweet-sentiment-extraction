{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V19\"\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "TRAIN_SPLIT_RATIO = 0.8\n",
    "NUM_FOLDS = 5\n",
    "RUN_ON_SAMPLE = False\n",
    "\n",
    "DROPOUT = 0.3\n",
    "MIN_LR = 3e-5\n",
    "BATCH_SIZE = 16\n",
    "PREDICT_BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, MaxPooling1D, Layer, AdditiveAttention, Attention\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(98765)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 16 02:15:09 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0', '2.8.0', '0.5.2']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making GPUs Flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  1\n",
      "selected_text  object  1\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27480, 'selected_text': 22463, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count        27481   \n",
      "unique       27481   \n",
      "top     ec38c9863d   \n",
      "freq             1   \n",
      "\n",
      "                                                                                                                      text  \\\n",
      "count                                                                                                                27480   \n",
      "unique                                                                                                               27480   \n",
      "top     _c re:headaches my wife is in the same situation and has finally decided to go with meds because its just too much   \n",
      "freq                                                                                                                     1   \n",
      "\n",
      "       selected_text sentiment  \n",
      "count          27480     27481  \n",
      "unique         22463         3  \n",
      "top             good   neutral  \n",
      "freq             199     11118  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/train.csv\", encoding=\"utf8\")\n",
    "\n",
    "if RUN_ON_SAMPLE:\n",
    "    df = df.sample(5000).copy()\n",
    "\n",
    "print(pd.concat((df.dtypes, df.isna().sum()), axis=1))\n",
    "print(df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df[i].nunique() for i in df.columns})\n",
    "print(df.describe())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID                                       text sentiment\n",
      "count         3534                                       3534      3534\n",
      "unique        3534                                       3534         3\n",
      "top     a6a3368db8  Midnight ice-cream weather! So **** bored   neutral\n",
      "freq             1                                          1      1430\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "print(pd.concat((test_df.dtypes, test_df.isna().sum()), axis=1))\n",
    "print(test_df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df[i].nunique() for i in test_df.columns})\n",
    "print(test_df.describe())\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>c77717b103</td>\n",
       "      <td>I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½</td>\n",
       "      <td>I love to!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>28dbada620</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "44   c77717b103   \n",
       "192  28dbada620   \n",
       "\n",
       "                                                                                                       text  \\\n",
       "44    I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½   \n",
       "192                                *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "                                                                selected_text  \\\n",
       "44                                                                 I love to!   \n",
       "192  *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "    sentiment  \n",
       "44   positive  \n",
       "192   neutral  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>7223fdccc2</td>\n",
       "      <td>tikcets are only ï¿½91...each...BUT I SO WANT TO GO</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>43ad351369</td>\n",
       "      <td>AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "145  7223fdccc2   \n",
       "618  43ad351369   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "145                                                                                             tikcets are only ï¿½91...each...BUT I SO WANT TO GO   \n",
       "618  AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s   \n",
       "\n",
       "    sentiment  \n",
       "145  positive  \n",
       "618  positive  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[test_df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment count in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>7781</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>11117</td>\n",
       "      <td>1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>8582</td>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text  text\n",
       "sentiment             \n",
       "negative    7781  1001\n",
       "neutral    11117  1430\n",
       "positive    8582  1103"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df.groupby(\"sentiment\")[[\"text\"]].count(), test_df.groupby(\"sentiment\")[[\"text\"]].count()], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For traceability\n",
    "df[\"original_index\"] = df.index\n",
    "test_df[\"original_index\"] = test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_idxs = [513, 1060, 5543, 8410, 13605, 24778, 25258] + [18, 27, 32, 39, 48, 49, 64, 66, 84, 92, 102, 116, 129, 132, 134, 149, 160, 166, 168, 189, 196, 197, 210, 223, 247, 251, 260, 285, 295, 296, 297, 309, 349, 362, 366, 382, 396, 398, 406, 410, 425, 458, 460, 482, 492, 500, 504, 517, 533, 537, 543, 569, 573, 581, 604, 605, 608, 634, 636, 637, 639, 642, 665, 670, 674, 678, 685, 702, 707, 709, 710, 727, 737, 746, 751, 757, 768, 778, 787, 794, 804, 809, 853, 854, 856, 863, 871, 872, 912, 931, 937, 942, 950, 956, 957, 963, 992, 993, 999, 1001, 1011, 1012, 1036, 1039, 1057, 1062, 1076, 1077, 1083, 1096, 1105, 1116, 1117, 1122, 1134, 1137, 1150, 1159, 1199, 1200, 1209, 1217, 1264, 1271, 1280, 1283, 1298, 1300, 1303, 1319, 1327, 1342, 1360, 1363, 1365, 1372, 1374, 1376, 1382, 1393, 1416, 1417, 1420, 1447, 1454, 1468, 1472, 1513, 1515, 1518, 1531, 1548, 1558, 1567, 1580, 1588, 1605, 1623, 1648, 1649, 1657, 1659, 1661, 1665, 1672, 1674, 1678, 1683, 1693, 1696, 1713, 1723, 1728, 1739, 1747, 1754, 1798, 1806, 1808, 1814, 1822, 1828, 1830, 1835, 1850, 1857, 1860, 1866, 1888, 1901, 1902, 1929, 1938, 1940, 1987, 1994, 2003, 2004, 2008, 2011, 2023, 2054, 2056, 2065, 2068, 2083, 2086, 2094, 2106, 2113, 2124, 2125, 2136, 2139, 2145, 2168, 2180, 2181, 2187, 2213, 2225, 2227, 2230, 2239, 2273, 2279, 2286, 2288, 2298, 2323, 2324, 2335, 2343, 2359, 2364, 2371, 2385, 2388, 2392, 2398, 2401, 2404, 2410, 2415, 2421, 2430, 2438, 2439, 2440, 2444, 2445, 2448, 2470, 2478, 2489, 2499, 2505, 2515, 2520, 2559, 2574, 2576, 2589, 2598, 2600, 2640, 2643, 2650, 2671, 2679, 2696, 2699, 2706, 2716, 2718, 2731, 2743, 2749, 2750, 2785, 2787, 2789, 2805, 2809, 2825, 2838, 2856, 2888, 2898, 2903, 2923, 2925, 2931, 2933, 2942, 2957, 2961, 2986, 2995, 3034, 3068, 3075, 3077, 3080, 3088, 3089, 3096, 3097, 3101, 3121, 3147, 3160, 3161, 3176, 3182, 3187, 3211, 3225, 3234, 3257, 3263, 3267, 3281, 3290, 3295, 3317, 3319, 3325, 3333, 3359, 3364, 3369, 3374, 3390, 3399, 3400, 3411, 3424, 3473, 3475, 3476, 3479, 3492, 3499, 3504, 3509, 3517, 3519, 3532, 3553, 3578, 3588, 3590, 3593, 3598, 3605, 3606, 3607, 3622, 3634, 3669, 3672, 3678, 3682, 3685, 3694, 3708, 3720, 3729, 3750, 3755, 3784, 3794, 3798, 3801, 3806, 3807, 3813, 3826, 3834, 3872, 3886, 3895, 3903, 3925, 3937, 3946, 3962, 3969, 3998, 4031, 4047, 4053, 4074, 4091, 4093, 4100, 4108, 4113, 4119, 4143, 4148, 4163, 4164, 4183, 4196, 4254, 4260, 4265, 4307, 4308, 4309, 4312, 4318, 4325, 4327, 4328, 4329, 4344, 4347, 4369, 4371, 4377, 4384, 4402, 4404, 4405, 4407, 4412, 4421, 4422, 4437, 4455, 4457, 4471, 4473, 4479, 4483, 4486, 4514, 4532, 4537, 4542, 4543, 4545, 4550, 4554, 4563, 4575, 4576, 4591, 4595, 4631, 4636, 4637, 4639, 4654, 4655, 4704, 4706, 4714, 4718, 4724, 4729, 4735, 4747, 4749, 4750, 4764, 4800, 4814, 4841, 4842, 4844, 4864, 4866, 4869, 4875, 4891, 4908, 4918, 4924, 4928, 4933, 4943, 4946, 4947, 4956, 4968, 4980, 4994, 5009, 5011, 5038, 5056, 5079, 5083, 5101, 5133, 5167, 5173, 5174, 5188, 5189, 5196, 5197, 5213, 5218, 5229, 5241, 5245, 5258, 5279, 5290, 5294, 5297, 5308, 5317, 5331, 5342, 5343, 5353, 5355, 5356, 5358, 5379, 5385, 5386, 5399, 5418, 5433, 5436, 5450, 5456, 5466, 5489, 5504, 5509, 5510, 5530, 5542, 5560, 5583, 5603, 5607, 5618, 5620, 5624, 5626, 5678, 5697, 5712, 5725, 5751, 5757, 5775, 5785, 5799, 5822, 5832, 5836, 5844, 5895, 5902, 5904, 5913, 5918, 5938, 5939, 5961, 5984, 5990, 6025, 6031, 6033, 6060, 6077, 6084, 6090, 6113, 6131, 6134, 6151, 6152, 6176, 6203, 6229, 6230, 6248, 6261, 6277, 6289, 6296, 6304, 6308, 6317, 6319, 6321, 6335, 6338, 6343, 6353, 6356, 6360, 6377, 6382, 6395, 6404, 6425, 6433, 6468, 6474, 6475, 6476, 6477, 6510, 6528, 6540, 6574, 6580, 6582, 6593, 6609, 6627, 6628, 6633, 6635, 6650, 6656, 6657, 6670, 6673, 6676, 6677, 6679, 6686, 6702, 6724, 6740, 6745, 6759, 6801, 6804, 6830, 6833, 6834, 6842, 6859, 6871, 6875, 6878, 6880, 6885, 6896, 6934, 6939, 6948, 6950, 6958, 6970, 6979, 6987, 7010, 7017, 7024, 7040, 7099, 7107, 7112, 7136, 7147, 7149, 7158, 7184, 7225, 7227, 7251, 7274, 7288, 7305, 7306, 7308, 7331, 7344, 7355, 7373, 7407, 7408, 7409, 7410, 7423, 7436, 7438, 7442, 7444, 7450, 7469, 7473, 7479, 7487, 7492, 7506, 7513, 7521, 7528, 7530, 7546, 7567, 7581, 7582, 7597, 7599, 7602, 7632, 7635, 7636, 7642, 7662, 7663, 7701, 7752, 7758, 7765, 7769, 7778, 7780, 7788, 7801, 7816, 7818, 7831, 7836, 7837, 7849, 7893, 7895, 7896, 7922, 7927, 7945, 7948, 7968, 7997, 8003, 8005, 8010, 8030, 8049, 8051, 8052, 8059, 8060, 8085, 8094, 8142, 8143, 8151, 8153, 8161, 8169, 8170, 8196, 8208, 8212, 8215, 8224, 8235, 8241, 8249, 8250, 8276, 8282, 8285, 8286, 8288, 8289, 8299, 8314, 8338, 8343, 8345, 8349, 8373, 8379, 8390, 8397, 8399, 8407, 8427, 8428, 8432, 8439, 8442, 8444, 8484, 8493, 8497, 8500, 8507, 8510, 8534, 8546, 8559, 8569, 8579, 8588, 8594, 8604, 8617, 8620, 8624, 8631, 8638, 8661, 8671, 8674, 8687, 8691, 8696, 8706, 8720, 8729, 8749, 8756, 8758, 8769, 8774, 8780, 8800, 8803, 8809, 8815, 8827, 8830, 8875, 8902, 8906, 8910, 8917, 8919, 8922, 8951, 8954, 8968, 8985, 8986, 8991, 8999, 9014, 9034, 9050, 9057, 9063, 9070, 9088, 9098, 9112, 9113, 9122, 9127, 9140, 9155, 9157, 9169, 9179, 9190, 9216, 9223, 9226, 9264, 9272, 9281, 9297, 9300, 9303, 9322, 9354, 9361, 9374, 9377, 9381, 9385, 9400, 9403, 9419, 9422, 9425, 9428, 9436, 9442, 9449, 9454, 9456, 9458, 9470, 9482, 9496, 9500, 9535, 9539, 9541, 9554, 9556, 9560, 9565, 9574, 9594, 9605, 9610, 9631, 9632, 9660, 9689, 9691, 9696, 9708, 9710, 9712, 9725, 9726, 9734, 9742, 9753, 9770, 9789, 9799, 9801, 9812, 9817, 9836, 9839, 9850, 9852, 9881, 9882, 9891, 9906, 9912, 9920, 9922, 9928, 9976, 9980, 9984, 10002, 10004, 10007, 10029, 10039, 10042, 10050, 10052, 10057, 10067, 10070, 10092, 10093, 10123, 10131, 10138, 10145, 10151, 10164, 10167, 10170, 10179, 10184, 10185, 10190, 10197, 10200, 10211, 10218, 10219, 10222, 10228, 10245, 10247, 10255, 10259, 10276, 10293, 10298, 10299, 10306, 10319, 10327, 10342, 10348, 10377, 10379, 10449, 10465, 10469, 10492, 10504, 10505, 10508, 10521, 10528, 10529, 10530, 10570, 10573, 10577, 10578, 10597, 10611, 10639, 10651, 10666, 10672, 10675, 10690, 10706, 10711, 10738, 10742, 10747, 10749, 10779, 10782, 10783, 10795, 10813, 10849, 10852, 10853, 10884, 10885, 10888, 10892, 10901, 10914, 10918, 10919, 10933, 10935, 10956, 10966, 10968, 10970, 10981, 10986, 10996, 10999, 11005, 11032, 11038, 11051, 11055, 11060, 11067, 11097, 11102, 11155, 11157, 11159, 11162, 11167, 11176, 11187, 11215, 11222, 11228, 11230, 11257, 11263, 11265, 11274, 11286, 11289, 11297, 11300, 11308, 11323, 11337, 11339, 11341, 11347, 11349, 11353, 11371, 11381, 11386, 11426, 11431, 11448, 11449, 11474, 11480, 11500, 11507, 11553, 11562, 11582, 11588, 11594, 11612, 11616, 11618, 11636, 11643, 11650, 11658, 11666, 11679, 11683, 11693, 11695, 11698, 11699, 11706, 11716, 11723, 11730, 11732, 11741, 11745, 11780, 11794, 11798, 11808, 11812, 11828, 11829, 11837, 11840, 11841, 11844, 11849, 11853, 11854, 11855, 11861, 11862, 11872, 11875, 11878, 11886, 11890, 11899, 11905, 11912, 11938, 11963, 11967, 11973, 11980, 11985, 11999, 12026, 12027, 12029, 12039, 12054, 12067, 12115, 12124, 12134, 12136, 12138, 12150, 12187, 12205, 12206, 12224, 12242, 12258, 12269, 12277, 12283, 12295, 12305, 12314, 12322, 12331, 12333, 12334, 12350, 12355, 12356, 12360, 12370, 12372, 12389, 12395, 12396, 12397, 12405, 12410, 12416, 12429, 12439, 12440, 12442, 12446, 12474, 12482, 12486, 12507, 12516, 12521, 12522, 12526, 12527, 12537, 12550, 12563, 12576, 12585, 12586, 12587, 12598, 12606, 12630, 12631, 12635, 12639, 12650, 12657, 12662, 12669, 12687, 12703, 12718, 12724, 12732, 12736, 12745, 12767, 12787, 12803, 12804, 12808, 12818, 12830, 12843, 12855, 12856, 12870, 12884, 12903, 12914, 12920, 12923, 12924, 12927, 12936, 12972, 12977, 12998, 13003, 13004, 13009, 13044, 13068, 13069, 13074, 13080, 13095, 13098, 13124, 13130, 13153, 13154, 13176, 13219, 13222, 13227, 13237, 13267, 13274, 13293, 13301, 13304, 13307, 13330, 13333, 13354, 13365, 13373, 13379, 13381, 13389, 13397, 13422, 13430, 13433, 13445, 13466, 13473, 13476, 13482, 13490, 13493, 13528, 13529, 13535, 13543, 13558, 13559, 13586, 13637, 13643, 13646, 13665, 13674, 13678, 13679, 13704, 13707, 13710, 13718, 13727, 13733, 13747, 13762, 13772, 13785, 13796, 13803, 13812, 13814, 13817, 13827, 13844, 13846, 13847, 13848, 13854, 13855, 13861, 13864, 13873, 13899, 13907, 13936, 13939, 13946, 13949, 13954, 13964, 13965, 13974, 13975, 13978, 13980, 14002, 14006, 14007, 14030, 14043, 14044, 14057, 14058, 14060, 14073, 14091, 14109, 14110, 14112, 14117, 14131, 14132, 14159, 14172, 14173, 14176, 14184, 14194, 14197, 14200, 14204, 14207, 14213, 14230, 14257, 14273, 14275, 14287, 14299, 14301, 14307, 14308, 14347, 14362, 14365, 14381, 14386, 14400, 14416, 14428, 14436, 14446, 14457, 14458, 14462, 14487, 14499, 14509, 14532, 14548, 14551, 14571, 14596, 14598, 14611, 14621, 14639, 14640, 14648, 14662, 14669, 14671, 14676, 14680, 14687, 14689, 14691, 14718, 14731, 14747, 14779, 14787, 14825, 14839, 14844, 14847, 14855, 14870, 14880, 14891, 14908, 14946, 14959, 14960, 14967, 14971, 14986, 15007, 15010, 15016, 15044, 15049, 15055, 15056, 15112, 15131, 15158, 15165, 15174, 15177, 15196, 15203, 15206, 15207, 15213, 15217, 15233, 15236, 15253, 15259, 15286, 15296, 15307, 15308, 15321, 15323, 15327, 15339, 15363, 15372, 15374, 15379, 15380, 15383, 15385, 15392, 15410, 15449, 15452, 15462, 15464, 15465, 15477, 15483, 15503, 15505, 15528, 15532, 15548, 15562, 15566, 15571, 15605, 15610, 15622, 15651, 15660, 15666, 15674, 15709, 15721, 15731, 15735, 15736, 15741, 15767, 15774, 15785, 15792, 15793, 15811, 15851, 15879, 15883, 15884, 15912, 15926, 15927, 15931, 15940, 15959, 15965, 15985, 15998, 16020, 16027, 16031, 16032, 16044, 16070, 16100, 16117, 16129, 16136, 16137, 16140, 16149, 16152, 16175, 16186, 16201, 16241, 16261, 16269, 16272, 16279, 16290, 16308, 16319, 16324, 16325, 16327, 16338, 16341, 16345, 16352, 16354, 16371, 16372, 16385, 16391, 16416, 16423, 16448, 16455, 16468, 16470, 16477, 16481, 16484, 16491, 16493, 16500, 16509, 16510, 16516, 16541, 16559, 16577, 16589, 16632, 16638, 16642, 16643, 16649, 16650, 16654, 16665, 16677, 16684, 16704, 16720, 16726, 16734, 16765, 16774, 16775, 16777, 16780, 16797, 16798, 16799, 16803, 16807, 16809, 16830, 16832, 16861, 16863, 16868, 16876, 16889, 16890, 16902, 16915, 16927, 16939, 16949, 16962, 16968, 17002, 17010, 17013, 17021, 17023, 17029, 17062, 17066, 17084, 17093, 17094, 17102, 17152, 17160, 17167, 17173, 17183, 17221, 17239, 17250, 17267, 17277, 17278, 17282, 17296, 17307, 17315, 17333, 17336, 17344, 17346, 17348, 17354, 17365, 17374, 17385, 17387, 17397, 17401, 17404, 17412, 17417, 17432, 17436, 17444, 17452, 17467, 17469, 17501, 17502, 17513, 17530, 17531, 17547, 17550, 17555, 17573, 17577, 17578, 17586, 17600, 17602, 17611, 17627, 17640, 17645, 17648, 17649, 17652, 17667, 17676, 17682, 17721, 17724, 17729, 17731, 17751, 17754, 17762, 17770, 17772, 17781, 17802, 17820, 17823, 17833, 17848, 17857, 17858, 17865, 17867, 17887, 17893, 17909, 17924, 17926, 17945, 17971, 17977, 17986, 17990, 18003, 18011, 18042, 18046, 18052, 18077, 18086, 18091, 18099, 18103, 18123, 18128, 18131, 18142, 18153, 18165, 18167, 18173, 18181, 18206, 18226, 18252, 18263, 18276, 18277, 18284, 18314, 18319, 18327, 18330, 18342, 18352, 18355, 18362, 18364, 18375, 18383, 18398, 18403, 18432, 18447, 18453, 18465, 18467, 18485, 18486, 18492, 18505, 18508, 18510, 18515, 18536, 18540, 18548, 18550, 18557, 18563, 18570, 18574, 18576, 18616, 18622, 18627, 18639, 18649, 18655, 18673, 18681, 18712, 18745, 18746, 18758, 18773, 18778, 18782, 18826, 18832, 18834, 18842, 18851, 18853, 18858, 18862, 18867, 18872, 18877, 18908, 18910, 18912, 18913, 18919, 18925, 18929, 18930, 18933, 18936, 18944, 18974, 18975, 18997, 19009, 19011, 19014, 19016, 19028, 19057, 19098, 19106, 19114, 19130, 19133, 19147, 19157, 19162, 19166, 19173, 19179, 19198, 19213, 19225, 19230, 19239, 19241, 19266, 19274, 19279, 19283, 19285, 19316, 19327, 19335, 19352, 19358, 19362, 19366, 19377, 19379, 19405, 19423, 19428, 19431, 19437, 19444, 19456, 19469, 19475, 19476, 19481, 19486, 19492, 19506, 19507, 19536, 19537, 19562, 19586, 19589, 19622, 19634, 19651, 19663, 19679, 19685, 19690, 19697, 19707, 19712, 19714, 19715, 19717, 19724, 19734, 19744, 19754, 19769, 19779, 19792, 19807, 19830, 19850, 19863, 19875, 19903, 19904, 19908, 19963, 19985, 19988, 20003, 20012, 20014, 20018, 20040, 20042, 20054, 20078, 20079, 20083, 20087, 20089, 20114, 20121, 20122, 20138, 20155, 20162, 20186, 20199, 20216, 20222, 20234, 20238, 20254, 20285, 20299, 20302, 20305, 20337, 20345, 20364, 20365, 20374, 20382, 20394, 20403, 20440, 20450, 20451, 20476, 20506, 20508, 20516, 20527, 20536, 20540, 20541, 20542, 20557, 20566, 20584, 20608, 20611, 20613, 20618, 20627, 20644, 20656, 20663, 20674, 20682, 20690, 20699, 20707, 20711, 20737, 20751, 20765, 20771, 20774, 20778, 20779, 20789, 20795, 20798, 20834, 20851, 20865, 20867, 20869, 20872, 20883, 20892, 20894, 20895, 20896, 20901, 20919, 20924, 20933, 20937, 20938, 20946, 20947, 20955, 20957, 20958, 20965, 20966, 20970, 20974, 20991, 21001, 21013, 21018, 21020, 21025, 21114, 21142, 21205, 21210, 21227, 21230, 21236, 21253, 21262, 21294, 21300, 21316, 21331, 21339, 21349, 21356, 21365, 21369, 21376, 21399, 21410, 21413, 21418, 21420, 21435, 21447, 21452, 21455, 21468, 21470, 21474, 21486, 21495, 21501, 21517, 21530, 21539, 21546, 21547, 21553, 21556, 21564, 21569, 21579, 21593, 21595, 21599, 21603, 21632, 21643, 21690, 21713, 21726, 21732, 21737, 21738, 21744, 21755, 21773, 21782, 21800, 21807, 21829, 21839, 21848, 21876, 21893, 21923, 21926, 21931, 21958, 21967, 21970, 21973, 21982, 22002, 22029, 22031, 22035, 22044, 22051, 22068, 22073, 22099, 22117, 22121, 22126, 22145, 22146, 22149, 22161, 22166, 22170, 22177, 22205, 22210, 22231, 22233, 22234, 22244, 22246, 22251, 22272, 22280, 22299, 22317, 22321, 22327, 22350, 22355, 22363, 22365, 22378, 22383, 22387, 22391, 22393, 22395, 22403, 22406, 22418, 22423, 22428, 22437, 22444, 22451, 22453, 22460, 22480, 22491, 22510, 22530, 22536, 22554, 22557, 22561, 22562, 22564, 22571, 22588, 22593, 22606, 22607, 22610, 22611, 22642, 22666, 22717, 22729, 22738, 22744, 22745, 22763, 22768, 22769, 22774, 22782, 22789, 22796, 22800, 22836, 22841, 22843, 22858, 22864, 22886, 22889, 22897, 22925, 22928, 22938, 22944, 22945, 22948, 22950, 22956, 22983, 22986, 22988, 22993, 22994, 22997, 23028, 23030, 23039, 23047, 23081, 23091, 23097, 23105, 23108, 23111, 23113, 23133, 23145, 23150, 23161, 23172, 23176, 23199, 23200, 23202, 23204, 23205, 23211, 23212, 23226, 23227, 23231, 23236, 23239, 23263, 23265, 23266, 23269, 23290, 23311, 23315, 23320, 23323, 23340, 23352, 23361, 23372, 23375, 23376, 23381, 23386, 23390, 23401, 23415, 23418, 23428, 23429, 23453, 23467, 23479, 23505, 23508, 23527, 23528, 23535, 23537, 23541, 23553, 23555, 23557, 23559, 23563, 23566, 23581, 23585, 23596, 23598, 23604, 23605, 23617, 23630, 23643, 23658, 23669, 23673, 23680, 23683, 23690, 23691, 23701, 23707, 23714, 23720, 23725, 23733, 23746, 23750, 23751, 23769, 23774, 23784, 23809, 23818, 23831, 23839, 23842, 23857, 23924, 23930, 23944, 23945, 23947, 23959, 23971, 23975, 23987, 23989, 24002, 24003, 24009, 24019, 24026, 24039, 24041, 24046, 24062, 24071, 24077, 24079, 24087, 24096, 24119, 24121, 24137, 24150, 24169, 24181, 24194, 24195, 24199, 24200, 24203, 24210, 24223, 24258, 24272, 24274, 24291, 24293, 24300, 24305, 24318, 24339, 24378, 24380, 24383, 24392, 24402, 24442, 24468, 24476, 24480, 24488, 24490, 24495, 24502, 24504, 24505, 24516, 24562, 24572, 24594, 24595, 24597, 24600, 24606, 24609, 24622, 24623, 24629, 24631, 24632, 24633, 24657, 24662, 24664, 24666, 24682, 24697, 24709, 24753, 24758, 24766, 24833, 24840, 24851, 24858, 24869, 24886, 24909, 24915, 24916, 24929, 24945, 24951, 24966, 24972, 24980, 24996, 25012, 25017, 25043, 25059, 25067, 25104, 25121, 25125, 25127, 25133, 25150, 25191, 25210, 25224, 25228, 25240, 25243, 25255, 25264, 25267, 25272, 25282, 25284, 25290, 25292, 25293, 25334, 25338, 25350, 25365, 25368, 25380, 25389, 25391, 25400, 25413, 25422, 25440, 25444, 25446, 25456, 25460, 25472, 25486, 25495, 25499, 25509, 25512, 25528, 25530, 25532, 25547, 25559, 25591, 25593, 25597, 25601, 25614, 25616, 25638, 25672, 25691, 25698, 25712, 25713, 25721, 25722, 25725, 25732, 25753, 25760, 25775, 25794, 25813, 25819, 25828, 25833, 25901, 25908, 25947, 25983, 25987, 25989, 25996, 26008, 26017, 26025, 26030, 26039, 26047, 26054, 26058, 26077, 26088, 26092, 26112, 26183, 26185, 26190, 26201, 26204, 26237, 26239, 26241, 26248, 26251, 26252, 26256, 26263, 26268, 26282, 26314, 26331, 26346, 26358, 26360, 26364, 26377, 26428, 26434, 26460, 26481, 26485, 26497, 26532, 26533, 26538, 26561, 26567, 26585, 26588, 26595, 26596, 26618, 26625, 26633, 26643, 26651, 26677, 26687, 26715, 26721, 26732, 26733, 26746, 26760, 26762, 26769, 26779, 26781, 26822, 26828, 26830, 26870, 26882, 26883, 26889, 26892, 26894, 26914, 26927, 26932, 26944, 26953, 26957, 26963, 26967, 26973, 26976, 26978, 26985, 27030, 27038, 27044, 27048, 27052, 27067, 27111, 27115, 27121, 27142, 27144, 27153, 27167, 27205, 27209, 27217, 27218, 27229, 27233, 27240, 27250, 27280, 27302, 27332, 27339, 27349, 27362, 27376, 27386, 27396, 27401, 27413, 27426, 27429, 27456, 27470, 27474, 27476, 27477, 27480]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27481, 5)\n",
      "(24857, 5)\n",
      "(24856, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df = df[(~df.index.isin(anomalous_idxs))]\n",
    "print(df.shape)\n",
    "df = df[(~df.text.isna())]\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.copy()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment_code\"] = df[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments = df[\"sentiment_code\"].cat.codes.values\n",
    "\n",
    "test_df[\"sentiment_code\"] = test_df[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments_test = test_df[\"sentiment_code\"].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"selected_text\"] = df[\"selected_text\"].astype(str)\n",
    "test_df[\"text\"] = test_df[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_mod\"] = \"<s> \" + df.text.str.strip() + \" </s> \" + df.sentiment + \" </s>\"\n",
    "test_df[\"text_mod\"] = \"<s> \" + test_df.text.str.strip() + \" </s> \" + test_df.sentiment + \" </s>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../results/tokenizers/roberta_tokenizer/vocab.json',\n",
       " '../results/tokenizers/roberta_tokenizer/merges.txt',\n",
       " '../results/tokenizers/roberta_tokenizer/special_tokens_map.json',\n",
       " '../results/tokenizers/roberta_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"../results/tokenizers/roberta_tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=\"../results/tokenizers/roberta_tokenizer/vocab.json\",\n",
    "    merges_file=\"../results/tokenizers/roberta_tokenizer/merges.txt\",\n",
    "    add_prefix_space=True,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../results/tokenizers/roberta_tokenizer/special_tokens_map.json\") as f:\n",
    "    special_tokens = json.load(f)\n",
    "\n",
    "tokenizer.add_special_tokens([i for i in special_tokens.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens = tokenizer.encode_batch(df.text_mod.tolist())\n",
    "Y_tokens = tokenizer.encode_batch(df.selected_text.tolist())\n",
    "X_tokens_test = tokenizer.encode_batch(test_df.text_mod.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i.ids for i in X_tokens]\n",
    "Y = [i.ids for i in Y_tokens]\n",
    "X_test = [i.ids for i in X_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_att = [i.attention_mask for i in X_tokens]\n",
    "Y_att = [i.attention_mask for i in Y_tokens]\n",
    "X_att_test = [i.attention_mask for i in X_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265 106\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(i) for i in X])\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "print(VOCAB_SIZE, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremities(l_string, s_string, print_it=False):\n",
    "    len_l = len(l_string)\n",
    "    len_s = len(s_string)\n",
    "    \n",
    "    for i in range(len_l - len_s + 1):\n",
    "        if (i + len_s) <= len_l:\n",
    "            substring = l_string[i:i+len_s]\n",
    "            if substring == s_string:\n",
    "                if print_it:\n",
    "                    print(l_string)\n",
    "                    print(substring)\n",
    "                    print(i, i+len_s, substring)\n",
    "                \n",
    "                start_vector, end_vector = [0] * len_l, [0] * len_l\n",
    "                start_vector[i], end_vector[i+len_s-1] = 1, 1\n",
    "                \n",
    "                return (start_vector, end_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24856 \t: #Processed\n",
      "0 \t: # of Anomalies\n"
     ]
    }
   ],
   "source": [
    "Y_starts, Y_stops = [], []\n",
    "anomaly_idx, counter = [], 0\n",
    "for num, (i,j) in enumerate(zip(X_tokens, Y_tokens)):\n",
    "    x,y = i.ids, j.ids\n",
    "    #s,e = get_extremities(x, y)\n",
    "    try:\n",
    "        s,e = get_extremities(x, y)\n",
    "    except TypeError as t:\n",
    "        counter += 1\n",
    "        anomaly_idx.append(num)\n",
    "    Y_starts.append(s)\n",
    "    Y_stops.append(e)\n",
    "print(num + 1, \"\\t: #Processed\")\n",
    "\n",
    "print(counter,\"\\t: # of Anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if counter:\n",
    "    print(anomaly_idx, sep=\",\")\n",
    "    print(df[\"original_index\"].iloc[anomaly_idx].tolist())\n",
    "    df.iloc[anomaly_idx].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or even NOOOOO NOT THE SECRET NAMEREBECCA PLEASE\n",
      "NOT THE SECRET NAMEREBECCA PLEASE\n",
      "[['<s>', 0, 0, 0], ['Ġor', 50, 0, 0], ['Ġeven', 190, 0, 0], ['Ġno', 117, 0, 0], ['oooo', 40386, 0, 0], ['Ġnot', 45, 1, 0], ['Ġthe', 5, 0, 0], ['Ġsecret', 3556, 0, 0], ['Ġname', 766, 0, 0], ['re', 241, 0, 0], ['becca', 39746, 0, 0], ['Ġplease', 2540, 0, 1], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0], ['Ġnegative', 2430, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0]]\n",
      "[[45, 'Ġnot'], [5, 'Ġthe'], [3556, 'Ġsecret'], [766, 'Ġname'], [241, 're'], [39746, 'becca'], [2540, 'Ġplease']]\n"
     ]
    }
   ],
   "source": [
    "check_idx = 156\n",
    "print(df.text[check_idx])\n",
    "print(df.selected_text[check_idx])\n",
    "print([[i,j,k,l] for i,j,k,l in zip(X_tokens[check_idx].tokens,\n",
    "                                    X_tokens[check_idx].ids,\n",
    "                                    Y_starts[check_idx],\n",
    "                                    Y_stops[check_idx])])\n",
    "print([[i,j] for i,j in zip(Y_tokens[check_idx].ids,\n",
    "                            Y_tokens[check_idx].tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X, maxlen=max_len, padding=\"post\")\n",
    "X_att = pad_sequences(X_att, maxlen=max_len, padding=\"post\")\n",
    "Y = pad_sequences(Y, maxlen=max_len, padding=\"post\")\n",
    "Y_starts = pad_sequences(Y_starts, maxlen=max_len, padding=\"post\").argmax(axis=1)\n",
    "Y_stops = pad_sequences(Y_stops, maxlen=max_len, padding=\"post\").argmax(axis=1)\n",
    "\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding=\"post\")\n",
    "X_att_test = pad_sequences(X_att_test, maxlen=max_len, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_flag = np.isin(Y_stops, np.unique(Y_stops, return_counts=True)[0][np.unique(Y_stops, return_counts=True)[1]>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (24856, 106) \t: X  \n",
      " (24856, 106) \t: X_att  \n",
      " (24856, 106) \t: Y  \n",
      " (24856,) \t: Y_starts  \n",
      " (24856,) \t: Y_stops  \n",
      " (3534, 106) \t: X_test  \n",
      " (3534, 106) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X.shape, \"\\t: X \", \"\\n\",\n",
    "     X_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[keep_flag]\n",
    "X_att = X_att[keep_flag]\n",
    "Y = Y[keep_flag]\n",
    "Y_starts = Y_starts[keep_flag]\n",
    "Y_stops = Y_stops[keep_flag]\n",
    "X_test = X_test\n",
    "X_att_test = X_att_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (24851, 106) \t: X  \n",
      " (24851, 106) \t: X_att  \n",
      " (24851, 106) \t: Y  \n",
      " (24851,) \t: Y_starts  \n",
      " (24851,) \t: Y_stops  \n",
      " (3534, 106) \t: X_test  \n",
      " (3534, 106) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X.shape, \"\\t: X \", \"\\n\",\n",
    "     X_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=NUM_FOLDS, train_size=TRAIN_SPLIT_RATIO, random_state=0)\n",
    "sss.get_n_splits(X, Y_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_words = [tokenizer.decode(i) for i in Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=6a6L_9USZxg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(pred_dict):\n",
    "    print(\"[INFO] \",\"=\"*15,\"Validation for FOLD#\", num, \"=\"*15)\n",
    "    funcs = [accuracy_score, f1_score, precision_score, recall_score, confusion_matrix]\n",
    "    for f in funcs:\n",
    "        for data_set in [\"train\",\"valid\"]:\n",
    "            for var in [\"starts\", \"stops\"]:\n",
    "                if f in [accuracy_score]:\n",
    "                    res = f(**pred_dict[data_set][var])\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "                elif f in [confusion_matrix]:\n",
    "                    res = f(**pred_dict[data_set][var], labels = np.arange(max_len))\n",
    "                    np.savetxt(X=res, fmt='%i', delimiter=\",\",\n",
    "                               fname=\"../results/ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                    print(\"[INFO] \\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__, \"\\t||\", \n",
    "                          \"../results/ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                else:\n",
    "                    res = f(**pred_dict[data_set][var], average=\"macro\")\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "        print(\"================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(string):\n",
    "    string = re.sub(pattern=\" (negative|positive|neutral)[ ]+$\", repl=\"\", string=string)\n",
    "    string = re.sub(pattern=\" +\", repl=\" \", string=string.strip())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    a = set(str1)\n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_sequences = Input((max_len), dtype=tf.int32, name=\"words\")\n",
    "    input_att_flags = Input((max_len), dtype=tf.int32, name=\"att_flags\")\n",
    "    input_token_ids = Input((max_len), dtype=tf.int32, name=\"token_ids\")\n",
    "\n",
    "    config = RobertaConfig.from_pretrained('roberta-base')\n",
    "    roberta_model = TFRobertaModel.from_pretrained('roberta-base', config=config)\n",
    "    x = roberta_model(inputs=input_sequences, attention_mask=input_att_flags, token_type_ids=input_token_ids)\n",
    "\n",
    "    x_start = tf.keras.layers.Reshape((max_len, 768, 1))(x[0])\n",
    "    x_start = tf.keras.layers.Dropout(DROPOUT)(x_start)\n",
    "    x_start = tf.keras.layers.Conv2D(filters=1,kernel_size=(1,768), data_format=\"channels_last\")(x_start)\n",
    "    x_start = tf.keras.layers.Flatten()(x_start)\n",
    "    output_starts = tf.keras.layers.Dense(max_len, activation='softmax', name=\"starts\")(x_start)\n",
    "    \n",
    "    x_stop = tf.keras.layers.Reshape((max_len, 768, 1))(x[0])\n",
    "    x_stop = tf.keras.layers.Dropout(DROPOUT)(x_stop)\n",
    "    x_stop = tf.keras.layers.Conv2D(filters=1,kernel_size=(1,768), data_format=\"channels_last\")(x_stop)\n",
    "    x_stop = tf.keras.layers.Flatten()(x_stop)\n",
    "    output_stops = tf.keras.layers.Dense(max_len, activation='softmax', name=\"stops\")(x_stop)\n",
    "    \n",
    "    model = Model([input_att_flags, input_sequences, input_token_ids],\n",
    "                  [output_starts, output_stops])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 106)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 106)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 106)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 106, 768), ( 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 106, 768, 1)  0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 106, 768, 1)  0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 106, 768, 1)  0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 106, 768, 1)  0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 106, 1, 1)    769         dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 106, 1, 1)    769         dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 106)          0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 106)          0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "starts (Dense)                  (None, 106)          11342       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "stops (Dense)                   (None, 106)          11342       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 124,669,854\n",
      "Trainable params: 124,669,854\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ==================== FOLD# 0  ====================\n",
      "Train on 19880 samples, validate on 4971 samples\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "19880/19880 [==============================] - 558s 28ms/sample - loss: 4.9497 - starts_loss: 2.0712 - stops_loss: 2.8784 - starts_accuracy: 0.5772 - stops_accuracy: 0.3210 - val_loss: 3.3138 - val_starts_loss: 1.5545 - val_stops_loss: 1.7599 - val_starts_accuracy: 0.6218 - val_stops_accuracy: 0.5520\n",
      "Epoch 2/4\n",
      "19880/19880 [==============================] - 532s 27ms/sample - loss: 3.0007 - starts_loss: 1.4246 - stops_loss: 1.5760 - starts_accuracy: 0.6177 - stops_accuracy: 0.5889 - val_loss: 2.5665 - val_starts_loss: 1.2144 - val_stops_loss: 1.3522 - val_starts_accuracy: 0.6437 - val_stops_accuracy: 0.6562\n",
      "Epoch 3/4\n",
      "19880/19880 [==============================] - 533s 27ms/sample - loss: 2.4679 - starts_loss: 1.1936 - stops_loss: 1.2743 - starts_accuracy: 0.6472 - stops_accuracy: 0.6572 - val_loss: 2.3025 - val_starts_loss: 1.1313 - val_stops_loss: 1.1709 - val_starts_accuracy: 0.6775 - val_stops_accuracy: 0.6846\n",
      "Epoch 4/4\n",
      "19880/19880 [==============================] - 532s 27ms/sample - loss: 2.1388 - starts_loss: 1.0497 - stops_loss: 1.0893 - starts_accuracy: 0.6825 - stops_accuracy: 0.6993 - val_loss: 2.2008 - val_starts_loss: 1.0982 - val_stops_loss: 1.1025 - val_starts_accuracy: 0.6904 - val_stops_accuracy: 0.7091\n",
      "=============== Validation for FOLD# 0 ===============\n",
      "74.03 \t|| train \t|| starts \t|| accuracy_score\n",
      "77.90 \t|| train \t|| stops \t|| accuracy_score\n",
      "69.04 \t|| valid \t|| starts \t|| accuracy_score\n",
      "70.91 \t|| valid \t|| stops \t|| accuracy_score\n",
      "================================================================\n",
      "27.79 \t|| train \t|| starts \t|| f1_score\n",
      "71.65 \t|| train \t|| stops \t|| f1_score\n",
      "24.10 \t|| valid \t|| starts \t|| f1_score\n",
      "67.54 \t|| valid \t|| stops \t|| f1_score\n",
      "================================================================\n",
      "30.14 \t|| train \t|| starts \t|| precision_score\n",
      "74.13 \t|| train \t|| stops \t|| precision_score\n",
      "24.19 \t|| valid \t|| starts \t|| precision_score\n",
      "69.84 \t|| valid \t|| stops \t|| precision_score\n",
      "================================================================\n",
      "30.05 \t|| train \t|| starts \t|| recall_score\n",
      "70.07 \t|| train \t|| stops \t|| recall_score\n",
      "26.25 \t|| valid \t|| starts \t|| recall_score\n",
      "66.30 \t|| valid \t|| stops \t|| recall_score\n",
      "================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_train_starts.csv\n",
      "\t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_train_stops.csv\n",
      "\t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_valid_starts.csv\n",
      "\t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_valid_stops.csv\n",
      "================================================================\n",
      "[INFO] Training fold: 0 finished at Sat May 16 02:54:47 2020\n",
      "[INFO] ==================== FOLD# 1  ====================\n",
      "Train on 19880 samples, validate on 4971 samples\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "19880/19880 [==============================] - 552s 28ms/sample - loss: 4.9609 - starts_loss: 1.9570 - stops_loss: 3.0032 - starts_accuracy: 0.5921 - stops_accuracy: 0.2743 - val_loss: 3.3839 - val_starts_loss: 1.5222 - val_stops_loss: 1.8634 - val_starts_accuracy: 0.6166 - val_stops_accuracy: 0.5580\n",
      "Epoch 2/4\n",
      "19880/19880 [==============================] - 535s 27ms/sample - loss: 3.2043 - starts_loss: 1.4560 - stops_loss: 1.7483 - starts_accuracy: 0.6148 - stops_accuracy: 0.5440 - val_loss: 2.8757 - val_starts_loss: 1.3626 - val_stops_loss: 1.5141 - val_starts_accuracy: 0.6234 - val_stops_accuracy: 0.6292\n",
      "Epoch 3/4\n",
      "19880/19880 [==============================] - 534s 27ms/sample - loss: 2.7039 - starts_loss: 1.2859 - stops_loss: 1.4175 - starts_accuracy: 0.6388 - stops_accuracy: 0.6290 - val_loss: 2.5947 - val_starts_loss: 1.2264 - val_stops_loss: 1.3685 - val_starts_accuracy: 0.6504 - val_stops_accuracy: 0.6616\n",
      "Epoch 4/4\n",
      "19880/19880 [==============================] - 533s 27ms/sample - loss: 2.3847 - starts_loss: 1.1524 - stops_loss: 1.2327 - starts_accuracy: 0.6619 - stops_accuracy: 0.6694 - val_loss: 2.4989 - val_starts_loss: 1.1757 - val_stops_loss: 1.3232 - val_starts_accuracy: 0.6588 - val_stops_accuracy: 0.6743\n",
      "=============== Validation for FOLD# 1 ===============\n",
      "70.33 \t|| train \t|| starts \t|| accuracy_score\n",
      "74.29 \t|| train \t|| stops \t|| accuracy_score\n",
      "65.88 \t|| valid \t|| starts \t|| accuracy_score\n",
      "67.43 \t|| valid \t|| stops \t|| accuracy_score\n",
      "================================================================\n",
      "19.33 \t|| train \t|| starts \t|| f1_score\n",
      "67.78 \t|| train \t|| stops \t|| f1_score\n",
      "19.89 \t|| valid \t|| starts \t|| f1_score\n",
      "59.62 \t|| valid \t|| stops \t|| f1_score\n",
      "================================================================\n",
      "21.33 \t|| train \t|| starts \t|| precision_score\n",
      "70.13 \t|| train \t|| stops \t|| precision_score\n",
      "21.79 \t|| valid \t|| starts \t|| precision_score\n",
      "61.31 \t|| valid \t|| stops \t|| precision_score\n",
      "================================================================\n",
      "19.68 \t|| train \t|| starts \t|| recall_score\n",
      "67.73 \t|| train \t|| stops \t|| recall_score\n",
      "20.03 \t|| valid \t|| starts \t|| recall_score\n",
      "59.17 \t|| valid \t|| stops \t|| recall_score\n",
      "================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_train_starts.csv\n",
      "\t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_train_stops.csv\n",
      "\t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_valid_starts.csv\n",
      "\t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_valid_stops.csv\n",
      "================================================================\n",
      "[INFO] Training fold: 1 finished at Sat May 16 03:34:05 2020\n",
      "[INFO] ==================== FOLD# 2  ====================\n",
      "Train on 19880 samples, validate on 4971 samples\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "19880/19880 [==============================] - 556s 28ms/sample - loss: 5.4508 - starts_loss: 2.1640 - stops_loss: 3.2859 - starts_accuracy: 0.5644 - stops_accuracy: 0.2005 - val_loss: 3.6013 - val_starts_loss: 1.6137 - val_stops_loss: 1.9863 - val_starts_accuracy: 0.6148 - val_stops_accuracy: 0.4951\n",
      "Epoch 2/4\n",
      "19880/19880 [==============================] - 535s 27ms/sample - loss: 3.2984 - starts_loss: 1.4914 - stops_loss: 1.8074 - starts_accuracy: 0.6218 - stops_accuracy: 0.5461 - val_loss: 2.9786 - val_starts_loss: 1.3908 - val_stops_loss: 1.5859 - val_starts_accuracy: 0.6208 - val_stops_accuracy: 0.6126\n",
      "Epoch 3/4\n",
      "19880/19880 [==============================] - 535s 27ms/sample - loss: 2.7560 - starts_loss: 1.3152 - stops_loss: 1.4405 - starts_accuracy: 0.6373 - stops_accuracy: 0.6389 - val_loss: 2.6951 - val_starts_loss: 1.2733 - val_stops_loss: 1.4204 - val_starts_accuracy: 0.6401 - val_stops_accuracy: 0.6632\n",
      "Epoch 4/4\n",
      "19880/19880 [==============================] - 536s 27ms/sample - loss: 2.4337 - starts_loss: 1.1724 - stops_loss: 1.2611 - starts_accuracy: 0.6635 - stops_accuracy: 0.6797 - val_loss: 2.4811 - val_starts_loss: 1.1815 - val_stops_loss: 1.2982 - val_starts_accuracy: 0.6610 - val_stops_accuracy: 0.6763\n",
      "=============== Validation for FOLD# 2 ===============\n",
      "70.76 \t|| train \t|| starts \t|| accuracy_score\n",
      "75.16 \t|| train \t|| stops \t|| accuracy_score\n",
      "66.10 \t|| valid \t|| starts \t|| accuracy_score\n",
      "67.63 \t|| valid \t|| stops \t|| accuracy_score\n",
      "================================================================\n",
      "18.98 \t|| train \t|| starts \t|| f1_score\n",
      "60.69 \t|| train \t|| stops \t|| f1_score\n",
      "17.82 \t|| valid \t|| starts \t|| f1_score\n",
      "54.88 \t|| valid \t|| stops \t|| f1_score\n",
      "================================================================\n",
      "24.29 \t|| train \t|| starts \t|| precision_score\n",
      "63.51 \t|| train \t|| stops \t|| precision_score\n",
      "21.18 \t|| valid \t|| starts \t|| precision_score\n",
      "56.57 \t|| valid \t|| stops \t|| precision_score\n",
      "================================================================\n",
      "19.90 \t|| train \t|| starts \t|| recall_score\n",
      "59.76 \t|| train \t|| stops \t|| recall_score\n",
      "18.61 \t|| valid \t|| starts \t|| recall_score\n",
      "54.42 \t|| valid \t|| stops \t|| recall_score\n",
      "================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_train_starts.csv\n",
      "\t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_train_stops.csv\n",
      "\t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_valid_starts.csv\n",
      "\t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_valid_stops.csv\n",
      "================================================================\n",
      "[INFO] Training fold: 2 finished at Sat May 16 04:13:32 2020\n",
      "[INFO] ==================== FOLD# 3  ====================\n",
      "Train on 19880 samples, validate on 4971 samples\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "19880/19880 [==============================] - 553s 28ms/sample - loss: 5.4426 - starts_loss: 2.2174 - stops_loss: 3.2241 - starts_accuracy: 0.5528 - stops_accuracy: 0.2370 - val_loss: 3.3871 - val_starts_loss: 1.4962 - val_stops_loss: 1.8902 - val_starts_accuracy: 0.6280 - val_stops_accuracy: 0.5114\n",
      "Epoch 2/4\n",
      "19880/19880 [==============================] - 534s 27ms/sample - loss: 3.1930 - starts_loss: 1.4716 - stops_loss: 1.7217 - starts_accuracy: 0.6133 - stops_accuracy: 0.5579 - val_loss: 2.6841 - val_starts_loss: 1.2492 - val_stops_loss: 1.4347 - val_starts_accuracy: 0.6461 - val_stops_accuracy: 0.6441\n",
      "Epoch 3/4\n",
      "19880/19880 [==============================] - 536s 27ms/sample - loss: 2.6108 - starts_loss: 1.2559 - stops_loss: 1.3550 - starts_accuracy: 0.6418 - stops_accuracy: 0.6531 - val_loss: 2.3869 - val_starts_loss: 1.1058 - val_stops_loss: 1.2811 - val_starts_accuracy: 0.6884 - val_stops_accuracy: 0.6876\n",
      "Epoch 4/4\n",
      "19880/19880 [==============================] - 534s 27ms/sample - loss: 2.2180 - starts_loss: 1.0737 - stops_loss: 1.1439 - starts_accuracy: 0.6813 - stops_accuracy: 0.7056 - val_loss: 2.2939 - val_starts_loss: 1.0465 - val_stops_loss: 1.2478 - val_starts_accuracy: 0.7015 - val_stops_accuracy: 0.7067\n",
      "=============== Validation for FOLD# 3 ===============\n",
      "74.79 \t|| train \t|| starts \t|| accuracy_score\n",
      "78.27 \t|| train \t|| stops \t|| accuracy_score\n",
      "70.15 \t|| valid \t|| starts \t|| accuracy_score\n",
      "70.67 \t|| valid \t|| stops \t|| accuracy_score\n",
      "================================================================\n",
      "26.57 \t|| train \t|| starts \t|| f1_score\n",
      "67.71 \t|| train \t|| stops \t|| f1_score\n",
      "23.92 \t|| valid \t|| starts \t|| f1_score\n",
      "60.34 \t|| valid \t|| stops \t|| f1_score\n",
      "================================================================\n",
      "31.83 \t|| train \t|| starts \t|| precision_score\n",
      "70.79 \t|| train \t|| stops \t|| precision_score\n",
      "25.99 \t|| valid \t|| starts \t|| precision_score\n",
      "60.70 \t|| valid \t|| stops \t|| precision_score\n",
      "================================================================\n",
      "28.85 \t|| train \t|| starts \t|| recall_score\n",
      "67.18 \t|| train \t|| stops \t|| recall_score\n",
      "26.18 \t|| valid \t|| starts \t|| recall_score\n",
      "60.75 \t|| valid \t|| stops \t|| recall_score\n",
      "================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_train_starts.csv\n",
      "\t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_train_stops.csv\n",
      "\t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_valid_starts.csv\n",
      "\t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_valid_stops.csv\n",
      "================================================================\n",
      "[INFO] Training fold: 3 finished at Sat May 16 04:52:54 2020\n",
      "[INFO] ==================== FOLD# 4  ====================\n",
      "Train on 19880 samples, validate on 4971 samples\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "19880/19880 [==============================] - 552s 28ms/sample - loss: 5.0049 - starts_loss: 1.9827 - stops_loss: 3.0208 - starts_accuracy: 0.5882 - stops_accuracy: 0.2945 - val_loss: 3.4161 - val_starts_loss: 1.5834 - val_stops_loss: 1.8326 - val_starts_accuracy: 0.5955 - val_stops_accuracy: 0.5703\n",
      "Epoch 2/4\n",
      "19880/19880 [==============================] - 535s 27ms/sample - loss: 3.0791 - starts_loss: 1.4690 - stops_loss: 1.6103 - starts_accuracy: 0.6100 - stops_accuracy: 0.5854 - val_loss: 2.7978 - val_starts_loss: 1.3757 - val_stops_loss: 1.4221 - val_starts_accuracy: 0.6105 - val_stops_accuracy: 0.6286\n",
      "Epoch 3/4\n",
      "19880/19880 [==============================] - 534s 27ms/sample - loss: 2.6167 - starts_loss: 1.2880 - stops_loss: 1.3284 - starts_accuracy: 0.6322 - stops_accuracy: 0.6408 - val_loss: 2.5053 - val_starts_loss: 1.2332 - val_stops_loss: 1.2716 - val_starts_accuracy: 0.6431 - val_stops_accuracy: 0.6725\n",
      "Epoch 4/4\n",
      "19880/19880 [==============================] - 534s 27ms/sample - loss: 2.2912 - starts_loss: 1.1461 - stops_loss: 1.1450 - starts_accuracy: 0.6574 - stops_accuracy: 0.6834 - val_loss: 2.3980 - val_starts_loss: 1.1675 - val_stops_loss: 1.2303 - val_starts_accuracy: 0.6532 - val_stops_accuracy: 0.6876\n",
      "=============== Validation for FOLD# 4 ===============\n",
      "71.42 \t|| train \t|| starts \t|| accuracy_score\n",
      "76.02 \t|| train \t|| stops \t|| accuracy_score\n",
      "65.32 \t|| valid \t|| starts \t|| accuracy_score\n",
      "68.76 \t|| valid \t|| stops \t|| accuracy_score\n",
      "================================================================\n",
      "20.66 \t|| train \t|| starts \t|| f1_score\n",
      "69.85 \t|| train \t|| stops \t|| f1_score\n",
      "16.66 \t|| valid \t|| starts \t|| f1_score\n",
      "64.05 \t|| valid \t|| stops \t|| f1_score\n",
      "================================================================\n",
      "22.91 \t|| train \t|| starts \t|| precision_score\n",
      "72.64 \t|| train \t|| stops \t|| precision_score\n",
      "16.50 \t|| valid \t|| starts \t|| precision_score\n",
      "65.38 \t|| valid \t|| stops \t|| precision_score\n",
      "================================================================\n",
      "22.30 \t|| train \t|| starts \t|| recall_score\n",
      "69.81 \t|| train \t|| stops \t|| recall_score\n",
      "18.26 \t|| valid \t|| starts \t|| recall_score\n",
      "64.21 \t|| valid \t|| stops \t|| recall_score\n",
      "================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_train_starts.csv\n",
      "\t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_train_stops.csv\n",
      "\t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_valid_starts.csv\n",
      "\t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V19_valid_stops.csv\n",
      "================================================================\n",
      "[INFO] Training fold: 4 finished at Sat May 16 05:32:15 2020\n",
      "Sat May 16 05:32:15 2020\n"
     ]
    }
   ],
   "source": [
    "for num, (t_index, v_index) in enumerate(sss.split(X, Y_stops)):\n",
    "    print(\"[INFO] ==================== FOLD#\", num, \" ====================\")\n",
    "    \n",
    "    if num > 0:\n",
    "        del history\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "        \n",
    "    model = build_model()\n",
    "    \n",
    "    adam = Adam(learning_rate=MIN_LR)\n",
    "    \n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=adam , metrics=['accuracy'])\n",
    "    \n",
    "    mcp = ModelCheckpoint(filepath=\"../results/\"+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\", monitor='val_loss',\n",
    "                          verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "    \n",
    "    history = model.fit(x={\"att_flags\":X_att[t_index],\n",
    "                           \"words\":X[t_index],\n",
    "                           \"token_ids\": np.zeros_like(X_att[t_index])},\n",
    "                        y={\"starts\":Y_starts[t_index],\n",
    "                           \"stops\":Y_stops[t_index]},\n",
    "                        shuffle=True,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        validation_data=({\"att_flags\":X_att[v_index],\n",
    "                                          \"words\":X[v_index],\n",
    "                                          \"token_ids\":np.zeros_like(X_att[v_index])},\n",
    "                                         {\"starts\":Y_starts[v_index],\n",
    "                                          \"stops\":Y_stops[v_index]}),\n",
    "                        verbose=1,\n",
    "                        callbacks=[mcp])\n",
    "    \n",
    "    model = build_model()\n",
    "    model.load_weights(\"../results/\"+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\")  \n",
    "    \n",
    "    pred_train = model.predict(x = {\"att_flags\":X_att[t_index],\n",
    "                                    \"words\":X[t_index],\n",
    "                                    \"token_ids\":np.zeros_like(X_att[t_index])},\n",
    "                               batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "    pred_val = model.predict(x = {\"att_flags\":X_att[v_index],\n",
    "                                  \"words\":X[v_index],\n",
    "                                  \"token_ids\":np.zeros_like(X_att[v_index])},\n",
    "                             batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    pred_starts_train, pred_stops_train = pred_train[0], pred_train[1]\n",
    "    pred_starts_val, pred_stops_val = pred_val[0], pred_val[1]\n",
    "    \n",
    "    #Accumulate test results for every fold\n",
    "    if num==0:\n",
    "        pred_test = model.predict(x = {\"att_flags\":X_att_test,\n",
    "                                       \"words\":X_test,\n",
    "                                       \"token_ids\":np.zeros_like(X_att_test)},\n",
    "                                  batch_size=PREDICT_BATCH_SIZE)\n",
    "    else:\n",
    "        pred_test_fold = model.predict(x = {\"att_flags\":X_att_test,\n",
    "                                             \"words\":X_test,\n",
    "                                             \"token_ids\":np.zeros_like(X_att_test)},\n",
    "                                       batch_size=PREDICT_BATCH_SIZE)\n",
    "        pred_test[0] += pred_test_fold[0]\n",
    "        pred_test[1] += pred_test_fold[1]\n",
    "    \n",
    "    # Tabulate\n",
    "    preds = {\n",
    "        \"train\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_starts[t_index],\n",
    "                \"y_pred\":pred_train[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_stops[t_index],\n",
    "                \"y_pred\":pred_train[1].argmax(axis=1)\n",
    "            }\n",
    "        },\n",
    "        \"valid\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_starts[v_index],\n",
    "                \"y_pred\":pred_val[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_stops[v_index],\n",
    "                \"y_pred\":pred_val[1].argmax(axis=1)\n",
    "            }        \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print_metrics(pred_dict=preds)\n",
    "\n",
    "    print(\"[INFO] Prediction shape for training data: \", pred_starts_train.shape, pred_stops_train.shape)\n",
    "    print(\"[INFO] Prediction shape for validation data: \", pred_starts_val.shape, pred_stops_val.shape)\n",
    "\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for training data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_train.argmax(axis=1),\n",
    "                                  pred_stops_train.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_train.shape[0])\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for validation data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_val.argmax(axis=1),\n",
    "                                  pred_stops_val.argmax(axis=1))]), \n",
    "          \"out of\", pred_starts_val.shape[0],)\n",
    "\n",
    "    pred_words_train = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X[t_index],\n",
    "                                                                                                                                   pred_starts_train.argmax(axis=1),\n",
    "                                                                                                                                   pred_stops_train.argmax(axis=1))]\n",
    "    pred_words_val = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X[v_index],\n",
    "                                                                                                                                 pred_starts_val.argmax(axis=1),\n",
    "                                                                                                                                 pred_stops_val.argmax(axis=1))]\n",
    "\n",
    "    print(\"[INFO] Training Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip(Y_words[t_index],\n",
    "                                                          pred_words_train)]))\n",
    "    print(\"[INFO] Validation Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip(Y_words[v_index],\n",
    "                                                          pred_words_val)]))\n",
    "    print(\"[INFO] Training for fold:\", num, \"finished at\", ctime(time()))\n",
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3534, 106) (3534, 106)\n"
     ]
    }
   ],
   "source": [
    "pred_starts_test, pred_stops_test = pred_test[0]/5, pred_test[1]/5\n",
    "print(pred_starts_test.shape, pred_stops_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal predictions (StartIndex less than EndIndex) for testing data: 2771 out of 3534\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal predictions (StartIndex less than EndIndex) for testing data:\",\n",
    "      sum([s<e for s,e in zip(pred_starts_test.argmax(axis=1),\n",
    "                              pred_stops_test.argmax(axis=1))]), \n",
    "      \"out of\",\n",
    "      pred_starts_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words_test = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t,s,e in zip(X_test,\n",
    "                                                                                                                             pred_starts_test.argmax(axis=1),\n",
    "                                                                                                                             pred_stops_test.argmax(axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it`s surprising how much billy idol turns up in tweets. i monitor constantly - not that i`m obsessed or anything!  #billyidol  neutral \n",
      "1\n",
      "33\n",
      "it`s surprising how much billy idol turns up in tweets. i monitor constantly - not that i`m obsessed or anything! #billyidol\n"
     ]
    }
   ],
   "source": [
    "check_idx = 963\n",
    "#print([[t,i,j,k] for t,i,j,k in zip(tokenizer.decode(),X_test[check_idx],pred_starts_test[check_idx],pred_stops_test[check_idx])])\n",
    "print(tokenizer.decode(X_test[check_idx]))\n",
    "print(pred_starts_test.argmax(axis=1)[check_idx])\n",
    "print(pred_stops_test.argmax(axis=1)[check_idx])\n",
    "print(post_process(tokenizer.decode(X_test[check_idx][pred_starts_test.argmax(axis=1)[check_idx]:1+pred_stops_test.argmax(axis=1)[check_idx]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['selected_text'] = pred_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_df.loc[test_df.sentiment == \"neutral\"][\"selected_text\"] = test_df.loc[test_df.sentiment == \"neutral\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>I want to see David cook!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>i want to see david cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>We didn`t...but we did have to wait a good twenty minutes for one.  Good night, though!</td>\n",
       "      <td>positive</td>\n",
       "      <td>we didn`t...but we did have to wait a good twenty minutes for one. good night, though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>Feeling very tierd! too much college work!!!!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>feeling very tierd! too much college work!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>im following you! im following you! im sorry!  ahhahah i missed you!</td>\n",
       "      <td>negative</td>\n",
       "      <td>sorry! ahhahah i missed you!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>Same here - events + writing songs. But I just finished my TMNT vs MMPR song, so I`m excited to record that on Wednesday</td>\n",
       "      <td>positive</td>\n",
       "      <td>i`m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>This month was a bad month to try and get an advert together for Kobold Quarterly</td>\n",
       "      <td>negative</td>\n",
       "      <td>bad month to try and get an advert together for kobold quarterly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>together</td>\n",
       "      <td>neutral</td>\n",
       "      <td>together</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Oh god, a moth was living in a **** power outlet!  (Actually, my PowerSquid.)</td>\n",
       "      <td>negative</td>\n",
       "      <td>**** power outlet! (actually, my powersquid.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2424</th>\n",
       "      <td>I am soooo disapoointed I couldn`t make it  I will be there next time! Glad it was productive.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i am soooo disapoointed i couldn`t make it i will be there next time! glad it was productive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>cool np. It`s got me working on a series of designs.</td>\n",
       "      <td>positive</td>\n",
       "      <td>cool np. it`s got me working on a series of designs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>I`m having lunch already  ur a lil late buddy!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i`m having lunch already ur a lil late buddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>Will do</td>\n",
       "      <td>neutral</td>\n",
       "      <td>will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>MUAHAHAHHAHAHA...   well, maybe they think I`m crazy or something. I haven`t scared  yet though</td>\n",
       "      <td>positive</td>\n",
       "      <td>muahahahhahaha... well, maybe they think i`m crazy or something. i haven`t scared yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>so sad i have to pay $60</td>\n",
       "      <td>negative</td>\n",
       "      <td>sad i have to pay $60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3041</th>\n",
       "      <td>@ Cleaning the house! going out at 3 to see a soccergame with friends</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@ cleaning the house! going out at 3 to see a soccergame with</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           text  \\\n",
       "105                                                                                                  I want to see David cook!!   \n",
       "906                                     We didn`t...but we did have to wait a good twenty minutes for one.  Good night, though!   \n",
       "1339                                                                            Feeling very tierd! too much college work!!!!!!   \n",
       "2292                                                       im following you! im following you! im sorry!  ahhahah i missed you!   \n",
       "1120   Same here - events + writing songs. But I just finished my TMNT vs MMPR song, so I`m excited to record that on Wednesday   \n",
       "1257                                          This month was a bad month to try and get an advert together for Kobold Quarterly   \n",
       "1200                                                                                                                   together   \n",
       "380                                               Oh god, a moth was living in a **** power outlet!  (Actually, my PowerSquid.)   \n",
       "2424                             I am soooo disapoointed I couldn`t make it  I will be there next time! Glad it was productive.   \n",
       "2988                                                                       cool np. It`s got me working on a series of designs.   \n",
       "1324                                                                             I`m having lunch already  ur a lil late buddy!   \n",
       "1228                                                                                                                    Will do   \n",
       "524                             MUAHAHAHHAHAHA...   well, maybe they think I`m crazy or something. I haven`t scared  yet though   \n",
       "1951                                                                                                   so sad i have to pay $60   \n",
       "3041                                                      @ Cleaning the house! going out at 3 to see a soccergame with friends   \n",
       "\n",
       "     sentiment  \\\n",
       "105   positive   \n",
       "906   positive   \n",
       "1339  negative   \n",
       "2292  negative   \n",
       "1120  positive   \n",
       "1257  negative   \n",
       "1200   neutral   \n",
       "380   negative   \n",
       "2424   neutral   \n",
       "2988  positive   \n",
       "1324   neutral   \n",
       "1228   neutral   \n",
       "524   positive   \n",
       "1951  negative   \n",
       "3041   neutral   \n",
       "\n",
       "                                                                                     selected_text  \n",
       "105                                                                       i want to see david cook  \n",
       "906          we didn`t...but we did have to wait a good twenty minutes for one. good night, though  \n",
       "1339                                                 feeling very tierd! too much college work!!!!  \n",
       "2292                                                                  sorry! ahhahah i missed you!  \n",
       "1120                                                                                           i`m  \n",
       "1257                              bad month to try and get an advert together for kobold quarterly  \n",
       "1200                                                                                      together  \n",
       "380                                                  **** power outlet! (actually, my powersquid.)  \n",
       "2424  i am soooo disapoointed i couldn`t make it i will be there next time! glad it was productive  \n",
       "2988                                          cool np. it`s got me working on a series of designs.  \n",
       "1324                                                  i`m having lunch already ur a lil late buddy  \n",
       "1228                                                                                          will  \n",
       "524          muahahahhahaha... well, maybe they think i`m crazy or something. i haven`t scared yet  \n",
       "1951                                                                         sad i have to pay $60  \n",
       "3041                                 @ cleaning the house! going out at 3 to see a soccergame with  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[[\"text\", \"sentiment\",\"selected_text\"]].sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[[\"textID\", \"selected_text\"]].to_csv(\"../results/submission_\"+MODEL_PREFIX+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 16 11:18:43 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
