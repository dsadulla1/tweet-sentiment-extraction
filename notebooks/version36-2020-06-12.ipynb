{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V36\" # Better start stop indices No removal of samples, Using my tokenizer, Roberta weights, Sentiment(LeakyKFold), SpanNoNeutral(NoLeakKFlod) With Label Smoothing\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 16\n",
    "PREDICT_BATCH_SIZE = 512\n",
    "DROPOUT = 0.3\n",
    "LABEL_SMOOTHING_PARAM = 0.2\n",
    "\n",
    "RUN_ON_SAMPLE = True\n",
    "EXCLUDE_NEUTRAL_CLASS = True\n",
    "SENTIMENT_MAX_LR = 5e-4\n",
    "SENTIMENT_MID_LR = 5e-5\n",
    "SENTIMENT_MIN_LR = 5e-6\n",
    "SENTIMENT_NUM_EPOCHS = [4, 4, 2]\n",
    "MAX_LR = 5e-4 #5e-3 #3e-5\n",
    "MID_LR = 5e-5 #1e-4 #3e-5\n",
    "MIN_LR = 5e-6 #1e-6 #3e-5\n",
    "NUM_EPOCHS = [4, 4, 1]\n",
    "NUM_FOLDS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"../results/\"\n",
    "DATA_DIR = \"../data/\"\n",
    "MODEL_DIR = \"../data/models/roberta-base/\"\n",
    "EXT_MODEL_DIR = \"../data/models/roberta-tokenizer/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, Layer, LeakyReLU, Subtract\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    a = set(str1)\n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeded_value = 6666\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 12 21:19:40 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0', '2.8.0', '0.5.2']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "if not os.path.exists(MODEL_DIR+\"tokenizers\"):\n",
    "    os.mkdir(MODEL_DIR+\"tokenizers\")\n",
    "\n",
    "if not os.path.exists(MODEL_DIR+\"tokenizers/roberta_tokenizer\"):\n",
    "    os.mkdir(MODEL_DIR+\"tokenizers/roberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\"  target=\"_blank\"><h2 id=\"limiting_gpu_memory_growth\" data-text=\"Limiting GPU memory growth\" tabindex=\"0\">Limiting GPU memory growth</h2></a>\n",
    "<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\"><code translate=\"no\" dir=\"ltr\">CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the <code translate=\"no\" dir=\"ltr\">tf.config.experimental.set_visible_devices</code> method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it okay to exclude neutral text from models and force it later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVERAGE: 0.5334592552617378\n",
      "MEAN JACCARD: 0.9808316180492903\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>c6217bd9cc</td>\n",
       "      <td>Good morning! Aww, sorry that you were stuck in the airport for 12 hours!!</td>\n",
       "      <td>Good morning! Aww, sorry that you were stuck in the airport for 12 hours!!</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>c9a3c66dc8</td>\n",
       "      <td>Wow, just saw your Tweet about the Proflowers fiasco. That`s so not fun!</td>\n",
       "      <td>Wow, just saw your Tweet about the Proflowers fiasco. That`s so not fun!</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>737f01fd34</td>\n",
       "      <td>Dimples was in the preview... are they not on today?  It hasn`t aired here yet... via http://twib.es/CPF</td>\n",
       "      <td>Dimples was in the preview... are they not on today?  It hasn`t aired here yet..</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>f8e02e629d</td>\n",
       "      <td>yessssss wore myself out this weekend planting my garden and working</td>\n",
       "      <td>yessssss wore myself out this weekend planting my garden and working</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9169</th>\n",
       "      <td>56114801d1</td>\n",
       "      <td>i want candy!!!</td>\n",
       "      <td>i want candy!!</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID  \\\n",
       "5449  c6217bd9cc   \n",
       "2004  c9a3c66dc8   \n",
       "5540  737f01fd34   \n",
       "1315  f8e02e629d   \n",
       "9169  56114801d1   \n",
       "\n",
       "                                                                                                          text  \\\n",
       "5449                                Good morning! Aww, sorry that you were stuck in the airport for 12 hours!!   \n",
       "2004                                  Wow, just saw your Tweet about the Proflowers fiasco. That`s so not fun!   \n",
       "5540  Dimples was in the preview... are they not on today?  It hasn`t aired here yet... via http://twib.es/CPF   \n",
       "1315                                      yessssss wore myself out this weekend planting my garden and working   \n",
       "9169                                                                                           i want candy!!!   \n",
       "\n",
       "                                                                         selected_text  \\\n",
       "5449        Good morning! Aww, sorry that you were stuck in the airport for 12 hours!!   \n",
       "2004          Wow, just saw your Tweet about the Proflowers fiasco. That`s so not fun!   \n",
       "5540  Dimples was in the preview... are they not on today?  It hasn`t aired here yet..   \n",
       "1315              yessssss wore myself out this weekend planting my garden and working   \n",
       "9169                                                                    i want candy!!   \n",
       "\n",
       "     sentiment  \n",
       "5449   neutral  \n",
       "2004   neutral  \n",
       "5540   neutral  \n",
       "1315   neutral  \n",
       "9169   neutral  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\")\n",
    "# cases where neutral's text and selected text columns are the same\n",
    "neutrals = df2.loc[df2.sentiment==\"neutral\"].copy()\n",
    "neutrals = neutrals.reset_index(drop=True)\n",
    "neutrals[\"text\"] = neutrals[\"text\"].astype(str)\n",
    "neutrals[\"selected_text\"] = neutrals[\"selected_text\"].astype(str)\n",
    "print(\"COVERAGE:\", np.sum(np.where((neutrals.text == neutrals.selected_text), 1, 0))/neutrals.shape[0])\n",
    "print(\"MEAN JACCARD:\", np.mean([jaccard(str1=i, str2=j) for i,j in zip(neutrals.selected_text,neutrals.text)]))\n",
    "\n",
    "neutrals.loc[neutrals.text != neutrals.selected_text].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  1\n",
      "selected_text  object  1\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27480, 'selected_text': 22463, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count        27481   \n",
      "unique       27481   \n",
      "top     47753d6abc   \n",
      "freq             1   \n",
      "\n",
      "                                                                                                           text  \\\n",
      "count                                                                                                     27480   \n",
      "unique                                                                                                    27480   \n",
      "top       do you know if anyone from the believers never die tour is going on warped? i know  i can`t wait haha   \n",
      "freq                                                                                                          1   \n",
      "\n",
      "       selected_text sentiment  \n",
      "count          27480     27481  \n",
      "unique         22463         3  \n",
      "top             good   neutral  \n",
      "freq             199     11118  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\")\n",
    "\n",
    "print(pd.concat((df.dtypes, df.isna().sum()), axis=1))\n",
    "print(df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df[i].nunique() for i in df.columns})\n",
    "print(df.describe())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID                  text sentiment\n",
      "count         3534                  3534      3534\n",
      "unique        3534                  3534         3\n",
      "top     32e183bbad  I wanna feel my chin   neutral\n",
      "freq             1                     1      1430\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(DATA_DIR+\"test.csv\", encoding=\"utf8\")\n",
    "print(pd.concat((test_df.dtypes, test_df.isna().sum()), axis=1))\n",
    "print(test_df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df[i].nunique() for i in test_df.columns})\n",
    "print(test_df.describe())\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>c77717b103</td>\n",
       "      <td>I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½</td>\n",
       "      <td>I love to!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>28dbada620</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "44   c77717b103   \n",
       "192  28dbada620   \n",
       "\n",
       "                                                                                                       text  \\\n",
       "44    I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½   \n",
       "192                                *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "                                                                selected_text  \\\n",
       "44                                                                 I love to!   \n",
       "192  *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "    sentiment  \n",
       "44   positive  \n",
       "192   neutral  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>7223fdccc2</td>\n",
       "      <td>tikcets are only ï¿½91...each...BUT I SO WANT TO GO</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>43ad351369</td>\n",
       "      <td>AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "145  7223fdccc2   \n",
       "618  43ad351369   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "145                                                                                             tikcets are only ï¿½91...each...BUT I SO WANT TO GO   \n",
       "618  AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s   \n",
       "\n",
       "    sentiment  \n",
       "145  positive  \n",
       "618  positive  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[test_df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"set\"], test_df[\"set\"] = \"train\", \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create smaller sample for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_ON_SAMPLE:\n",
    "    df = df.sample(1000).reset_index(drop=True)\n",
    "    test_df = test_df.sample(1000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine datasets for pretraining using sentiment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 3)\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat((df[[\"text\",\"set\",\"sentiment\"]],\n",
    "                  test_df[[\"text\",\"set\",\"sentiment\"]]), axis=0)\n",
    "data[\"text\"] = data[\"text\"].astype(str)\n",
    "data = data.sample(frac=1.0).reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment count in combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">test</th>\n",
       "      <th>negative</th>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">train</th>\n",
       "      <th>negative</th>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 text\n",
       "set   sentiment      \n",
       "test  negative    294\n",
       "      neutral     398\n",
       "      positive    308\n",
       "train negative    297\n",
       "      neutral     403\n",
       "      positive    300"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"set\",\"sentiment\"])[[\"text\"]].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file=EXT_MODEL_DIR+'/vocab.json',\n",
    "                                             merges_file=EXT_MODEL_DIR+'/merges.txt',                                         \n",
    "                                             add_prefix_space=True,\n",
    "                                             lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(EXT_MODEL_DIR+\"/special_tokens_map.json\") as f:\n",
    "    special_tokens = json.load(f)\n",
    "\n",
    "tokenizer.add_special_tokens([i for i in special_tokens.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for sentiment detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_addspace(text:str) -> str:\n",
    "    text = text.lower()\n",
    "    text = \" \" + text.strip(\" \") + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text_mod\"] = data.apply(lambda x: trim_addspace(x.text), axis=1)\n",
    "data[\"text_mod\"] = \"<s>\"  + data[\"text_mod\"] + \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lookup = {\"positive\":2,\"neutral\":1,\"negative\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': (2000, 49), 'X_att': (2000, 49), 'Y': (2000,), 'VOCAB_SIZE': 50265, 'MAX_SEQ_LEN': 49}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentiment(text_series=data.text_mod.tolist(), sentiment_series=data.sentiment):\n",
    "\n",
    "    X_tokens = tokenizer.encode_batch(text_series)\n",
    "\n",
    "    X = [i.ids for i in X_tokens]\n",
    "    MAX_SEQ_LEN = max([len(i) for i in X])\n",
    "    X = pad_sequences(X, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "    X_att = [i.attention_mask for i in X_tokens]\n",
    "    X_att = pad_sequences(X_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "    Y = sentiment_series.apply(lambda x: sentiment_lookup[x]).values\n",
    "\n",
    "    VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "\n",
    "    print({\n",
    "        \"X\":X.shape,\n",
    "        \"X_att\":X_att.shape,\n",
    "        \"Y\":Y.shape,\n",
    "        \"VOCAB_SIZE\":VOCAB_SIZE,\n",
    "        \"MAX_SEQ_LEN\":MAX_SEQ_LEN\n",
    "    })\n",
    "    \n",
    "    return X_tokens, X, X_att, Y, VOCAB_SIZE, MAX_SEQ_LEN\n",
    "\n",
    "X_sent_tokens, X_sent, X_sent_att, Y_sent, VOCAB_SIZE, MAX_SEQ_LEN_SENT = preprocess_sentiment(**{\n",
    "    \"text_series\" : data.text.tolist(),\n",
    "    \"sentiment_series\" : data.sentiment\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  0\n",
      "selected_text  object  0\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27481, 'selected_text': 22464, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count        27481   \n",
      "unique       27481   \n",
      "top     47753d6abc   \n",
      "freq             1   \n",
      "\n",
      "                                                                                                           text  \\\n",
      "count                                                                                                     27481   \n",
      "unique                                                                                                    27481   \n",
      "top       do you know if anyone from the believers never die tour is going on warped? i know  i can`t wait haha   \n",
      "freq                                                                                                          1   \n",
      "\n",
      "       selected_text sentiment  \n",
      "count          27481     27481  \n",
      "unique         22464         3  \n",
      "top             good   neutral  \n",
      "freq             199     11118  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\").fillna('')\n",
    "\n",
    "print(pd.concat((df_span.dtypes, df_span.isna().sum()), axis=1))\n",
    "print(df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df_span[i].nunique() for i in df_span.columns})\n",
    "print(df_span.describe())\n",
    "df_span.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID                  text sentiment\n",
      "count         3534                  3534      3534\n",
      "unique        3534                  3534         3\n",
      "top     32e183bbad  I wanna feel my chin   neutral\n",
      "freq             1                     1      1430\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_span = pd.read_csv(DATA_DIR+\"test.csv\", encoding=\"utf8\").fillna('')\n",
    "print(pd.concat((test_df_span.dtypes, test_df_span.isna().sum()), axis=1))\n",
    "print(test_df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df_span[i].nunique() for i in test_df_span.columns})\n",
    "print(test_df_span.describe())\n",
    "test_df_span.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text\"] = df_span[\"text\"].astype(str)\n",
    "df_span[\"selected_text\"] = df_span[\"selected_text\"].astype(str)\n",
    "test_df_span[\"text\"] = test_df_span[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_addspace(text:str) -> str:\n",
    "    text = text.lower()\n",
    "    text = \" \" + text.strip(\" \") + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(text:str, selected_text:str) -> (str, str, int, int):\n",
    "    \n",
    "    text, selected_text = text.lower(), selected_text.lower()\n",
    "    \n",
    "    text = trim_addspace(text)\n",
    "    \n",
    "    substring_ = re.findall(pattern=\"\\\\s[^\\s]*?\"+re.escape(selected_text)+\"[^\\s]*?\\\\s\", string=text)[0]\n",
    "    \n",
    "    return pd.Series([text, \" \"+substring_.strip(\" \"), text.find(substring_), len(substring_) + text.find(substring_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[[\"text_mod\", \"selected_text_mod\", \"start\", \"stop\"]] = df_span[['text','selected_text']].apply(lambda x: find_indices(x.text, x.selected_text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'textID': '4eac33d1c0',\n",
       " 'text': ' wish we could come see u on Denver  husband lost his job and can`t afford it',\n",
       " 'selected_text': 'd lost',\n",
       " 'sentiment': 'negative',\n",
       " 'text_mod': ' wish we could come see u on denver  husband lost his job and can`t afford it ',\n",
       " 'selected_text_mod': ' husband lost',\n",
       " 'start': 36,\n",
       " 'stop': 50}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span.iloc[27476].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span['text_mod'] = test_df_span['text'].apply(trim_addspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'textID': {12154: 'adfbcc6806'},\n",
       " 'text': {12154: 'i wanna see `up` tonight, but no one will go with me. whhhyyy'},\n",
       " 'selected_text': {12154: 'but no one will go with me.'},\n",
       " 'sentiment': {12154: 'negative'},\n",
       " 'text_mod': {12154: ' i wanna see `up` tonight, but no one will go with me. whhhyyy '},\n",
       " 'selected_text_mod': {12154: ' but no one will go with me.'},\n",
       " 'start': {12154: 26},\n",
       " 'stop': {12154: 55}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span.loc[df_span.text_mod.str.contains(\"tonight, but no one will go\")].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"sentiment_code\"] = df_span[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments = df_span[\"sentiment_code\"].cat.codes.values\n",
    "\n",
    "test_df_span[\"sentiment_code\"] = test_df_span[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments_test = test_df_span[\"sentiment_code\"].cat.codes.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neutral': [7974], 'negative': [2430], 'positive': [1313]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{t:tokenizer.encode(\" \"+t).ids for t in df_span.sentiment.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text_mod\"] = \"<s>\" + df_span['text_mod'] + \"</s> </s> \" + df_span.sentiment + \" </s>\"\n",
    "test_df_span[\"text_mod\"] = \"<s>\" + test_df_span['text_mod'] + \"</s> </s> \" + test_df_span.sentiment + \" </s>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclusions for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCLUDE_NEUTRAL_CLASS: (16363, 9)\n",
      "Train RUN_ON_SAMPLE (2000, 9)\n",
      "Test  RUN_ON_SAMPLE (2000, 5)\n"
     ]
    }
   ],
   "source": [
    "if EXCLUDE_NEUTRAL_CLASS:\n",
    "    df_span = df_span.loc[df_span.sentiment!=\"neutral\"].copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"EXCLUDE_NEUTRAL_CLASS:\", df_span.shape)\n",
    "\n",
    "\n",
    "if RUN_ON_SAMPLE:\n",
    "    df_span = df_span.sample(2000).copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"Train RUN_ON_SAMPLE\", df_span.shape)\n",
    "    test_df_span = test_df_span.sample(2000).copy()\n",
    "    test_df_span = test_df_span.reset_index(drop=True)\n",
    "    print(\"Test  RUN_ON_SAMPLE\", test_df_span.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_tokens = tokenizer.encode_batch(df_span.text_mod.tolist())\n",
    "Y_span_tokens = tokenizer.encode_batch(df_span.selected_text_mod.tolist())\n",
    "X_span_tokens_test = tokenizer.encode_batch(test_df_span.text_mod.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = [i.ids for i in X_span_tokens]\n",
    "Y_span = [i.ids for i in Y_span_tokens]\n",
    "X_span_test = [i.ids for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_att = [i.attention_mask for i in X_span_tokens]\n",
    "Y_span_att = [i.attention_mask for i in Y_span_tokens] # Useless\n",
    "X_span_att_test = [i.attention_mask for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN_SPAN = max([len(i) for i in X_span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremities(l_string, s_string, print_it=False):\n",
    "    len_l = len(l_string)\n",
    "    len_s = len(s_string)\n",
    "    \n",
    "    for i in range(len_l - len_s + 1):\n",
    "        if (i + len_s) <= len_l:\n",
    "            substring = l_string[i:i+len_s]\n",
    "            if substring == s_string:\n",
    "                if print_it:\n",
    "                    print(l_string)\n",
    "                    print(substring)\n",
    "                    print(i, i+len_s, substring)\n",
    "                \n",
    "                start_vector, end_vector = [0] * len_l, [0] * len_l\n",
    "                start_vector[i], end_vector[i+len_s-1] = 1, 1\n",
    "                \n",
    "                return (start_vector, end_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 \t: #Processed\n",
      "0 \t: # of Anomalies\n"
     ]
    }
   ],
   "source": [
    "Y_span_starts, Y_span_stops = [], []\n",
    "anomaly_idx, counter = [], 0\n",
    "for num, (i,j) in enumerate(zip(X_span_tokens, Y_span_tokens)):\n",
    "    x,y = i.ids, j.ids\n",
    "    try:\n",
    "        s,e = get_extremities(x, y)\n",
    "        Y_span_starts.append(s)\n",
    "        Y_span_stops.append(e)\n",
    "    except TypeError as t:\n",
    "        counter += 1\n",
    "        anomaly_idx.append(num)\n",
    "        Y_span_starts.append([0]*15)\n",
    "        Y_span_stops.append([0]*15)\n",
    "print(num + 1, \"\\t: #Processed\")\n",
    "\n",
    "print(counter,\"\\t: # of Anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I haven`t had a good homemade flour tortilla in ages.\n",
      "haven`t had a good\n",
      "[['<s>', 0, 0, 0], ['Ġi', 939, 0, 0], ['Ġhaven', 2220, 1, 0], ['`', 12905, 0, 0], ['t', 90, 0, 0], ['Ġhad', 56, 0, 0], ['Ġa', 10, 0, 0], ['Ġgood', 205, 0, 1], ['Ġhomemade', 17798, 0, 0], ['Ġflour', 15039, 0, 0], ['Ġtort', 17082, 0, 0], ['illa', 4699, 0, 0], ['Ġin', 11, 0, 0], ['Ġages', 4864, 0, 0], ['.', 4, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0], ['Ġnegative', 2430, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0]]\n",
      "[[2220, 'Ġhaven'], [12905, '`'], [90, 't'], [56, 'Ġhad'], [10, 'Ġa'], [205, 'Ġgood']]\n"
     ]
    }
   ],
   "source": [
    "check_idx = 758\n",
    "print(df_span.text[check_idx])\n",
    "print(df_span.selected_text[check_idx])\n",
    "print([[i,j,k,l] for i,j,k,l in zip(X_span_tokens[check_idx].tokens,\n",
    "                                    X_span_tokens[check_idx].ids,\n",
    "                                    Y_span_starts[check_idx],\n",
    "                                    Y_span_stops[check_idx])])\n",
    "print([[i,j] for i,j in zip(Y_span_tokens[check_idx].ids,\n",
    "                            Y_span_tokens[check_idx].tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "\n",
    "Y_span_starts = pad_sequences(Y_span_starts, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")#.argmax(axis=1)\n",
    "Y_span_stops = pad_sequences(Y_span_stops, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")#.argmax(axis=1)\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAX_SEQ_LEN_SPAN': 59,\n",
      " 'VOCAB_SIZE': 50265,\n",
      " 'X_span': (2000, 59),\n",
      " 'X_span_att': (2000, 59),\n",
      " 'X_span_att_test': (2000, 59),\n",
      " 'X_span_test': (2000, 59),\n",
      " 'Y_span': (2000, 59),\n",
      " 'Y_span_starts': (2000, 59),\n",
      " 'Y_span_stops': (2000, 59)}\n"
     ]
    }
   ],
   "source": [
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \"Y_span_starts\" : Y_span_starts.shape,\n",
    "    \"Y_span_stops\" : Y_span_stops.shape,\n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "    \"VOCAB_SIZE\":VOCAB_SIZE,\n",
    "    \"MAX_SEQ_LEN_SPAN\":MAX_SEQ_LEN_SPAN\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_flag = np.isin(Y_span_stops.argmax(axis=1),\n",
    "                    np.unique(Y_span_stops.argmax(axis=1),\n",
    "                              return_counts=True)[0][np.unique(Y_span_stops.argmax(axis=1),\n",
    "                                                               return_counts=True)[1]>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1998, 2000, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(keep_flag), df_span.shape[0], df_span.shape[0] - sum(keep_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (2000, 59) \t: X  \n",
      " (2000, 59) \t: X_att  \n",
      " (2000, 59) \t: Y  \n",
      " (2000, 59) \t: Y_starts  \n",
      " (2000, 59) \t: Y_stops  \n",
      " (2000, 59) \t: X_test  \n",
      " (2000, 59) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = X_span[keep_flag]\n",
    "X_span_att = X_span_att[keep_flag]\n",
    "Y_span = Y_span[keep_flag]\n",
    "Y_span_starts = Y_span_starts[keep_flag]\n",
    "Y_span_stops = Y_span_stops[keep_flag]\n",
    "X_span_test = X_span_test\n",
    "X_span_att_test = X_span_att_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (1998, 59) \t: X  \n",
      " (1998, 59) \t: X_att  \n",
      " (1998, 59) \t: Y  \n",
      " (1998, 59) \t: Y_starts  \n",
      " (1998, 59) \t: Y_stops  \n",
      " (2000, 59) \t: X_test  \n",
      " (2000, 59) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_span_words = [tokenizer.decode(i) for i in Y_span]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 59, 49)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LEN = max(MAX_SEQ_LEN_SENT, MAX_SEQ_LEN_SPAN)\n",
    "MAX_SEQ_LEN, MAX_SEQ_LEN_SPAN, MAX_SEQ_LEN_SENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X_sent': (2000, 59),\n",
      " 'X_sent_att': (2000, 59),\n",
      " 'X_span': (1998, 59),\n",
      " 'X_span_att': (1998, 59),\n",
      " 'X_span_att_test': (2000, 59),\n",
      " 'X_span_test': (2000, 59),\n",
      " 'Y_span': (1998, 59)}\n"
     ]
    }
   ],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span_starts = pad_sequences(Y_span_starts, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span_stops = pad_sequences(Y_span_stops, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "X_sent = pad_sequences(X_sent, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_sent_att = pad_sequences(X_sent_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "    \n",
    "    \"X_sent\" : X_sent.shape,\n",
    "    \"X_sent_att\" : X_sent_att.shape,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_sequences = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"words\")\n",
    "    input_att_flags = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"att_flags\")\n",
    "    input_token_ids = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"token_ids\")\n",
    "    \n",
    "    config = RobertaConfig.from_pretrained(MODEL_DIR+'config.json')\n",
    "    roberta_model = TFRobertaModel.from_pretrained(MODEL_DIR+'tf_model.h5', config=config)\n",
    "    x = roberta_model(inputs=input_sequences, attention_mask=input_att_flags, token_type_ids=input_token_ids)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(DROPOUT)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    output_starts_0 = tf.keras.layers.Activation('softmax', name=\"starts_0\")(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(DROPOUT)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    output_stops_0 = tf.keras.layers.Activation('softmax', name=\"stops_0\")(x2)\n",
    "    \n",
    "    x3 = tf.keras.layers.Dropout(DROPOUT)(x[0]) \n",
    "    x3 = tf.keras.layers.Conv1D(768, 2,padding='same')(x3)\n",
    "    x3 = tf.keras.layers.LeakyReLU()(x3)\n",
    "    x3 = tf.keras.layers.Dense(1)(x3)\n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "    x3 = tf.keras.layers.Dense(3)(x3)\n",
    "    output_sentiment = tf.keras.layers.Activation('softmax', name=\"output_sentiments\")(x3)\n",
    "    \n",
    "    sentiment_model = Model([input_att_flags, input_sequences, input_token_ids], [output_sentiment])\n",
    "    \n",
    "    output_subtract = tf.keras.layers.Subtract()([output_starts_0, output_stops_0])\n",
    "    output_flat = concatenate([output_starts_0, output_stops_0, output_subtract, output_sentiment])\n",
    "    output_starts_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"starts_1\")(output_flat)\n",
    "    output_stops_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"stops_1\")(output_flat)\n",
    "    \n",
    "    span_detection_model = Model([input_att_flags, input_sequences, input_token_ids],\n",
    "                                 [output_starts_0, output_stops_0, output_starts_1, output_stops_1])\n",
    "    \n",
    "    return sentiment_model, span_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model, span_detection_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 59)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 59)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 59)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 59, 768), (N 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 59, 768)      0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 59, 768)      1180416     dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 59, 768)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 59, 1)        769         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 59)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            180         flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_sentiments (Activation)  (None, 3)            0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 125,826,997\n",
      "Trainable params: 125,826,997\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 59)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 59)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 59)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 59, 768), (N 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 59, 768)      0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 59, 768)      0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 59, 768)      0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 59, 768)      1180416     dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 59, 768)      1180416     dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 59, 768)      1180416     dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 59, 768)      0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 59, 768)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 59, 768)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 59, 1)        769         leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 59, 1)        769         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 59, 1)        769         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 59)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 59)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 59)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "starts_0 (Activation)           (None, 59)           0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "stops_0 (Activation)            (None, 59)           0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            180         flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 59)           0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_sentiments (Activation)  (None, 3)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 180)          0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "                                                                 subtract[0][0]                   \n",
      "                                                                 output_sentiments[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "starts_1 (Dense)                (None, 59)           10679       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "stops_1 (Dense)                 (None, 59)           10679       concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 128,210,725\n",
      "Trainable params: 128,210,725\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "span_detection_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_sentiment(x):\n",
    "    encoded_repr = tokenizer.encode_batch(x.tolist())\n",
    "\n",
    "    sample_text_ids = pad_sequences([i.ids for i in encoded_repr],\n",
    "                                    maxlen=MAX_SEQ_LEN,\n",
    "                                    padding=\"post\")\n",
    "    sample_text_att = pad_sequences([i.attention_mask for i in encoded_repr],\n",
    "                                    maxlen=MAX_SEQ_LEN,\n",
    "                                    padding=\"post\")\n",
    "    pred = sentiment_model.predict({\"att_flags\":sample_text_att,\n",
    "                                    \"words\":sample_text_ids,\n",
    "                                    \"token_ids\":np.zeros_like(sample_text_att)},\n",
    "                                  batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    res = pd.DataFrame({\"predicted_sentiment\":pred.argmax(axis=1)})\n",
    "    \n",
    "    return res.predicted_sentiment.apply(lambda x:[k for k,v in sentiment_lookup.items() if v==x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_results(data):\n",
    "    data[\"predicted_sentiment\"] = infer_sentiment(x=data.text)\n",
    "\n",
    "    print(classification_report(y_true=data.sentiment[data.index.isin(tr_index)],\n",
    "                                y_pred=data.predicted_sentiment[data.index.isin(tr_index)]))\n",
    "\n",
    "    print(classification_report(y_true=data.sentiment[data.index.isin(va_index)],\n",
    "                                y_pred=data.predicted_sentiment[data.index.isin(va_index)]))\n",
    "\n",
    "    print(confusion_matrix(y_true=data.sentiment,\n",
    "                     y_pred=data.predicted_sentiment,\n",
    "                     labels=['positive', 'neutral', 'negative']))\n",
    "\n",
    "    print(confusion_matrix(y_true=data.sentiment,\n",
    "                     y_pred=data.predicted_sentiment,\n",
    "                     labels=['positive', 'neutral', 'negative'],\n",
    "                     normalize=\"all\"))\n",
    "\n",
    "    data[\"set2\"] = np.where(data.index.isin(tr_index), \"train\", \"valid\")\n",
    "    print(data.groupby(\"set2\").apply(lambda x : accuracy_score(y_true=x.sentiment, y_pred=x.predicted_sentiment)))\n",
    "\n",
    "    print(data.groupby(\"set\").apply(lambda x : accuracy_score(y_true=x.sentiment, y_pred=x.predicted_sentiment)))\n",
    "\n",
    "    print(pd.concat({\n",
    "        \"accuracy\" : data.groupby([\"set\", \"set2\"]).apply(lambda x : accuracy_score(y_true=x.sentiment,\n",
    "                                                                                   y_pred=x.predicted_sentiment)),\n",
    "        \"count\" : data.groupby([\"set\", \"set2\"])[\"sentiment\"].count()\n",
    "    }, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,c = np.unique(Y_sent, return_counts=True)\n",
    "cw = class_weight.compute_class_weight('balanced', np.unique(Y_sent), Y_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_FOLDS):\n",
    "    if os.path.exists(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\"):\n",
    "        os.remove(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(pred_dict):\n",
    "    print(\"[INFO] \",\"=\"*15,\"Validation for FOLD#\", num, \"=\"*15)\n",
    "    funcs = [accuracy_score, f1_score, precision_score, recall_score, confusion_matrix]\n",
    "    for f in funcs:\n",
    "        for data_set in [\"train\",\"valid\"]:\n",
    "            for var in [\"starts\", \"stops\"]:\n",
    "                if f in [accuracy_score]:\n",
    "                    res = f(**pred_dict[data_set][var])\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "                elif f in [confusion_matrix]:\n",
    "                    res = f(**pred_dict[data_set][var], labels=np.arange(MAX_SEQ_LEN))\n",
    "                    np.savetxt(X=res, fmt='%i', delimiter=\",\",\n",
    "                               fname=RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                    print(\"[INFO] \\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__, \"\\t||\", \n",
    "                          RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                else:\n",
    "                    res = f(**pred_dict[data_set][var], average=\"macro\")\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "        print(\"=======================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(string):\n",
    "    string = re.sub(pattern=\" (negative|positive|neutral)[ ]+$\", repl=\"\", string=string)\n",
    "    string = re.sub(pattern=\" +\", repl=\" \", string=string.strip())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ==================== FOLD# 0 ====================\n",
      "[INFO] Training Sentiment only the final layers at higher learning rates.\n",
      "Train on 1333 samples, validate on 667 samples\n",
      "Epoch 1/4\n",
      "1333/1333 [==============================] - 29s 22ms/sample - loss: 1.0788 - accuracy: 0.4944 - val_loss: 0.8906 - val_accuracy: 0.5982\n",
      "Epoch 2/4\n",
      "1333/1333 [==============================] - 13s 10ms/sample - loss: 1.0090 - accuracy: 0.5101 - val_loss: 0.8696 - val_accuracy: 0.6162\n",
      "Epoch 3/4\n",
      "1333/1333 [==============================] - 13s 10ms/sample - loss: 0.9590 - accuracy: 0.5386 - val_loss: 0.8761 - val_accuracy: 0.6267\n",
      "Epoch 4/4\n",
      "1333/1333 [==============================] - 13s 10ms/sample - loss: 0.9213 - accuracy: 0.5574 - val_loss: 0.8672 - val_accuracy: 0.5997\n",
      "[INFO] Training Span only the final layers at higher learning rates.\n",
      "Train on 1332 samples, validate on 666 samples\n",
      "Epoch 1/4\n",
      "1332/1332 [==============================] - 31s 23ms/sample - loss: 16.5534 - starts_0_loss: 2.7459 - stops_0_loss: 2.7062 - starts_1_loss: 3.6014 - stops_1_loss: 3.7489 - starts_0_accuracy: 0.3971 - stops_0_accuracy: 0.4114 - starts_1_accuracy: 0.3101 - stops_1_accuracy: 0.0751 - val_loss: 15.8063 - val_starts_0_loss: 2.4664 - val_stops_0_loss: 2.4414 - val_starts_1_loss: 3.5317 - val_stops_1_loss: 3.6854 - val_starts_0_accuracy: 0.4775 - val_stops_0_accuracy: 0.4294 - val_starts_1_accuracy: 0.3033 - val_stops_1_accuracy: 0.1261\n",
      "Epoch 2/4\n",
      "1332/1332 [==============================] - 24s 18ms/sample - loss: 16.2754 - starts_0_loss: 2.7082 - stops_0_loss: 2.6513 - starts_1_loss: 3.5224 - stops_1_loss: 3.7036 - starts_0_accuracy: 0.4129 - stops_0_accuracy: 0.4399 - starts_1_accuracy: 0.3101 - stops_1_accuracy: 0.0916 - val_loss: 15.7063 - val_starts_0_loss: 2.4708 - val_stops_0_loss: 2.4547 - val_starts_1_loss: 3.4740 - val_stops_1_loss: 3.6547 - val_starts_0_accuracy: 0.4805 - val_stops_0_accuracy: 0.4219 - val_starts_1_accuracy: 0.3108 - val_stops_1_accuracy: 0.1592\n",
      "Epoch 3/4\n",
      "1332/1332 [==============================] - 19s 14ms/sample - loss: 16.0608 - starts_0_loss: 2.6667 - stops_0_loss: 2.6047 - starts_1_loss: 3.4536 - stops_1_loss: 3.6630 - starts_0_accuracy: 0.4384 - stops_0_accuracy: 0.4489 - starts_1_accuracy: 0.3101 - stops_1_accuracy: 0.1171 - val_loss: 15.5741 - val_starts_0_loss: 2.4704 - val_stops_0_loss: 2.4538 - val_starts_1_loss: 3.4147 - val_stops_1_loss: 3.6192 - val_starts_0_accuracy: 0.4580 - val_stops_0_accuracy: 0.4309 - val_starts_1_accuracy: 0.3108 - val_stops_1_accuracy: 0.1757\n",
      "Epoch 4/4\n",
      "1332/1332 [==============================] - 15s 11ms/sample - loss: 15.8557 - starts_0_loss: 2.6089 - stops_0_loss: 2.5882 - starts_1_loss: 3.3931 - stops_1_loss: 3.6318 - starts_0_accuracy: 0.4407 - stops_0_accuracy: 0.4474 - starts_1_accuracy: 0.3101 - stops_1_accuracy: 0.1216 - val_loss: 15.4628 - val_starts_0_loss: 2.4600 - val_stops_0_loss: 2.4592 - val_starts_1_loss: 3.3659 - val_stops_1_loss: 3.5905 - val_starts_0_accuracy: 0.4655 - val_stops_0_accuracy: 0.4339 - val_starts_1_accuracy: 0.3108 - val_stops_1_accuracy: 0.1832\n",
      "[INFO] Training Sentiment only the final layers at lower learning rates.\n",
      "Train on 1333 samples, validate on 667 samples\n",
      "Epoch 1/4\n",
      "1333/1333 [==============================] - 28s 21ms/sample - loss: 5.5624 - accuracy: 0.4119 - val_loss: 1.5948 - val_accuracy: 0.4288\n",
      "Epoch 2/4\n",
      "1333/1333 [==============================] - 14s 10ms/sample - loss: 1.0786 - accuracy: 0.5259 - val_loss: 0.9435 - val_accuracy: 0.5397\n",
      "Epoch 3/4\n",
      "1333/1333 [==============================] - 14s 10ms/sample - loss: 0.9732 - accuracy: 0.5476 - val_loss: 0.9260 - val_accuracy: 0.5457\n",
      "Epoch 4/4\n",
      "1333/1333 [==============================] - 14s 10ms/sample - loss: 0.9619 - accuracy: 0.5484 - val_loss: 0.9147 - val_accuracy: 0.5607\n",
      "[INFO] Training Span only the final layers at lower learning rates.\n",
      "Train on 1332 samples, validate on 666 samples\n",
      "Epoch 1/4\n",
      "1332/1332 [==============================] - 31s 23ms/sample - loss: 15.7431 - starts_0_loss: 2.6029 - stops_0_loss: 2.5586 - starts_1_loss: 3.3741 - stops_1_loss: 3.6109 - starts_0_accuracy: 0.4459 - stops_0_accuracy: 0.4610 - starts_1_accuracy: 0.3101 - stops_1_accuracy: 0.1614 - val_loss: 15.4398 - val_starts_0_loss: 2.4636 - val_stops_0_loss: 2.4639 - val_starts_1_loss: 3.3533 - val_stops_1_loss: 3.5816 - val_starts_0_accuracy: 0.4775 - val_stops_0_accuracy: 0.4384 - val_starts_1_accuracy: 0.3108 - val_stops_1_accuracy: 0.1907\n",
      "Epoch 2/4\n",
      "1332/1332 [==============================] - 15s 11ms/sample - loss: 15.6959 - starts_0_loss: 2.5857 - stops_0_loss: 2.5372 - starts_1_loss: 3.3539 - stops_1_loss: 3.6021 - starts_0_accuracy: 0.4474 - stops_0_accuracy: 0.4632 - starts_1_accuracy: 0.3108 - stops_1_accuracy: 0.1562 - val_loss: 15.4240 - val_starts_0_loss: 2.4641 - val_stops_0_loss: 2.4639 - val_starts_1_loss: 3.3463 - val_stops_1_loss: 3.5770 - val_starts_0_accuracy: 0.4700 - val_stops_0_accuracy: 0.4354 - val_starts_1_accuracy: 0.3108 - val_stops_1_accuracy: 0.1907\n",
      "Epoch 3/4\n",
      "1332/1332 [==============================] - 15s 11ms/sample - loss: 15.6774 - starts_0_loss: 2.5958 - stops_0_loss: 2.5257 - starts_1_loss: 3.3510 - stops_1_loss: 3.6003 - starts_0_accuracy: 0.4565 - stops_0_accuracy: 0.4617 - starts_1_accuracy: 0.3101 - stops_1_accuracy: 0.1524 - val_loss: 15.4217 - val_starts_0_loss: 2.4672 - val_stops_0_loss: 2.4710 - val_starts_1_loss: 3.3416 - val_stops_1_loss: 3.5732 - val_starts_0_accuracy: 0.4670 - val_stops_0_accuracy: 0.4339 - val_starts_1_accuracy: 0.3108 - val_stops_1_accuracy: 0.1922\n",
      "Epoch 4/4\n",
      "1332/1332 [==============================] - 15s 11ms/sample - loss: 15.6778 - starts_0_loss: 2.5969 - stops_0_loss: 2.5490 - starts_1_loss: 3.3535 - stops_1_loss: 3.5967 - starts_0_accuracy: 0.4520 - stops_0_accuracy: 0.4640 - starts_1_accuracy: 0.3101 - stops_1_accuracy: 0.1517 - val_loss: 15.4047 - val_starts_0_loss: 2.4667 - val_stops_0_loss: 2.4688 - val_starts_1_loss: 3.3354 - val_stops_1_loss: 3.5691 - val_starts_0_accuracy: 0.4640 - val_stops_0_accuracy: 0.4309 - val_starts_1_accuracy: 0.3108 - val_stops_1_accuracy: 0.1937\n",
      "[INFO] Training Sentiment only the final layers at higher learning rates.\n",
      "Train on 1333 samples, validate on 667 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "1333/1333 [==============================] - 45s 34ms/sample - loss: 1.1042 - accuracy: 0.5214 - val_loss: 0.9446 - val_accuracy: 0.5427\n",
      "Epoch 2/2\n",
      "1333/1333 [==============================] - 27s 20ms/sample - loss: 0.9079 - accuracy: 0.5641 - val_loss: 0.9056 - val_accuracy: 0.5607\n",
      "[INFO] Unfreezing Span RoBerta layer and training at lowest learning rates.\n",
      "Train on 1332 samples, validate on 666 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "1332/1332 [==============================] - 49s 37ms/sample - loss: 15.6275 - starts_0_loss: 2.5743 - stops_0_loss: 2.5289 - starts_1_loss: 3.3443 - stops_1_loss: 3.5918 - starts_0_accuracy: 0.4655 - stops_0_accuracy: 0.4632 - starts_1_accuracy: 0.3101 - stops_1_accuracy: 0.1674 - val_loss: 15.3370 - val_starts_0_loss: 2.4353 - val_stops_0_loss: 2.4544 - val_starts_1_loss: 3.3308 - val_stops_1_loss: 3.5605 - val_starts_0_accuracy: 0.4865 - val_stops_0_accuracy: 0.4369 - val_starts_1_accuracy: 0.3108 - val_stops_1_accuracy: 0.2117\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.13      0.00      0.01       408\n",
      "     neutral       0.41      1.00      0.58       542\n",
      "    positive       1.00      0.00      0.01       383\n",
      "\n",
      "    accuracy                           0.41      1333\n",
      "   macro avg       0.51      0.34      0.20      1333\n",
      "weighted avg       0.50      0.41      0.24      1333\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00       183\n",
      "     neutral       0.39      1.00      0.56       259\n",
      "    positive       0.00      0.00      0.00       225\n",
      "\n",
      "    accuracy                           0.39       667\n",
      "   macro avg       0.13      0.33      0.19       667\n",
      "weighted avg       0.15      0.39      0.22       667\n",
      "\n",
      "[[  1 588  19]\n",
      " [  0 801   0]\n",
      " [  0 589   2]]\n",
      "[[0.0005 0.294  0.0095]\n",
      " [0.     0.4005 0.    ]\n",
      " [0.     0.2945 0.001 ]]\n",
      "set2\n",
      "train    0.408852\n",
      "valid    0.388306\n",
      "dtype: float64\n",
      "set\n",
      "test     0.400\n",
      "train    0.404\n",
      "dtype: float64\n",
      "             accuracy  count\n",
      "set   set2                  \n",
      "test  train  0.403670    654\n",
      "      valid  0.393064    346\n",
      "train train  0.413844    679\n",
      "      valid  0.383178    321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]  =============== Validation for FOLD# 0 ===============\n",
      "[INFO] 53.68 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 53.90 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 48.65 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 43.69 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 44.61 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 51.26 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 35.57 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 39.54 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 47.10 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 49.82 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 39.25 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 37.94 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 45.16 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 57.20 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 36.29 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 45.50 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V36_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V36_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V36_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V36_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (1332, 59) (1332, 59)\n",
      "[INFO] Prediction shape for validation data:  (666, 59) (666, 59)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  1116 out of 1332\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  564 out of 666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training Jaccard Score:  0.5436226589407911\n",
      "[INFO] Validation Jaccard Score:  0.5167843380680915\n",
      "[INFO] Training for fold: 0 finished at Fri Jun 12 21:51:27 2020\n",
      "[INFO] ==================== FOLD# 1 ====================\n",
      "[INFO] Training Sentiment only the final layers at higher learning rates.\n",
      "Train on 1333 samples, validate on 667 samples\n",
      "Epoch 1/4\n",
      "1333/1333 [==============================] - 28s 21ms/sample - loss: 1.1991 - accuracy: 0.3886 - val_loss: 0.9785 - val_accuracy: 0.5217\n",
      "Epoch 2/4\n",
      "1333/1333 [==============================] - 13s 10ms/sample - loss: 1.1218 - accuracy: 0.4441 - val_loss: 1.0156 - val_accuracy: 0.4798\n",
      "Epoch 3/4\n",
      "1333/1333 [==============================] - 14s 10ms/sample - loss: 1.0163 - accuracy: 0.4906 - val_loss: 1.0156 - val_accuracy: 0.4588\n",
      "Epoch 4/4\n",
      "1333/1333 [==============================] - 14s 10ms/sample - loss: 0.9952 - accuracy: 0.5206 - val_loss: 0.9872 - val_accuracy: 0.5157\n",
      "[INFO] Training Span only the final layers at higher learning rates.\n",
      "Train on 1332 samples, validate on 666 samples\n",
      "Epoch 1/4\n",
      "1332/1332 [==============================] - 31s 23ms/sample - loss: 17.9708 - starts_0_loss: 2.9372 - stops_0_loss: 2.9141 - starts_1_loss: 4.0519 - stops_1_loss: 4.0359 - starts_0_accuracy: 0.3378 - stops_0_accuracy: 0.3416 - starts_1_accuracy: 0.0368 - stops_1_accuracy: 0.0353 - val_loss: 17.1738 - val_starts_0_loss: 2.6223 - val_stops_0_loss: 2.5670 - val_starts_1_loss: 3.9939 - val_stops_1_loss: 3.9955 - val_starts_0_accuracy: 0.4204 - val_stops_0_accuracy: 0.4294 - val_starts_1_accuracy: 0.0465 - val_stops_1_accuracy: 0.0556\n",
      "Epoch 2/4\n",
      "1332/1332 [==============================] - 14s 11ms/sample - loss: 17.3504 - starts_0_loss: 2.7865 - stops_0_loss: 2.7135 - starts_1_loss: 3.9415 - stops_1_loss: 3.9566 - starts_0_accuracy: 0.4077 - stops_0_accuracy: 0.3926 - starts_1_accuracy: 0.0571 - stops_1_accuracy: 0.0480 - val_loss: 16.8214 - val_starts_0_loss: 2.5766 - val_stops_0_loss: 2.5196 - val_starts_1_loss: 3.8844 - val_stops_1_loss: 3.9210 - val_starts_0_accuracy: 0.4444 - val_stops_0_accuracy: 0.4294 - val_starts_1_accuracy: 0.0526 - val_stops_1_accuracy: 0.0661\n",
      "Epoch 3/4\n",
      "1332/1332 [==============================] - 15s 11ms/sample - loss: 17.0662 - starts_0_loss: 2.7604 - stops_0_loss: 2.7051 - starts_1_loss: 3.8444 - stops_1_loss: 3.8857 - starts_0_accuracy: 0.3934 - stops_0_accuracy: 0.4092 - starts_1_accuracy: 0.0713 - stops_1_accuracy: 0.0676 - val_loss: 16.5767 - val_starts_0_loss: 2.5530 - val_stops_0_loss: 2.5186 - val_starts_1_loss: 3.7875 - val_stops_1_loss: 3.8594 - val_starts_0_accuracy: 0.4294 - val_stops_0_accuracy: 0.4414 - val_starts_1_accuracy: 0.0991 - val_stops_1_accuracy: 0.0766\n",
      "Epoch 4/4\n",
      "1332/1332 [==============================] - 15s 11ms/sample - loss: 16.7956 - starts_0_loss: 2.7417 - stops_0_loss: 2.6462 - starts_1_loss: 3.7561 - stops_1_loss: 3.8297 - starts_0_accuracy: 0.4039 - stops_0_accuracy: 0.4384 - starts_1_accuracy: 0.1877 - stops_1_accuracy: 0.0773 - val_loss: 16.4300 - val_starts_0_loss: 2.5731 - val_stops_0_loss: 2.5427 - val_starts_1_loss: 3.6997 - val_stops_1_loss: 3.8079 - val_starts_0_accuracy: 0.4249 - val_stops_0_accuracy: 0.4219 - val_starts_1_accuracy: 0.2673 - val_stops_1_accuracy: 0.0766\n",
      "[INFO] Training Sentiment only the final layers at lower learning rates.\n",
      "Train on 1333 samples, validate on 667 samples\n",
      "Epoch 1/4\n",
      "1333/1333 [==============================] - 28s 21ms/sample - loss: 3.8652 - accuracy: 0.3818 - val_loss: 1.1338 - val_accuracy: 0.4813\n",
      "Epoch 2/4\n",
      "1333/1333 [==============================] - 13s 10ms/sample - loss: 1.0907 - accuracy: 0.5191 - val_loss: 1.0418 - val_accuracy: 0.4963\n",
      "Epoch 3/4\n",
      "1333/1333 [==============================] - 14s 10ms/sample - loss: 0.9851 - accuracy: 0.5379 - val_loss: 1.0386 - val_accuracy: 0.4993\n",
      "Epoch 4/4\n",
      "1333/1333 [==============================] - 13s 10ms/sample - loss: 0.9596 - accuracy: 0.5424 - val_loss: 1.0000 - val_accuracy: 0.5067\n",
      "[INFO] Training Span only the final layers at lower learning rates.\n",
      "Train on 1332 samples, validate on 666 samples\n",
      "Epoch 1/4\n",
      "1332/1332 [==============================] - 31s 24ms/sample - loss: 16.5354 - starts_0_loss: 2.6369 - stops_0_loss: 2.5847 - starts_1_loss: 3.7080 - stops_1_loss: 3.7997 - starts_0_accuracy: 0.4212 - stops_0_accuracy: 0.4444 - starts_1_accuracy: 0.2613 - stops_1_accuracy: 0.0698 - val_loss: 16.3365 - val_starts_0_loss: 2.5464 - val_stops_0_loss: 2.4964 - val_starts_1_loss: 3.6944 - val_stops_1_loss: 3.8003 - val_starts_0_accuracy: 0.4324 - val_stops_0_accuracy: 0.4429 - val_starts_1_accuracy: 0.2778 - val_stops_1_accuracy: 0.0781\n",
      "Epoch 2/4\n",
      "1332/1332 [==============================] - 15s 11ms/sample - loss: 16.4603 - starts_0_loss: 2.6303 - stops_0_loss: 2.5543 - starts_1_loss: 3.6973 - stops_1_loss: 3.7901 - starts_0_accuracy: 0.4392 - stops_0_accuracy: 0.4730 - starts_1_accuracy: 0.2583 - stops_1_accuracy: 0.0833 - val_loss: 16.3035 - val_starts_0_loss: 2.5384 - val_stops_0_loss: 2.4935 - val_starts_1_loss: 3.6839 - val_stops_1_loss: 3.7945 - val_starts_0_accuracy: 0.4324 - val_stops_0_accuracy: 0.4399 - val_starts_1_accuracy: 0.2853 - val_stops_1_accuracy: 0.0781\n",
      "Epoch 3/4\n",
      "1332/1332 [==============================] - 15s 11ms/sample - loss: 16.4517 - starts_0_loss: 2.6371 - stops_0_loss: 2.5634 - starts_1_loss: 3.6868 - stops_1_loss: 3.7825 - starts_0_accuracy: 0.4279 - stops_0_accuracy: 0.4520 - starts_1_accuracy: 0.2718 - stops_1_accuracy: 0.0818 - val_loss: 16.2794 - val_starts_0_loss: 2.5367 - val_stops_0_loss: 2.4883 - val_starts_1_loss: 3.6762 - val_stops_1_loss: 3.7897 - val_starts_0_accuracy: 0.4309 - val_stops_0_accuracy: 0.4384 - val_starts_1_accuracy: 0.2883 - val_stops_1_accuracy: 0.0811\n",
      "Epoch 4/4\n",
      "1332/1332 [==============================] - 15s 11ms/sample - loss: 16.3597 - starts_0_loss: 2.6032 - stops_0_loss: 2.5216 - starts_1_loss: 3.6777 - stops_1_loss: 3.7767 - starts_0_accuracy: 0.4557 - stops_0_accuracy: 0.4670 - starts_1_accuracy: 0.2688 - stops_1_accuracy: 0.0796 - val_loss: 16.2539 - val_starts_0_loss: 2.5348 - val_stops_0_loss: 2.4832 - val_starts_1_loss: 3.6674 - val_stops_1_loss: 3.7848 - val_starts_0_accuracy: 0.4324 - val_stops_0_accuracy: 0.4354 - val_starts_1_accuracy: 0.3063 - val_stops_1_accuracy: 0.0841\n",
      "[INFO] Training Sentiment only the final layers at higher learning rates.\n",
      "Train on 1333 samples, validate on 667 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "  16/1333 [..............................] - ETA: 1:57:30"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[944,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/tf_roberta_model/roberta/encoder/layer_._8/attention/self/query/Tensordot/MatMul (defined at c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\transformers\\modeling_tf_bert.py:230) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Reshape_827/_550]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[944,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/tf_roberta_model/roberta/encoder/layer_._8/attention/self/query/Tensordot/MatMul (defined at c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\transformers\\modeling_tf_bert.py:230) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_distributed_function_705217]\n\nFunction call stack:\ndistributed_function -> distributed_function\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-98f8680b18db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    142\u001b[0m                                                  \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                                                  \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                                                  callbacks=[sentiment_mcp, sentiment_csvl])\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[INFO] Unfreezing Span RoBerta layer and training at lowest learning rates.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   2044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m     \u001b[0mrow_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2046\u001b[1;33m     \u001b[0mrow_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2047\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2048\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m     \u001b[0mrow_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2046\u001b[1;33m     \u001b[0mrow_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2047\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2048\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "for (num2, (tr_index, va_index)), (num, (t_index, v_index)) in zip(enumerate(sentiment_kf.split(X_sent, Y_sent)), enumerate(span_kf.split(X_span, Y_span_stops))):\n",
    "    \n",
    "    if num > 0:\n",
    "        del span_history\n",
    "        del span_detection_model\n",
    "        del sent_history\n",
    "        del sentiment_model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "        \n",
    "        sentiment_model, span_detection_model = build_model()\n",
    "    \n",
    "    print(\"[INFO] ==================== FOLD#\", num, \"====================\")\n",
    "    \n",
    "    sentiment_mcp = ModelCheckpoint(filepath=RESULTS_DIR+\"Sentiment_\"+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\", monitor='val_loss',\n",
    "                                    verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "    sentiment_csvl = CSVLogger(filename=RESULTS_DIR+\"Sentiment_\"+MODEL_PREFIX+\"_LossLogs_\"+str(num)+\".csv\",\n",
    "                               separator=\",\", append=True)\n",
    "\n",
    "    span_mcp = ModelCheckpoint(filepath=RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\", monitor='val_loss',\n",
    "                               verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "    \n",
    "    span_csvl = CSVLogger(filename=RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(num)+\".csv\",\n",
    "                          separator=\",\", append=True)\n",
    "    \n",
    "    print(\"[INFO] Training Sentiment only the final layers at higher learning rates.\")\n",
    "    sentiment_model.layers[3].trainable = False\n",
    "    adam = Adam(learning_rate=SENTIMENT_MAX_LR)\n",
    "    sentiment_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                            optimizer=adam,\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "    sent_history = sentiment_model.fit(x={\"att_flags\":X_sent_att[tr_index],\n",
    "                                          \"words\":X_sent[tr_index],\n",
    "                                          \"token_ids\":np.zeros_like(X_sent_att[tr_index])},\n",
    "                                       y={\"output_sentiments\":Y_sent[tr_index]},\n",
    "                                       shuffle=True,\n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       epochs=SENTIMENT_NUM_EPOCHS[0],\n",
    "                                       validation_data=({\"att_flags\":X_sent_att[va_index],\n",
    "                                                         \"words\":X_sent[va_index],\n",
    "                                                         \"token_ids\":np.zeros_like(X_sent_att[va_index])},\n",
    "                                                        {\"output_sentiments\":Y_sent[va_index]}),\n",
    "                                       verbose=1,\n",
    "                                       class_weight=cw,\n",
    "                                       callbacks=[sentiment_mcp, sentiment_csvl])\n",
    "\n",
    "    print(\"[INFO] Training Span only the final layers at higher learning rates.\")\n",
    "    span_detection_model.layers[3].trainable = False\n",
    "    adam = Adam(learning_rate=MAX_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=LABEL_SMOOTHING_PARAM),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":2.0})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[0],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl])\n",
    "    \n",
    "    print(\"[INFO] Training Sentiment only the final layers at lower learning rates.\")\n",
    "    sentiment_model.layers[3].trainable = False\n",
    "    adam = Adam(learning_rate=SENTIMENT_MID_LR)\n",
    "    sentiment_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                            optimizer=adam,\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "    sent_history = sentiment_model.fit(x={\"att_flags\":X_sent_att[tr_index],\n",
    "                                          \"words\":X_sent[tr_index],\n",
    "                                          \"token_ids\":np.zeros_like(X_sent_att[tr_index])},\n",
    "                                       y={\"output_sentiments\":Y_sent[tr_index]},\n",
    "                                       shuffle=True,\n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       epochs=SENTIMENT_NUM_EPOCHS[1],\n",
    "                                       validation_data=({\"att_flags\":X_sent_att[va_index],\n",
    "                                                         \"words\":X_sent[va_index],\n",
    "                                                         \"token_ids\":np.zeros_like(X_sent_att[va_index])},\n",
    "                                                        {\"output_sentiments\":Y_sent[va_index]}),\n",
    "                                       verbose=1,\n",
    "                                       class_weight=cw,\n",
    "                                       callbacks=[sentiment_mcp, sentiment_csvl])\n",
    "    \n",
    "    print(\"[INFO] Training Span only the final layers at lower learning rates.\")\n",
    "    adam = Adam(learning_rate=MID_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=LABEL_SMOOTHING_PARAM),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":2.0})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[1],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl])\n",
    "    \n",
    "    print(\"[INFO] Training Sentiment only the final layers at higher learning rates.\")\n",
    "    sentiment_model.layers[3].trainable = True\n",
    "    adam = Adam(learning_rate=SENTIMENT_MIN_LR)\n",
    "    sentiment_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                            optimizer=adam,\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "    sent_history_finetuned = sentiment_model.fit(x={\"att_flags\":X_sent_att[tr_index],\n",
    "                                                    \"words\":X_sent[tr_index],\n",
    "                                                    \"token_ids\":np.zeros_like(X_sent_att[tr_index])},\n",
    "                                                 y={\"output_sentiments\":Y_sent[tr_index]},\n",
    "                                                 shuffle=True,\n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                 epochs=SENTIMENT_NUM_EPOCHS[2],\n",
    "                                                 validation_data=({\"att_flags\":X_sent_att[va_index],\n",
    "                                                                   \"words\":X_sent[va_index],\n",
    "                                                                   \"token_ids\":np.zeros_like(X_sent_att[va_index])},\n",
    "                                                                  {\"output_sentiments\":Y_sent[va_index]}),\n",
    "                                                 verbose=1,\n",
    "                                                 class_weight=cw,\n",
    "                                                 callbacks=[sentiment_mcp, sentiment_csvl])\n",
    "    \n",
    "    print(\"[INFO] Unfreezing Span RoBerta layer and training at lowest learning rates.\")\n",
    "    span_detection_model.layers[3].trainable = True\n",
    "    adam = Adam(learning_rate=MIN_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=LABEL_SMOOTHING_PARAM),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":2.0})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[2],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl])\n",
    "    \n",
    "    get_sentiment_results(data)\n",
    "    \n",
    "    # Loading best weights per fold\n",
    "    span_detection_model.load_weights(RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\")\n",
    "    \n",
    "    pred_train = span_detection_model.predict(x = {\"att_flags\":X_span_att[t_index],\n",
    "                                                   \"words\":X_span[t_index],\n",
    "                                                   \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                              batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "    pred_val = span_detection_model.predict(x = {\"att_flags\":X_span_att[v_index],\n",
    "                                                 \"words\":X_span[v_index],\n",
    "                                                 \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                            batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    pred_starts_train, pred_stops_train = (pred_train[0]/2.0 + pred_train[2]/2.0), (pred_train[1]/2.0 + pred_train[3]/2.0)\n",
    "    pred_starts_val, pred_stops_val = (pred_val[0]/2.0 + pred_val[2]/2.0), (pred_val[1]/2.0 + pred_val[3]/2.0)\n",
    "    \n",
    "    # Accumulate test results after training every fold\n",
    "    pred_test_fold = span_detection_model.predict(x = {\"att_flags\":X_span_att_test,\n",
    "                                                       \"words\":X_span_test,\n",
    "                                                       \"token_ids\":np.zeros_like(X_span_att_test)},\n",
    "                                                  batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    if num==0:\n",
    "        pred_test = []\n",
    "        pred_test.append(pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test.append(pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    else:\n",
    "        pred_test[0] += (pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test[1] += (pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    \n",
    "    # Tabulate\n",
    "    preds = {\n",
    "        \"train\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_span_starts[t_index].argmax(axis=1),\n",
    "                \"y_pred\":pred_train[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_span_stops[t_index].argmax(axis=1),\n",
    "                \"y_pred\":pred_train[1].argmax(axis=1)\n",
    "            }\n",
    "        },\n",
    "        \"valid\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_span_starts[v_index].argmax(axis=1),\n",
    "                \"y_pred\":pred_val[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_span_stops[v_index].argmax(axis=1),\n",
    "                \"y_pred\":pred_val[1].argmax(axis=1)\n",
    "            }        \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print_metrics(pred_dict=preds)\n",
    "\n",
    "    print(\"[INFO] Prediction shape for training data: \", pred_starts_train.shape, pred_stops_train.shape)\n",
    "    print(\"[INFO] Prediction shape for validation data: \", pred_starts_val.shape, pred_stops_val.shape)\n",
    "\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for training data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_train.argmax(axis=1),\n",
    "                                  pred_stops_train.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_train.shape[0])\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for validation data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_val.argmax(axis=1),\n",
    "                                  pred_stops_val.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_val.shape[0])\n",
    "\n",
    "    pred_words_train = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[t_index],\n",
    "                                                                                                                                   pred_starts_train.argmax(axis=1),\n",
    "                                                                                                                                   pred_stops_train.argmax(axis=1))]\n",
    "    \n",
    "    pred_words_val = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[v_index],\n",
    "                                                                                                                                 pred_starts_val.argmax(axis=1),\n",
    "                                                                                                                                 pred_stops_val.argmax(axis=1))]\n",
    "\n",
    "    print(\"[INFO] Training Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in t_index],\n",
    "                                                          pred_words_train)]))\n",
    "    print(\"[INFO] Validation Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in v_index],\n",
    "                                                          pred_words_val)]))\n",
    "    print(\"[INFO] Training for fold:\", num, \"finished at\", ctime(time()))\n",
    "\n",
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = data.text.sample(5).tolist()\n",
    "\n",
    "encoded_repr = tokenizer.encode_batch(sample_text)\n",
    "\n",
    "sample_text_ids = pad_sequences([i.ids for i in encoded_repr],\n",
    "                                maxlen=MAX_SEQ_LEN,\n",
    "                                padding=\"post\")\n",
    "sample_text_att = pad_sequences([i.attention_mask for i in encoded_repr],\n",
    "                                maxlen=MAX_SEQ_LEN,\n",
    "                                padding=\"post\")\n",
    "pred = sentiment_model.predict({\"att_flags\":sample_text_att,\n",
    "                                \"words\":sample_text_ids,\n",
    "                                \"token_ids\":np.zeros_like(sample_text_att)})\n",
    "\n",
    "pprint({\n",
    "    num:{\n",
    "        \"text\":i,\n",
    "        \"predicted_sentiment\":[k for k,v in sentiment_lookup.items() if v==j][0]\n",
    "    } for num,(i,j) in enumerate(zip(sample_text, pred.argmax(axis=1)))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data.sentiment == \"neutral\") & (data.predicted_sentiment == \"negative\")].sample(5) # most incorrect in this cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span Detection Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><h2 id=\"finetuning\">Fine-tuning</h2></a>\n",
    "<p>Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.</p>\n",
    " <p>This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.</p>\n",
    " <p>It is critical to only do this step <em>after</em> the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.</p>\n",
    " <p>It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>\n",
    "\n",
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><p><strong>Important note about <code>compile()</code> and <code>trainable</code></strong></p></a>\n",
    "<p>Calling <code>compile()</code> on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the <code>trainable</code>\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until <code>compile</code> is called again. Hence, if you change any <code>trainable</code> value, make sure\n",
    " to call <code>compile()</code> again on your\n",
    "model for your changes to be taken into account.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_starts_test, pred_stops_test = pred_test[0]/NUM_FOLDS, pred_test[1]/NUM_FOLDS\n",
    "print(\"[INFO] Prediction shape for testing data: \", pred_starts_test.shape, pred_stops_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normal predictions (StartIndex less than EndIndex) for testing data:\",\n",
    "      sum([s<e for s,e in zip(pred_starts_test.argmax(axis=1),\n",
    "                              pred_stops_test.argmax(axis=1))]), \n",
    "      \"out of\",\n",
    "      pred_starts_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words_test = [\n",
    "    post_process(tokenizer.decode(t[s:e+1])) if s<e else post_process(tokenizer.decode(t[e:])) for t,s,e in zip(X_span_test,\n",
    "                                                                                                              pred_starts_test.argmax(axis=1),\n",
    "                                                                                                              pred_stops_test.argmax(axis=1))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_idx = 158\n",
    "#print([[t,i,j,k] for t,i,j,k in zip(tokenizer.decode(),X_test[check_idx],pred_starts_test[check_idx],pred_stops_test[check_idx])])\n",
    "print(tokenizer.decode(X_span_test[check_idx]))\n",
    "print(pred_starts_test.argmax(axis=1)[check_idx])\n",
    "print(pred_stops_test.argmax(axis=1)[check_idx])\n",
    "print(post_process(tokenizer.decode(X_span_test[check_idx][pred_starts_test.argmax(axis=1)[check_idx]:1+pred_stops_test.argmax(axis=1)[check_idx]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span['selected_text'] = pred_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[\"selected_text\"] = np.where(test_df_span[\"sentiment\"] == \"neutral\",\n",
    "                                         test_df_span[\"text\"],\n",
    "                                         test_df_span[\"selected_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[[\"textID\", \"selected_text\"]].to_csv(RESULTS_DIR+\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span.loc[test_df_span.sentiment!=\"neutral\"][[\"text\", \"sentiment\",\"selected_text\"]].sample(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
