{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "MODEL_PREFIX = \"V24\"\n",
    "MODEL_NUMBER = MODEL_PREFIX[-2:]\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 16\n",
    "PREDICT_BATCH_SIZE = 512\n",
    "DROPOUT = 0.3\n",
    "\n",
    "RUN_ON_SAMPLE = False\n",
    "EXCLUDE_NEUTRAL_CLASS = True\n",
    "SENTIMENT_MAX_LR = 5e-4\n",
    "SENTIMENT_MIN_LR = 5e-6\n",
    "SENTIMENT_NUM_EPOCHS = [5, 5]\n",
    "MAX_LR = 5e-3\n",
    "MID_LR = 1e-4\n",
    "MIN_LR = 1e-6\n",
    "NUM_EPOCHS = [5, 5, 5]\n",
    "NUM_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"../results/\"\n",
    "DATA_DIR = \"../data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pickle, os, sys, re, json, gc\n",
    "from time import time, ctime\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, MaxPooling2D, Softmax, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Reshape, Activation, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector, Multiply, Layer, LeakyReLU, Subtract\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "import tokenizers, transformers\n",
    "from transformers import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    a = set(str1)\n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeded_value = 88888\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.random.seed(seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  1 01:20:26 2020\n"
     ]
    }
   ],
   "source": [
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0', '2.8.0', '0.5.2']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__,\n",
    "    transformers.__version__,\n",
    "    tokenizers.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\"  target=\"_blank\"><h2 id=\"limiting_gpu_memory_growth\" data-text=\"Limiting GPU memory growth\" tabindex=\"0\">Limiting GPU memory growth</h2></a>\n",
    "<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\"><code translate=\"no\" dir=\"ltr\">CUDA_VISIBLE_DEVICES</code></a>) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the <code translate=\"no\" dir=\"ltr\">tf.config.experimental.set_visible_devices</code> method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.list_logical_devices('CPU'))\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "print(tf.config.experimental.list_physical_devices('CPU'))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it okay to exclude neutral text from models and force it later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVERAGE: 0.5334592552617378\n",
      "MEAN JACCARD: 0.9808316180492903\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>c68863089e</td>\n",
       "      <td>_tifullyTragic ... in London last night so give it a few hours &amp; I`m sure a few will appear</td>\n",
       "      <td>in London last night so give it a few hours &amp; I`m sure a few will appear</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2211</th>\n",
       "      <td>edd2aceb1c</td>\n",
       "      <td>who sings `I Remember`? i alwaysss hear it on Radio 1 but never catch the artist</td>\n",
       "      <td>who sings `I Remember`? i alwaysss hear it on Radio 1 but never catch the artist</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8286</th>\n",
       "      <td>ee53bf0e43</td>\n",
       "      <td>this time there is a theme and it is 'purple'</td>\n",
       "      <td>this time there is a theme and it is 'purple'</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4240</th>\n",
       "      <td>2736a522fa</td>\n",
       "      <td>why isnt everyone with you?</td>\n",
       "      <td>why isnt everyone with you?</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10069</th>\n",
       "      <td>8ba7a1720e</td>\n",
       "      <td>and by the way it`s short stack, not sway sway  sway sway baby is a song of theirs</td>\n",
       "      <td>and by the way it`s short stack, not sway sway  sway sway baby is a song of theirs</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID  \\\n",
       "4896   c68863089e   \n",
       "2211   edd2aceb1c   \n",
       "8286   ee53bf0e43   \n",
       "4240   2736a522fa   \n",
       "10069  8ba7a1720e   \n",
       "\n",
       "                                                                                              text  \\\n",
       "4896   _tifullyTragic ... in London last night so give it a few hours & I`m sure a few will appear   \n",
       "2211              who sings `I Remember`? i alwaysss hear it on Radio 1 but never catch the artist   \n",
       "8286                                                 this time there is a theme and it is 'purple'   \n",
       "4240                                                                   why isnt everyone with you?   \n",
       "10069           and by the way it`s short stack, not sway sway  sway sway baby is a song of theirs   \n",
       "\n",
       "                                                                            selected_text  \\\n",
       "4896             in London last night so give it a few hours & I`m sure a few will appear   \n",
       "2211     who sings `I Remember`? i alwaysss hear it on Radio 1 but never catch the artist   \n",
       "8286                                        this time there is a theme and it is 'purple'   \n",
       "4240                                                          why isnt everyone with you?   \n",
       "10069  and by the way it`s short stack, not sway sway  sway sway baby is a song of theirs   \n",
       "\n",
       "      sentiment  \n",
       "4896    neutral  \n",
       "2211    neutral  \n",
       "8286    neutral  \n",
       "4240    neutral  \n",
       "10069   neutral  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\")\n",
    "# cases where neutral's text and selected text columns are the same\n",
    "neutrals = df2.loc[df2.sentiment==\"neutral\"].copy()\n",
    "neutrals = neutrals.reset_index(drop=True)\n",
    "neutrals[\"text\"] = neutrals[\"text\"].astype(str)\n",
    "neutrals[\"selected_text\"] = neutrals[\"selected_text\"].astype(str)\n",
    "print(\"COVERAGE:\", np.sum(np.where((neutrals.text == neutrals.selected_text), 1, 0))/neutrals.shape[0])\n",
    "print(\"MEAN JACCARD:\", np.mean([jaccard(str1=i, str2=j) for i,j in zip(neutrals.selected_text,neutrals.text)]))\n",
    "\n",
    "neutrals.loc[neutrals.text != neutrals.selected_text].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  1\n",
      "selected_text  object  1\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27480, 'selected_text': 22463, 'sentiment': 3}\n",
      "            textID                                             text  \\\n",
      "count        27481                                            27480   \n",
      "unique       27481                                            27480   \n",
      "top     b5ddf13c3f  Missed the hello kitty  not enough time oh well   \n",
      "freq             1                                                1   \n",
      "\n",
      "       selected_text sentiment  \n",
      "count          27480     27481  \n",
      "unique         22463         3  \n",
      "top             good   neutral  \n",
      "freq             199     11118  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\")\n",
    "\n",
    "print(pd.concat((df.dtypes, df.isna().sum()), axis=1))\n",
    "print(df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df[i].nunique() for i in df.columns})\n",
    "print(df.describe())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count         3534   \n",
      "unique        3534   \n",
      "top     aa221b6a7d   \n",
      "freq             1   \n",
      "\n",
      "                                                                                                                                       text  \\\n",
      "count                                                                                                                                  3534   \n",
      "unique                                                                                                                                 3534   \n",
      "top     The lab informs me the logic board is dead as well. It is farking expensive to replace. I don`t know what to do.  #MyPowerBookG4RIP   \n",
      "freq                                                                                                                                      1   \n",
      "\n",
      "       sentiment  \n",
      "count       3534  \n",
      "unique         3  \n",
      "top      neutral  \n",
      "freq        1430  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(DATA_DIR+\"test.csv\", encoding=\"utf8\")\n",
    "print(pd.concat((test_df.dtypes, test_df.isna().sum()), axis=1))\n",
    "print(test_df.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df[i].nunique() for i in test_df.columns})\n",
    "print(test_df.describe())\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>c77717b103</td>\n",
       "      <td>I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½</td>\n",
       "      <td>I love to!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>28dbada620</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>*phew*  Will make a note in case anyone else runs into the same issueï¿½</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "44   c77717b103   \n",
       "192  28dbada620   \n",
       "\n",
       "                                                                                                       text  \\\n",
       "44    I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.ï¿½   \n",
       "192                                *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "                                                                selected_text  \\\n",
       "44                                                                 I love to!   \n",
       "192  *phew*  Will make a note in case anyone else runs into the same issueï¿½   \n",
       "\n",
       "    sentiment  \n",
       "44   positive  \n",
       "192   neutral  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>7223fdccc2</td>\n",
       "      <td>tikcets are only ï¿½91...each...BUT I SO WANT TO GO</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>43ad351369</td>\n",
       "      <td>AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID  \\\n",
       "145  7223fdccc2   \n",
       "618  43ad351369   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "145                                                                                             tikcets are only ï¿½91...each...BUT I SO WANT TO GO   \n",
       "618  AHHH - Whatchu talkinï¿½ baby?  HAHAHA I canï¿½t believe youu:O heh, actually I can. Life is worth taking risks... http://tumblr.com/xs81qy54s   \n",
       "\n",
       "    sentiment  \n",
       "145  positive  \n",
       "618  positive  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[test_df['text'].astype('str').apply(lambda x : len(re.findall(pattern=\"ï¿½\", string=x))>0)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"set\"], test_df[\"set\"] = \"train\", \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create smaller sample for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_ON_SAMPLE:\n",
    "    df = df.sample(500).reset_index(drop=True)\n",
    "    test_df = test_df.sample(500).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine datasets for pretraining using sentiment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31015, 3)\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat((df[[\"text\",\"set\",\"sentiment\"]],\n",
    "                  test_df[[\"text\",\"set\",\"sentiment\"]]), axis=0)\n",
    "data[\"text\"] = data[\"text\"].astype(str)\n",
    "data = data.sample(frac=1.0).reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment count in combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">test</th>\n",
       "      <th>negative</th>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">train</th>\n",
       "      <th>negative</th>\n",
       "      <td>7781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>11118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>8582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  text\n",
       "set   sentiment       \n",
       "test  negative    1001\n",
       "      neutral     1430\n",
       "      positive    1103\n",
       "train negative    7781\n",
       "      neutral    11118\n",
       "      positive    8582"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"set\",\"sentiment\"])[[\"text\"]].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../results/tokenizers/roberta_tokenizer/vocab.json',\n",
       " '../results/tokenizers/roberta_tokenizer/merges.txt',\n",
       " '../results/tokenizers/roberta_tokenizer/special_tokens_map.json',\n",
       " '../results/tokenizers/roberta_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer.save_pretrained(RESULTS_DIR+\"tokenizers/roberta_tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=RESULTS_DIR+\"tokenizers/roberta_tokenizer/vocab.json\",\n",
    "    merges_file=RESULTS_DIR+\"tokenizers/roberta_tokenizer/merges.txt\",\n",
    "    add_prefix_space=True,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(RESULTS_DIR+\"tokenizers/roberta_tokenizer/special_tokens_map.json\") as f:\n",
    "    special_tokens = json.load(f)\n",
    "\n",
    "tokenizer.add_special_tokens([i for i in special_tokens.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for sentiment detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lookup = {\"positive\":2,\"neutral\":1,\"negative\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': (31015, 100), 'X_att': (31015, 100), 'Y': (31015,), 'VOCAB_SIZE': 50265, 'MAX_SEQ_LEN': 100}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentiment(text_series=data.text.tolist(), sentiment_series=data.sentiment):\n",
    "\n",
    "    X_tokens = tokenizer.encode_batch(text_series)\n",
    "\n",
    "    X = [i.ids for i in X_tokens]\n",
    "    MAX_SEQ_LEN = max([len(i) for i in X])\n",
    "    X = pad_sequences(X, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "    X_att = [i.attention_mask for i in X_tokens]\n",
    "    X_att = pad_sequences(X_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "    Y = sentiment_series.apply(lambda x: sentiment_lookup[x]).values\n",
    "\n",
    "    VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "\n",
    "    print({\n",
    "        \"X\":X.shape,\n",
    "        \"X_att\":X_att.shape,\n",
    "        \"Y\":Y.shape,\n",
    "        \"VOCAB_SIZE\":VOCAB_SIZE,\n",
    "        \"MAX_SEQ_LEN\":MAX_SEQ_LEN\n",
    "    })\n",
    "    \n",
    "    return X_tokens, X, X_att, Y, VOCAB_SIZE, MAX_SEQ_LEN\n",
    "\n",
    "X_sent_tokens, X_sent, X_sent_att, Y_sent, VOCAB_SIZE, MAX_SEQ_LEN_SENT = preprocess_sentiment(**{\n",
    "    \"text_series\" : data.text.tolist(),\n",
    "    \"sentiment_series\" : data.sentiment\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1\n",
      "textID         object  0\n",
      "text           object  1\n",
      "selected_text  object  1\n",
      "sentiment      object  0\n",
      "(27481, 4)\n",
      "{'textID': 27481, 'text': 27480, 'selected_text': 22463, 'sentiment': 3}\n",
      "            textID                                             text  \\\n",
      "count        27481                                            27480   \n",
      "unique       27481                                            27480   \n",
      "top     b5ddf13c3f  Missed the hello kitty  not enough time oh well   \n",
      "freq             1                                                1   \n",
      "\n",
      "       selected_text sentiment  \n",
      "count          27480     27481  \n",
      "unique         22463         3  \n",
      "top             good   neutral  \n",
      "freq             199     11118  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span = pd.read_csv(DATA_DIR+\"train.csv\", encoding=\"utf8\")\n",
    "\n",
    "print(pd.concat((df_span.dtypes, df_span.isna().sum()), axis=1))\n",
    "print(df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:df_span[i].nunique() for i in df_span.columns})\n",
    "print(df_span.describe())\n",
    "df_span.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0  1\n",
      "textID     object  0\n",
      "text       object  0\n",
      "sentiment  object  0\n",
      "(3534, 3)\n",
      "{'textID': 3534, 'text': 3534, 'sentiment': 3}\n",
      "            textID  \\\n",
      "count         3534   \n",
      "unique        3534   \n",
      "top     aa221b6a7d   \n",
      "freq             1   \n",
      "\n",
      "                                                                                                                                       text  \\\n",
      "count                                                                                                                                  3534   \n",
      "unique                                                                                                                                 3534   \n",
      "top     The lab informs me the logic board is dead as well. It is farking expensive to replace. I don`t know what to do.  #MyPowerBookG4RIP   \n",
      "freq                                                                                                                                      1   \n",
      "\n",
      "       sentiment  \n",
      "count       3534  \n",
      "unique         3  \n",
      "top      neutral  \n",
      "freq        1430  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  f87dea47db   \n",
       "1  96d74cb729   \n",
       "\n",
       "                                                                                                      text  \\\n",
       "0                                                        Last session of the day  http://twitpic.com/67ezh   \n",
       "1   Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_span = pd.read_csv(DATA_DIR+\"test.csv\")\n",
    "print(pd.concat((test_df_span.dtypes, test_df_span.isna().sum()), axis=1))\n",
    "print(test_df_span.shape)\n",
    "\n",
    "# Counts of various columns\n",
    "print({i:test_df_span[i].nunique() for i in test_df_span.columns})\n",
    "print(test_df_span.describe())\n",
    "test_df_span.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for span detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For traceability\n",
    "df_span[\"original_index\"] = df_span.index\n",
    "test_df_span[\"original_index\"] = test_df_span.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_idxs = [513, 1060, 5543, 8410, 13605, 24778, 25258] + [18, 27, 32, 39, 48, 49, 64, 66, 84, 92, 102, 116, 129, 132, 134, 149, 160, 166, 168, 189, 196, 197, 210, 223, 247, 251, 260, 285, 295, 296, 297, 309, 349, 362, 366, 382, 396, 398, 406, 410, 425, 458, 460, 482, 492, 500, 504, 517, 533, 537, 543, 569, 573, 581, 604, 605, 608, 634, 636, 637, 639, 642, 665, 670, 674, 678, 685, 702, 707, 709, 710, 727, 737, 746, 751, 757, 768, 778, 787, 794, 804, 809, 853, 854, 856, 863, 871, 872, 912, 931, 937, 942, 950, 956, 957, 963, 992, 993, 999, 1001, 1011, 1012, 1036, 1039, 1057, 1062, 1076, 1077, 1083, 1096, 1105, 1116, 1117, 1122, 1134, 1137, 1150, 1159, 1199, 1200, 1209, 1217, 1264, 1271, 1280, 1283, 1298, 1300, 1303, 1319, 1327, 1342, 1360, 1363, 1365, 1372, 1374, 1376, 1382, 1393, 1416, 1417, 1420, 1447, 1454, 1468, 1472, 1513, 1515, 1518, 1531, 1548, 1558, 1567, 1580, 1588, 1605, 1623, 1648, 1649, 1657, 1659, 1661, 1665, 1672, 1674, 1678, 1683, 1693, 1696, 1713, 1723, 1728, 1739, 1747, 1754, 1798, 1806, 1808, 1814, 1822, 1828, 1830, 1835, 1850, 1857, 1860, 1866, 1888, 1901, 1902, 1929, 1938, 1940, 1987, 1994, 2003, 2004, 2008, 2011, 2023, 2054, 2056, 2065, 2068, 2083, 2086, 2094, 2106, 2113, 2124, 2125, 2136, 2139, 2145, 2168, 2180, 2181, 2187, 2213, 2225, 2227, 2230, 2239, 2273, 2279, 2286, 2288, 2298, 2323, 2324, 2335, 2343, 2359, 2364, 2371, 2385, 2388, 2392, 2398, 2401, 2404, 2410, 2415, 2421, 2430, 2438, 2439, 2440, 2444, 2445, 2448, 2470, 2478, 2489, 2499, 2505, 2515, 2520, 2559, 2574, 2576, 2589, 2598, 2600, 2640, 2643, 2650, 2671, 2679, 2696, 2699, 2706, 2716, 2718, 2731, 2743, 2749, 2750, 2785, 2787, 2789, 2805, 2809, 2825, 2838, 2856, 2888, 2898, 2903, 2923, 2925, 2931, 2933, 2942, 2957, 2961, 2986, 2995, 3034, 3068, 3075, 3077, 3080, 3088, 3089, 3096, 3097, 3101, 3121, 3147, 3160, 3161, 3176, 3182, 3187, 3211, 3225, 3234, 3257, 3263, 3267, 3281, 3290, 3295, 3317, 3319, 3325, 3333, 3359, 3364, 3369, 3374, 3390, 3399, 3400, 3411, 3424, 3473, 3475, 3476, 3479, 3492, 3499, 3504, 3509, 3517, 3519, 3532, 3553, 3578, 3588, 3590, 3593, 3598, 3605, 3606, 3607, 3622, 3634, 3669, 3672, 3678, 3682, 3685, 3694, 3708, 3720, 3729, 3750, 3755, 3784, 3794, 3798, 3801, 3806, 3807, 3813, 3826, 3834, 3872, 3886, 3895, 3903, 3925, 3937, 3946, 3962, 3969, 3998, 4031, 4047, 4053, 4074, 4091, 4093, 4100, 4108, 4113, 4119, 4143, 4148, 4163, 4164, 4183, 4196, 4254, 4260, 4265, 4307, 4308, 4309, 4312, 4318, 4325, 4327, 4328, 4329, 4344, 4347, 4369, 4371, 4377, 4384, 4402, 4404, 4405, 4407, 4412, 4421, 4422, 4437, 4455, 4457, 4471, 4473, 4479, 4483, 4486, 4514, 4532, 4537, 4542, 4543, 4545, 4550, 4554, 4563, 4575, 4576, 4591, 4595, 4631, 4636, 4637, 4639, 4654, 4655, 4704, 4706, 4714, 4718, 4724, 4729, 4735, 4747, 4749, 4750, 4764, 4800, 4814, 4841, 4842, 4844, 4864, 4866, 4869, 4875, 4891, 4908, 4918, 4924, 4928, 4933, 4943, 4946, 4947, 4956, 4968, 4980, 4994, 5009, 5011, 5038, 5056, 5079, 5083, 5101, 5133, 5167, 5173, 5174, 5188, 5189, 5196, 5197, 5213, 5218, 5229, 5241, 5245, 5258, 5279, 5290, 5294, 5297, 5308, 5317, 5331, 5342, 5343, 5353, 5355, 5356, 5358, 5379, 5385, 5386, 5399, 5418, 5433, 5436, 5450, 5456, 5466, 5489, 5504, 5509, 5510, 5530, 5542, 5560, 5583, 5603, 5607, 5618, 5620, 5624, 5626, 5678, 5697, 5712, 5725, 5751, 5757, 5775, 5785, 5799, 5822, 5832, 5836, 5844, 5895, 5902, 5904, 5913, 5918, 5938, 5939, 5961, 5984, 5990, 6025, 6031, 6033, 6060, 6077, 6084, 6090, 6113, 6131, 6134, 6151, 6152, 6176, 6203, 6229, 6230, 6248, 6261, 6277, 6289, 6296, 6304, 6308, 6317, 6319, 6321, 6335, 6338, 6343, 6353, 6356, 6360, 6377, 6382, 6395, 6404, 6425, 6433, 6468, 6474, 6475, 6476, 6477, 6510, 6528, 6540, 6574, 6580, 6582, 6593, 6609, 6627, 6628, 6633, 6635, 6650, 6656, 6657, 6670, 6673, 6676, 6677, 6679, 6686, 6702, 6724, 6740, 6745, 6759, 6801, 6804, 6830, 6833, 6834, 6842, 6859, 6871, 6875, 6878, 6880, 6885, 6896, 6934, 6939, 6948, 6950, 6958, 6970, 6979, 6987, 7010, 7017, 7024, 7040, 7099, 7107, 7112, 7136, 7147, 7149, 7158, 7184, 7225, 7227, 7251, 7274, 7288, 7305, 7306, 7308, 7331, 7344, 7355, 7373, 7407, 7408, 7409, 7410, 7423, 7436, 7438, 7442, 7444, 7450, 7469, 7473, 7479, 7487, 7492, 7506, 7513, 7521, 7528, 7530, 7546, 7567, 7581, 7582, 7597, 7599, 7602, 7632, 7635, 7636, 7642, 7662, 7663, 7701, 7752, 7758, 7765, 7769, 7778, 7780, 7788, 7801, 7816, 7818, 7831, 7836, 7837, 7849, 7893, 7895, 7896, 7922, 7927, 7945, 7948, 7968, 7997, 8003, 8005, 8010, 8030, 8049, 8051, 8052, 8059, 8060, 8085, 8094, 8142, 8143, 8151, 8153, 8161, 8169, 8170, 8196, 8208, 8212, 8215, 8224, 8235, 8241, 8249, 8250, 8276, 8282, 8285, 8286, 8288, 8289, 8299, 8314, 8338, 8343, 8345, 8349, 8373, 8379, 8390, 8397, 8399, 8407, 8427, 8428, 8432, 8439, 8442, 8444, 8484, 8493, 8497, 8500, 8507, 8510, 8534, 8546, 8559, 8569, 8579, 8588, 8594, 8604, 8617, 8620, 8624, 8631, 8638, 8661, 8671, 8674, 8687, 8691, 8696, 8706, 8720, 8729, 8749, 8756, 8758, 8769, 8774, 8780, 8800, 8803, 8809, 8815, 8827, 8830, 8875, 8902, 8906, 8910, 8917, 8919, 8922, 8951, 8954, 8968, 8985, 8986, 8991, 8999, 9014, 9034, 9050, 9057, 9063, 9070, 9088, 9098, 9112, 9113, 9122, 9127, 9140, 9155, 9157, 9169, 9179, 9190, 9216, 9223, 9226, 9264, 9272, 9281, 9297, 9300, 9303, 9322, 9354, 9361, 9374, 9377, 9381, 9385, 9400, 9403, 9419, 9422, 9425, 9428, 9436, 9442, 9449, 9454, 9456, 9458, 9470, 9482, 9496, 9500, 9535, 9539, 9541, 9554, 9556, 9560, 9565, 9574, 9594, 9605, 9610, 9631, 9632, 9660, 9689, 9691, 9696, 9708, 9710, 9712, 9725, 9726, 9734, 9742, 9753, 9770, 9789, 9799, 9801, 9812, 9817, 9836, 9839, 9850, 9852, 9881, 9882, 9891, 9906, 9912, 9920, 9922, 9928, 9976, 9980, 9984, 10002, 10004, 10007, 10029, 10039, 10042, 10050, 10052, 10057, 10067, 10070, 10092, 10093, 10123, 10131, 10138, 10145, 10151, 10164, 10167, 10170, 10179, 10184, 10185, 10190, 10197, 10200, 10211, 10218, 10219, 10222, 10228, 10245, 10247, 10255, 10259, 10276, 10293, 10298, 10299, 10306, 10319, 10327, 10342, 10348, 10377, 10379, 10449, 10465, 10469, 10492, 10504, 10505, 10508, 10521, 10528, 10529, 10530, 10570, 10573, 10577, 10578, 10597, 10611, 10639, 10651, 10666, 10672, 10675, 10690, 10706, 10711, 10738, 10742, 10747, 10749, 10779, 10782, 10783, 10795, 10813, 10849, 10852, 10853, 10884, 10885, 10888, 10892, 10901, 10914, 10918, 10919, 10933, 10935, 10956, 10966, 10968, 10970, 10981, 10986, 10996, 10999, 11005, 11032, 11038, 11051, 11055, 11060, 11067, 11097, 11102, 11155, 11157, 11159, 11162, 11167, 11176, 11187, 11215, 11222, 11228, 11230, 11257, 11263, 11265, 11274, 11286, 11289, 11297, 11300, 11308, 11323, 11337, 11339, 11341, 11347, 11349, 11353, 11371, 11381, 11386, 11426, 11431, 11448, 11449, 11474, 11480, 11500, 11507, 11553, 11562, 11582, 11588, 11594, 11612, 11616, 11618, 11636, 11643, 11650, 11658, 11666, 11679, 11683, 11693, 11695, 11698, 11699, 11706, 11716, 11723, 11730, 11732, 11741, 11745, 11780, 11794, 11798, 11808, 11812, 11828, 11829, 11837, 11840, 11841, 11844, 11849, 11853, 11854, 11855, 11861, 11862, 11872, 11875, 11878, 11886, 11890, 11899, 11905, 11912, 11938, 11963, 11967, 11973, 11980, 11985, 11999, 12026, 12027, 12029, 12039, 12054, 12067, 12115, 12124, 12134, 12136, 12138, 12150, 12187, 12205, 12206, 12224, 12242, 12258, 12269, 12277, 12283, 12295, 12305, 12314, 12322, 12331, 12333, 12334, 12350, 12355, 12356, 12360, 12370, 12372, 12389, 12395, 12396, 12397, 12405, 12410, 12416, 12429, 12439, 12440, 12442, 12446, 12474, 12482, 12486, 12507, 12516, 12521, 12522, 12526, 12527, 12537, 12550, 12563, 12576, 12585, 12586, 12587, 12598, 12606, 12630, 12631, 12635, 12639, 12650, 12657, 12662, 12669, 12687, 12703, 12718, 12724, 12732, 12736, 12745, 12767, 12787, 12803, 12804, 12808, 12818, 12830, 12843, 12855, 12856, 12870, 12884, 12903, 12914, 12920, 12923, 12924, 12927, 12936, 12972, 12977, 12998, 13003, 13004, 13009, 13044, 13068, 13069, 13074, 13080, 13095, 13098, 13124, 13130, 13153, 13154, 13176, 13219, 13222, 13227, 13237, 13267, 13274, 13293, 13301, 13304, 13307, 13330, 13333, 13354, 13365, 13373, 13379, 13381, 13389, 13397, 13422, 13430, 13433, 13445, 13466, 13473, 13476, 13482, 13490, 13493, 13528, 13529, 13535, 13543, 13558, 13559, 13586, 13637, 13643, 13646, 13665, 13674, 13678, 13679, 13704, 13707, 13710, 13718, 13727, 13733, 13747, 13762, 13772, 13785, 13796, 13803, 13812, 13814, 13817, 13827, 13844, 13846, 13847, 13848, 13854, 13855, 13861, 13864, 13873, 13899, 13907, 13936, 13939, 13946, 13949, 13954, 13964, 13965, 13974, 13975, 13978, 13980, 14002, 14006, 14007, 14030, 14043, 14044, 14057, 14058, 14060, 14073, 14091, 14109, 14110, 14112, 14117, 14131, 14132, 14159, 14172, 14173, 14176, 14184, 14194, 14197, 14200, 14204, 14207, 14213, 14230, 14257, 14273, 14275, 14287, 14299, 14301, 14307, 14308, 14347, 14362, 14365, 14381, 14386, 14400, 14416, 14428, 14436, 14446, 14457, 14458, 14462, 14487, 14499, 14509, 14532, 14548, 14551, 14571, 14596, 14598, 14611, 14621, 14639, 14640, 14648, 14662, 14669, 14671, 14676, 14680, 14687, 14689, 14691, 14718, 14731, 14747, 14779, 14787, 14825, 14839, 14844, 14847, 14855, 14870, 14880, 14891, 14908, 14946, 14959, 14960, 14967, 14971, 14986, 15007, 15010, 15016, 15044, 15049, 15055, 15056, 15112, 15131, 15158, 15165, 15174, 15177, 15196, 15203, 15206, 15207, 15213, 15217, 15233, 15236, 15253, 15259, 15286, 15296, 15307, 15308, 15321, 15323, 15327, 15339, 15363, 15372, 15374, 15379, 15380, 15383, 15385, 15392, 15410, 15449, 15452, 15462, 15464, 15465, 15477, 15483, 15503, 15505, 15528, 15532, 15548, 15562, 15566, 15571, 15605, 15610, 15622, 15651, 15660, 15666, 15674, 15709, 15721, 15731, 15735, 15736, 15741, 15767, 15774, 15785, 15792, 15793, 15811, 15851, 15879, 15883, 15884, 15912, 15926, 15927, 15931, 15940, 15959, 15965, 15985, 15998, 16020, 16027, 16031, 16032, 16044, 16070, 16100, 16117, 16129, 16136, 16137, 16140, 16149, 16152, 16175, 16186, 16201, 16241, 16261, 16269, 16272, 16279, 16290, 16308, 16319, 16324, 16325, 16327, 16338, 16341, 16345, 16352, 16354, 16371, 16372, 16385, 16391, 16416, 16423, 16448, 16455, 16468, 16470, 16477, 16481, 16484, 16491, 16493, 16500, 16509, 16510, 16516, 16541, 16559, 16577, 16589, 16632, 16638, 16642, 16643, 16649, 16650, 16654, 16665, 16677, 16684, 16704, 16720, 16726, 16734, 16765, 16774, 16775, 16777, 16780, 16797, 16798, 16799, 16803, 16807, 16809, 16830, 16832, 16861, 16863, 16868, 16876, 16889, 16890, 16902, 16915, 16927, 16939, 16949, 16962, 16968, 17002, 17010, 17013, 17021, 17023, 17029, 17062, 17066, 17084, 17093, 17094, 17102, 17152, 17160, 17167, 17173, 17183, 17221, 17239, 17250, 17267, 17277, 17278, 17282, 17296, 17307, 17315, 17333, 17336, 17344, 17346, 17348, 17354, 17365, 17374, 17385, 17387, 17397, 17401, 17404, 17412, 17417, 17432, 17436, 17444, 17452, 17467, 17469, 17501, 17502, 17513, 17530, 17531, 17547, 17550, 17555, 17573, 17577, 17578, 17586, 17600, 17602, 17611, 17627, 17640, 17645, 17648, 17649, 17652, 17667, 17676, 17682, 17721, 17724, 17729, 17731, 17751, 17754, 17762, 17770, 17772, 17781, 17802, 17820, 17823, 17833, 17848, 17857, 17858, 17865, 17867, 17887, 17893, 17909, 17924, 17926, 17945, 17971, 17977, 17986, 17990, 18003, 18011, 18042, 18046, 18052, 18077, 18086, 18091, 18099, 18103, 18123, 18128, 18131, 18142, 18153, 18165, 18167, 18173, 18181, 18206, 18226, 18252, 18263, 18276, 18277, 18284, 18314, 18319, 18327, 18330, 18342, 18352, 18355, 18362, 18364, 18375, 18383, 18398, 18403, 18432, 18447, 18453, 18465, 18467, 18485, 18486, 18492, 18505, 18508, 18510, 18515, 18536, 18540, 18548, 18550, 18557, 18563, 18570, 18574, 18576, 18616, 18622, 18627, 18639, 18649, 18655, 18673, 18681, 18712, 18745, 18746, 18758, 18773, 18778, 18782, 18826, 18832, 18834, 18842, 18851, 18853, 18858, 18862, 18867, 18872, 18877, 18908, 18910, 18912, 18913, 18919, 18925, 18929, 18930, 18933, 18936, 18944, 18974, 18975, 18997, 19009, 19011, 19014, 19016, 19028, 19057, 19098, 19106, 19114, 19130, 19133, 19147, 19157, 19162, 19166, 19173, 19179, 19198, 19213, 19225, 19230, 19239, 19241, 19266, 19274, 19279, 19283, 19285, 19316, 19327, 19335, 19352, 19358, 19362, 19366, 19377, 19379, 19405, 19423, 19428, 19431, 19437, 19444, 19456, 19469, 19475, 19476, 19481, 19486, 19492, 19506, 19507, 19536, 19537, 19562, 19586, 19589, 19622, 19634, 19651, 19663, 19679, 19685, 19690, 19697, 19707, 19712, 19714, 19715, 19717, 19724, 19734, 19744, 19754, 19769, 19779, 19792, 19807, 19830, 19850, 19863, 19875, 19903, 19904, 19908, 19963, 19985, 19988, 20003, 20012, 20014, 20018, 20040, 20042, 20054, 20078, 20079, 20083, 20087, 20089, 20114, 20121, 20122, 20138, 20155, 20162, 20186, 20199, 20216, 20222, 20234, 20238, 20254, 20285, 20299, 20302, 20305, 20337, 20345, 20364, 20365, 20374, 20382, 20394, 20403, 20440, 20450, 20451, 20476, 20506, 20508, 20516, 20527, 20536, 20540, 20541, 20542, 20557, 20566, 20584, 20608, 20611, 20613, 20618, 20627, 20644, 20656, 20663, 20674, 20682, 20690, 20699, 20707, 20711, 20737, 20751, 20765, 20771, 20774, 20778, 20779, 20789, 20795, 20798, 20834, 20851, 20865, 20867, 20869, 20872, 20883, 20892, 20894, 20895, 20896, 20901, 20919, 20924, 20933, 20937, 20938, 20946, 20947, 20955, 20957, 20958, 20965, 20966, 20970, 20974, 20991, 21001, 21013, 21018, 21020, 21025, 21114, 21142, 21205, 21210, 21227, 21230, 21236, 21253, 21262, 21294, 21300, 21316, 21331, 21339, 21349, 21356, 21365, 21369, 21376, 21399, 21410, 21413, 21418, 21420, 21435, 21447, 21452, 21455, 21468, 21470, 21474, 21486, 21495, 21501, 21517, 21530, 21539, 21546, 21547, 21553, 21556, 21564, 21569, 21579, 21593, 21595, 21599, 21603, 21632, 21643, 21690, 21713, 21726, 21732, 21737, 21738, 21744, 21755, 21773, 21782, 21800, 21807, 21829, 21839, 21848, 21876, 21893, 21923, 21926, 21931, 21958, 21967, 21970, 21973, 21982, 22002, 22029, 22031, 22035, 22044, 22051, 22068, 22073, 22099, 22117, 22121, 22126, 22145, 22146, 22149, 22161, 22166, 22170, 22177, 22205, 22210, 22231, 22233, 22234, 22244, 22246, 22251, 22272, 22280, 22299, 22317, 22321, 22327, 22350, 22355, 22363, 22365, 22378, 22383, 22387, 22391, 22393, 22395, 22403, 22406, 22418, 22423, 22428, 22437, 22444, 22451, 22453, 22460, 22480, 22491, 22510, 22530, 22536, 22554, 22557, 22561, 22562, 22564, 22571, 22588, 22593, 22606, 22607, 22610, 22611, 22642, 22666, 22717, 22729, 22738, 22744, 22745, 22763, 22768, 22769, 22774, 22782, 22789, 22796, 22800, 22836, 22841, 22843, 22858, 22864, 22886, 22889, 22897, 22925, 22928, 22938, 22944, 22945, 22948, 22950, 22956, 22983, 22986, 22988, 22993, 22994, 22997, 23028, 23030, 23039, 23047, 23081, 23091, 23097, 23105, 23108, 23111, 23113, 23133, 23145, 23150, 23161, 23172, 23176, 23199, 23200, 23202, 23204, 23205, 23211, 23212, 23226, 23227, 23231, 23236, 23239, 23263, 23265, 23266, 23269, 23290, 23311, 23315, 23320, 23323, 23340, 23352, 23361, 23372, 23375, 23376, 23381, 23386, 23390, 23401, 23415, 23418, 23428, 23429, 23453, 23467, 23479, 23505, 23508, 23527, 23528, 23535, 23537, 23541, 23553, 23555, 23557, 23559, 23563, 23566, 23581, 23585, 23596, 23598, 23604, 23605, 23617, 23630, 23643, 23658, 23669, 23673, 23680, 23683, 23690, 23691, 23701, 23707, 23714, 23720, 23725, 23733, 23746, 23750, 23751, 23769, 23774, 23784, 23809, 23818, 23831, 23839, 23842, 23857, 23924, 23930, 23944, 23945, 23947, 23959, 23971, 23975, 23987, 23989, 24002, 24003, 24009, 24019, 24026, 24039, 24041, 24046, 24062, 24071, 24077, 24079, 24087, 24096, 24119, 24121, 24137, 24150, 24169, 24181, 24194, 24195, 24199, 24200, 24203, 24210, 24223, 24258, 24272, 24274, 24291, 24293, 24300, 24305, 24318, 24339, 24378, 24380, 24383, 24392, 24402, 24442, 24468, 24476, 24480, 24488, 24490, 24495, 24502, 24504, 24505, 24516, 24562, 24572, 24594, 24595, 24597, 24600, 24606, 24609, 24622, 24623, 24629, 24631, 24632, 24633, 24657, 24662, 24664, 24666, 24682, 24697, 24709, 24753, 24758, 24766, 24833, 24840, 24851, 24858, 24869, 24886, 24909, 24915, 24916, 24929, 24945, 24951, 24966, 24972, 24980, 24996, 25012, 25017, 25043, 25059, 25067, 25104, 25121, 25125, 25127, 25133, 25150, 25191, 25210, 25224, 25228, 25240, 25243, 25255, 25264, 25267, 25272, 25282, 25284, 25290, 25292, 25293, 25334, 25338, 25350, 25365, 25368, 25380, 25389, 25391, 25400, 25413, 25422, 25440, 25444, 25446, 25456, 25460, 25472, 25486, 25495, 25499, 25509, 25512, 25528, 25530, 25532, 25547, 25559, 25591, 25593, 25597, 25601, 25614, 25616, 25638, 25672, 25691, 25698, 25712, 25713, 25721, 25722, 25725, 25732, 25753, 25760, 25775, 25794, 25813, 25819, 25828, 25833, 25901, 25908, 25947, 25983, 25987, 25989, 25996, 26008, 26017, 26025, 26030, 26039, 26047, 26054, 26058, 26077, 26088, 26092, 26112, 26183, 26185, 26190, 26201, 26204, 26237, 26239, 26241, 26248, 26251, 26252, 26256, 26263, 26268, 26282, 26314, 26331, 26346, 26358, 26360, 26364, 26377, 26428, 26434, 26460, 26481, 26485, 26497, 26532, 26533, 26538, 26561, 26567, 26585, 26588, 26595, 26596, 26618, 26625, 26633, 26643, 26651, 26677, 26687, 26715, 26721, 26732, 26733, 26746, 26760, 26762, 26769, 26779, 26781, 26822, 26828, 26830, 26870, 26882, 26883, 26889, 26892, 26894, 26914, 26927, 26932, 26944, 26953, 26957, 26963, 26967, 26973, 26976, 26978, 26985, 27030, 27038, 27044, 27048, 27052, 27067, 27111, 27115, 27121, 27142, 27144, 27153, 27167, 27205, 27209, 27217, 27218, 27229, 27233, 27240, 27250, 27280, 27302, 27332, 27339, 27349, 27362, 27376, 27386, 27396, 27401, 27413, 27426, 27429, 27456, 27470, 27474, 27476, 27477, 27480]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27481, 5)\n",
      "(24857, 5)\n",
      "(24856, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df_span.shape)\n",
    "df_span = df_span[(~df_span.index.isin(anomalous_idxs))]\n",
    "print(df_span.shape)\n",
    "df_span = df_span[(~df_span.text.isna())]\n",
    "df_span = df_span.reset_index(drop=True)\n",
    "df_span = df_span.copy()\n",
    "print(df_span.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"sentiment_code\"] = df_span[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments = df_span[\"sentiment_code\"].cat.codes.values\n",
    "\n",
    "test_df_span[\"sentiment_code\"] = test_df_span[\"sentiment\"].astype(\"category\")\n",
    "X_sentiments_test = test_df_span[\"sentiment_code\"].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text\"] = df_span[\"text\"].astype(str)\n",
    "df_span[\"selected_text\"] = df_span[\"selected_text\"].astype(str)\n",
    "test_df_span[\"text\"] = test_df_span[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_span[\"text_mod\"] = \"<s> \" + df_span.text.str.strip() + \" </s> \" + df_span.sentiment + \" </s>\"\n",
    "test_df_span[\"text_mod\"] = \"<s> \" + test_df_span.text.str.strip() + \" </s> \" + test_df_span.sentiment + \" </s>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclusions for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCLUDE_NEUTRAL_CLASS: (14017, 7)\n"
     ]
    }
   ],
   "source": [
    "if EXCLUDE_NEUTRAL_CLASS:\n",
    "    df_span = df_span.loc[df_span.sentiment!=\"neutral\"].copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"EXCLUDE_NEUTRAL_CLASS:\", df_span.shape)\n",
    "\n",
    "if RUN_ON_SAMPLE:\n",
    "    df_span = df_span.sample(1000).copy()\n",
    "    df_span = df_span.reset_index(drop=True)\n",
    "    print(\"Train RUN_ON_SAMPLE\", df_span.shape)\n",
    "    test_df_span = test_df_span.sample(1000).copy()\n",
    "    test_df_span = test_df_span.reset_index(drop=True)\n",
    "    print(\"Test  RUN_ON_SAMPLE\", test_df_span.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_tokens = tokenizer.encode_batch(df_span.text_mod.tolist())\n",
    "Y_span_tokens = tokenizer.encode_batch(df_span.selected_text.tolist())\n",
    "X_span_tokens_test = tokenizer.encode_batch(test_df_span.text_mod.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = [i.ids for i in X_span_tokens]\n",
    "Y_span = [i.ids for i in Y_span_tokens]\n",
    "X_span_test = [i.ids for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span_att = [i.attention_mask for i in X_span_tokens]\n",
    "Y_span_att = [i.attention_mask for i in Y_span_tokens]\n",
    "X_span_att_test = [i.attention_mask for i in X_span_tokens_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN_SPAN = max([len(i) for i in X_span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extremities(l_string, s_string, print_it=False):\n",
    "    len_l = len(l_string)\n",
    "    len_s = len(s_string)\n",
    "    \n",
    "    for i in range(len_l - len_s + 1):\n",
    "        if (i + len_s) <= len_l:\n",
    "            substring = l_string[i:i+len_s]\n",
    "            if substring == s_string:\n",
    "                if print_it:\n",
    "                    print(l_string)\n",
    "                    print(substring)\n",
    "                    print(i, i+len_s, substring)\n",
    "                \n",
    "                start_vector, end_vector = [0] * len_l, [0] * len_l\n",
    "                start_vector[i], end_vector[i+len_s-1] = 1, 1\n",
    "                \n",
    "                return (start_vector, end_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14017 \t: #Processed\n",
      "0 \t: # of Anomalies\n"
     ]
    }
   ],
   "source": [
    "Y_span_starts, Y_span_stops = [], []\n",
    "anomaly_idx, counter = [], 0\n",
    "for num, (i,j) in enumerate(zip(X_span_tokens, Y_span_tokens)):\n",
    "    x,y = i.ids, j.ids\n",
    "    try:\n",
    "        s,e = get_extremities(x, y)\n",
    "    except TypeError as t:\n",
    "        counter += 1\n",
    "        anomaly_idx.append(num)\n",
    "    Y_span_starts.append(s)\n",
    "    Y_span_stops.append(e)\n",
    "print(num + 1, \"\\t: #Processed\")\n",
    "\n",
    "print(counter,\"\\t: # of Anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if counter:\n",
    "    print(anomaly_idx, sep=\",\")\n",
    "    print(df_span[\"original_index\"].iloc[anomaly_idx].tolist())\n",
    "    df_span.iloc[anomaly_idx].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcfly gig last nightt omg it was amazin didnt sit down through the whole thing  mcfly did you see me and ma best mate we were in tutus\n",
      "it was amazin\n",
      "[['<s>', 0, 0, 0], ['Ġmc', 44355, 0, 0], ['fly', 19252, 0, 0], ['Ġgig', 10196, 0, 0], ['Ġlast', 94, 0, 0], ['Ġnight', 363, 0, 0], ['t', 90, 0, 0], ['Ġo', 1021, 0, 0], ['mg', 22984, 0, 0], ['Ġit', 24, 1, 0], ['Ġwas', 21, 0, 0], ['Ġamaz', 42402, 0, 0], ['in', 179, 0, 1], ['Ġdidnt', 46405, 0, 0], ['Ġsit', 2662, 0, 0], ['Ġdown', 159, 0, 0], ['Ġthrough', 149, 0, 0], ['Ġthe', 5, 0, 0], ['Ġwhole', 1086, 0, 0], ['Ġthing', 631, 0, 0], ['Ġ', 1437, 0, 0], ['Ġmc', 44355, 0, 0], ['fly', 19252, 0, 0], ['Ġdid', 222, 0, 0], ['Ġyou', 47, 0, 0], ['Ġsee', 192, 0, 0], ['Ġme', 162, 0, 0], ['Ġand', 8, 0, 0], ['Ġma', 9131, 0, 0], ['Ġbest', 275, 0, 0], ['Ġmate', 12563, 0, 0], ['Ġwe', 52, 0, 0], ['Ġwere', 58, 0, 0], ['Ġin', 11, 0, 0], ['Ġtut', 15511, 0, 0], ['us', 687, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0], ['Ġpositive', 1313, 0, 0], ['Ġ', 1437, 0, 0], ['</s>', 2, 0, 0]]\n",
      "[[24, 'Ġit'], [21, 'Ġwas'], [42402, 'Ġamaz'], [179, 'in']]\n"
     ]
    }
   ],
   "source": [
    "check_idx = 156\n",
    "print(df_span.text[check_idx])\n",
    "print(df_span.selected_text[check_idx])\n",
    "print([[i,j,k,l] for i,j,k,l in zip(X_span_tokens[check_idx].tokens,\n",
    "                                    X_span_tokens[check_idx].ids,\n",
    "                                    Y_span_starts[check_idx],\n",
    "                                    Y_span_stops[check_idx])])\n",
    "print([[i,j] for i,j in zip(Y_span_tokens[check_idx].ids,\n",
    "                            Y_span_tokens[check_idx].tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "\n",
    "Y_span_starts = pad_sequences(Y_span_starts, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\").argmax(axis=1)\n",
    "Y_span_stops = pad_sequences(Y_span_stops, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\").argmax(axis=1)\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN_SPAN, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAX_SEQ_LEN_SPAN': 76,\n",
      " 'VOCAB_SIZE': 50265,\n",
      " 'X_span': (14017, 76),\n",
      " 'X_span_att': (14017, 76),\n",
      " 'X_span_att_test': (3534, 76),\n",
      " 'X_span_test': (3534, 76),\n",
      " 'Y_span': (14017, 76),\n",
      " 'Y_span_starts': (14017,),\n",
      " 'Y_span_stops': (14017,)}\n"
     ]
    }
   ],
   "source": [
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \"Y_span_starts\" : Y_span_starts.shape,\n",
    "    \"Y_span_stops\" : Y_span_stops.shape,\n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "    \"VOCAB_SIZE\":VOCAB_SIZE,\n",
    "    \"MAX_SEQ_LEN_SPAN\":MAX_SEQ_LEN_SPAN\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_flag = np.isin(Y_span_stops,\n",
    "                    np.unique(Y_span_stops,\n",
    "                              return_counts=True)[0][np.unique(Y_span_stops,\n",
    "                                                               return_counts=True)[1]>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (14017, 76) \t: X  \n",
      " (14017, 76) \t: X_att  \n",
      " (14017, 76) \t: Y  \n",
      " (14017,) \t: Y_starts  \n",
      " (14017,) \t: Y_stops  \n",
      " (3534, 76) \t: X_test  \n",
      " (3534, 76) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_span = X_span[keep_flag]\n",
    "X_span_att = X_span_att[keep_flag]\n",
    "Y_span = Y_span[keep_flag]\n",
    "Y_span_starts = Y_span_starts[keep_flag]\n",
    "Y_span_stops = Y_span_stops[keep_flag]\n",
    "X_span_test = X_span_test\n",
    "X_span_att_test = X_span_att_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (14015, 76) \t: X  \n",
      " (14015, 76) \t: X_att  \n",
      " (14015, 76) \t: Y  \n",
      " (14015,) \t: Y_starts  \n",
      " (14015,) \t: Y_stops  \n",
      " (3534, 76) \t: X_test  \n",
      " (3534, 76) \t: X_att_test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",\n",
    "     X_span.shape, \"\\t: X \", \"\\n\",\n",
    "     X_span_att.shape, \"\\t: X_att \", \"\\n\",\n",
    "     Y_span.shape, \"\\t: Y \", \"\\n\",\n",
    "     Y_span_starts.shape, \"\\t: Y_starts \", \"\\n\",\n",
    "     Y_span_stops.shape, \"\\t: Y_stops \", \"\\n\",\n",
    "     X_span_test.shape, \"\\t: X_test \", \"\\n\",\n",
    "     X_span_att_test.shape, \"\\t: X_att_test \", \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sss = StratifiedKFold(n_splits=NUM_FOLDS,shuffle=True,random_state=seeded_value)\n",
    "sss = KFold(n_splits=NUM_FOLDS,shuffle=True,random_state=seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_span_words = [tokenizer.decode(i) for i in Y_span]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = max(MAX_SEQ_LEN_SENT, MAX_SEQ_LEN_SPAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X_sent': (31015, 100),\n",
      " 'X_sent_att': (31015, 100),\n",
      " 'X_span': (14015, 100),\n",
      " 'X_span_att': (14015, 100),\n",
      " 'X_span_att_test': (3534, 100),\n",
      " 'X_span_test': (3534, 100),\n",
      " 'Y_span': (14015, 100)}\n"
     ]
    }
   ],
   "source": [
    "X_span = pad_sequences(X_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att = pad_sequences(X_span_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "Y_span = pad_sequences(Y_span, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "X_span_test = pad_sequences(X_span_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_span_att_test = pad_sequences(X_span_att_test, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "X_sent = pad_sequences(X_sent, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "X_sent_att = pad_sequences(X_sent_att, maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
    "\n",
    "pprint({\n",
    "    \"X_span\" : X_span.shape,\n",
    "    \"X_span_att\" : X_span_att.shape,\n",
    "    \"Y_span\" : Y_span.shape,\n",
    "    \n",
    "    \"X_span_test\" : X_span_test.shape,\n",
    "    \"X_span_att_test\" : X_span_att_test.shape,\n",
    "    \n",
    "    \"X_sent\" : X_sent.shape,\n",
    "    \"X_sent_att\" : X_sent_att.shape,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_sequences = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"words\")\n",
    "    input_att_flags = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"att_flags\")\n",
    "    input_token_ids = Input((MAX_SEQ_LEN), dtype=tf.int32, name=\"token_ids\")\n",
    "    \n",
    "    config = RobertaConfig.from_pretrained('roberta-base')\n",
    "    roberta_model = TFRobertaModel.from_pretrained('roberta-base', config=config)\n",
    "    x = roberta_model(inputs=input_sequences, attention_mask=input_att_flags, token_type_ids=input_token_ids)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(DROPOUT)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    output_starts_0 = tf.keras.layers.Activation('softmax', name=\"starts_0\")(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(DROPOUT)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    output_stops_0 = tf.keras.layers.Activation('softmax', name=\"stops_0\")(x2)\n",
    "    \n",
    "    x3 = tf.keras.layers.Dropout(DROPOUT)(x[0]) \n",
    "    x3 = tf.keras.layers.Conv1D(768, 2,padding='same')(x3)\n",
    "    x3 = tf.keras.layers.LeakyReLU()(x3)\n",
    "    x3 = tf.keras.layers.Dense(1)(x3)\n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "    x3 = tf.keras.layers.Dense(3)(x3)\n",
    "    output_sentiment = tf.keras.layers.Activation('softmax', name=\"output_sentiments\")(x3)\n",
    "    \n",
    "    sentiment_model = Model([input_att_flags, input_sequences, input_token_ids], [output_sentiment])\n",
    "    \n",
    "    output_subtract = tf.keras.layers.Subtract()([output_starts_0, output_stops_0])\n",
    "    output_flat = concatenate([output_starts_0, output_stops_0, output_subtract, output_sentiment])\n",
    "    output_starts_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"starts_1\")(output_flat)\n",
    "    output_stops_1 = Dense(MAX_SEQ_LEN, activation='softmax', name=\"stops_1\")(output_flat)\n",
    "    \n",
    "    span_detection_model = Model([input_att_flags, input_sequences, input_token_ids],\n",
    "                                 [output_starts_0, output_stops_0, output_starts_1, output_stops_1])\n",
    "    \n",
    "    return sentiment_model, span_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model, span_detection_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 100, 768)     0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 100, 768)     1180416     dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 100, 768)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100, 1)       769         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            303         flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_sentiments (Activation)  (None, 3)            0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 125,827,120\n",
      "Trainable params: 125,827,120\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 100, 768)     0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 100, 768)     0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 100, 768)     0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 100, 768)     1180416     dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 100, 768)     1180416     dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 100, 768)     1180416     dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 100, 768)     0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 100, 768)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 100, 768)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100, 1)       769         leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100, 1)       769         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100, 1)       769         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "starts_0 (Activation)           (None, 100)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "stops_0 (Activation)            (None, 100)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            303         flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 100)          0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_sentiments (Activation)  (None, 3)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 303)          0           starts_0[0][0]                   \n",
      "                                                                 stops_0[0][0]                    \n",
      "                                                                 subtract[0][0]                   \n",
      "                                                                 output_sentiments[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "starts_1 (Dense)                (None, 100)          30400       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "stops_1 (Dense)                 (None, 100)          30400       concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 128,250,290\n",
      "Trainable params: 128,250,290\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "span_detection_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mcp = ModelCheckpoint(filepath=RESULTS_DIR+\"Sentiment_\"+MODEL_PREFIX+\"BestCheckpoint.h5\", monitor='val_loss',\n",
    "                                verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "sentiment_csvl = CSVLogger(filename=RESULTS_DIR+\"Sentiment_\"+MODEL_PREFIX+\"_LossLogs.csv\",\n",
    "                           separator=\",\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_index, va_index, _, _ = train_test_split(np.arange(X_sent.shape[0]),\n",
    "                                            np.arange(Y_sent.shape[0]),\n",
    "                                            test_size=TRAIN_SPLIT_RATIO,\n",
    "                                            random_state=seeded_value,\n",
    "                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,c = np.unique(Y_sent, return_counts=True)\n",
    "cw = class_weight.compute_class_weight('balanced', np.unique(Y_sent), Y_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model.layers[3].trainable = False\n",
    "adam = Adam(learning_rate=SENTIMENT_MAX_LR)\n",
    "sentiment_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                        optimizer=adam,\n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_flags (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_ids (InputLayer)          [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 100, 768)     0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 100, 768)     1180416     dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 100, 768)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100, 1)       769         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            303         flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_sentiments (Activation)  (None, 3)            0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 125,827,120\n",
      "Trainable params: 1,181,488\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24812 samples, validate on 6203 samples\n",
      "Epoch 1/5\n",
      "24812/24812 [==============================] - 293s 12ms/sample - loss: 1.0098 - accuracy: 0.4847 - val_loss: 0.8771 - val_accuracy: 0.5659\n",
      "Epoch 2/5\n",
      "24812/24812 [==============================] - 274s 11ms/sample - loss: 0.8975 - accuracy: 0.5652 - val_loss: 0.8576 - val_accuracy: 0.5771\n",
      "Epoch 3/5\n",
      "24812/24812 [==============================] - 274s 11ms/sample - loss: 0.8614 - accuracy: 0.5931 - val_loss: 0.7767 - val_accuracy: 0.6497\n",
      "Epoch 4/5\n",
      "24812/24812 [==============================] - 274s 11ms/sample - loss: 0.8396 - accuracy: 0.6121 - val_loss: 0.7656 - val_accuracy: 0.6618\n",
      "Epoch 5/5\n",
      "24812/24812 [==============================] - 276s 11ms/sample - loss: 0.8258 - accuracy: 0.6242 - val_loss: 0.7777 - val_accuracy: 0.6565\n"
     ]
    }
   ],
   "source": [
    "sent_history = sentiment_model.fit(x={\"att_flags\":X_sent_att[tr_index],\n",
    "                                      \"words\":X_sent[tr_index],\n",
    "                                      \"token_ids\":np.zeros_like(X_sent_att[tr_index])},\n",
    "                                   y={\"output_sentiments\":Y_sent[tr_index]},\n",
    "                                   shuffle=True,\n",
    "                                   batch_size=BATCH_SIZE,\n",
    "                                   epochs=SENTIMENT_NUM_EPOCHS[0],\n",
    "                                   validation_data=({\"att_flags\":X_sent_att[va_index],\n",
    "                                                     \"words\":X_sent[va_index],\n",
    "                                                     \"token_ids\":np.zeros_like(X_sent_att[va_index])},\n",
    "                                                    {\"output_sentiments\":Y_sent[va_index]}),\n",
    "                                   verbose=1,\n",
    "                                   class_weight=cw,\n",
    "                                   callbacks=[sentiment_mcp, sentiment_csvl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model.layers[3].trainable = True\n",
    "adam = Adam(learning_rate=SENTIMENT_MIN_LR)\n",
    "sentiment_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                        optimizer=adam,\n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24812 samples, validate on 6203 samples\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "24812/24812 [==============================] - 653s 26ms/sample - loss: 0.6856 - accuracy: 0.7135 - val_loss: 0.5921 - val_accuracy: 0.7595\n",
      "Epoch 2/5\n",
      "24812/24812 [==============================] - 638s 26ms/sample - loss: 0.5885 - accuracy: 0.7563 - val_loss: 0.5586 - val_accuracy: 0.7706\n",
      "Epoch 3/5\n",
      "24812/24812 [==============================] - 637s 26ms/sample - loss: 0.5327 - accuracy: 0.7863 - val_loss: 0.5681 - val_accuracy: 0.7712\n",
      "Epoch 4/5\n",
      "24812/24812 [==============================] - 635s 26ms/sample - loss: 0.4721 - accuracy: 0.8078 - val_loss: 0.6016 - val_accuracy: 0.7787\n",
      "Epoch 5/5\n",
      "24812/24812 [==============================] - 635s 26ms/sample - loss: 0.4262 - accuracy: 0.8306 - val_loss: 0.5643 - val_accuracy: 0.7733\n"
     ]
    }
   ],
   "source": [
    "sent_history_finetuned = sentiment_model.fit(x={\"att_flags\":X_sent_att[tr_index],\n",
    "                                                \"words\":X_sent[tr_index],\n",
    "                                                \"token_ids\":np.zeros_like(X_sent_att[tr_index])},\n",
    "                                             y={\"output_sentiments\":Y_sent[tr_index]},\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             epochs=SENTIMENT_NUM_EPOCHS[1],\n",
    "                                             validation_data=({\"att_flags\":X_sent_att[va_index],\n",
    "                                                               \"words\":X_sent[va_index],\n",
    "                                                               \"token_ids\":np.zeros_like(X_sent_att[va_index])},\n",
    "                                                              {\"output_sentiments\":Y_sent[va_index]}),\n",
    "                                             verbose=1,\n",
    "                                             class_weight=cw,\n",
    "                                             callbacks=[sentiment_mcp, sentiment_csvl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'predicted_sentiment': 'negative',\n",
      "     'text': 'Going out to Miranda shopping centre to spend time with the '\n",
      "             'family, before going away for 2 weeks to Malaysia. Gonna miss '\n",
      "             'them!'},\n",
      " 1: {'predicted_sentiment': 'neutral',\n",
      "     'text': '  Now I need to find the Keynote one! At least I know where to '\n",
      "             'go! #ScreenCastsOnline'},\n",
      " 2: {'predicted_sentiment': 'positive',\n",
      "     'text': 'relaxing night at home with best people'},\n",
      " 3: {'predicted_sentiment': 'negative',\n",
      "     'text': 'Watching Ace of Cakes: LOST edition omfgggg'},\n",
      " 4: {'predicted_sentiment': 'negative',\n",
      "     'text': 'doneeee wheeee hahaaaaaaaa so tired and sleepy  peter u suck not '\n",
      "             'coming to my bday!'}}\n"
     ]
    }
   ],
   "source": [
    "sample_text = data.text.sample(5).tolist()\n",
    "\n",
    "encoded_repr = tokenizer.encode_batch(sample_text)\n",
    "\n",
    "sample_text_ids = pad_sequences([i.ids for i in encoded_repr],\n",
    "                                maxlen=MAX_SEQ_LEN,\n",
    "                                padding=\"post\")\n",
    "sample_text_att = pad_sequences([i.attention_mask for i in encoded_repr],\n",
    "                                maxlen=MAX_SEQ_LEN,\n",
    "                                padding=\"post\")\n",
    "pred = sentiment_model.predict({\"att_flags\":sample_text_att,\n",
    "                                \"words\":sample_text_ids,\n",
    "                                \"token_ids\":np.zeros_like(sample_text_att)})\n",
    "\n",
    "pprint({\n",
    "    num:{\n",
    "        \"text\":i,\n",
    "        \"predicted_sentiment\":[k for k,v in sentiment_lookup.items() if v==j][0]\n",
    "    } for num,(i,j) in enumerate(zip(sample_text, pred.argmax(axis=1)))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_sentiment(x):\n",
    "    encoded_repr = tokenizer.encode_batch(x.tolist())\n",
    "\n",
    "    sample_text_ids = pad_sequences([i.ids for i in encoded_repr],\n",
    "                                    maxlen=MAX_SEQ_LEN,\n",
    "                                    padding=\"post\")\n",
    "    sample_text_att = pad_sequences([i.attention_mask for i in encoded_repr],\n",
    "                                    maxlen=MAX_SEQ_LEN,\n",
    "                                    padding=\"post\")\n",
    "    pred = sentiment_model.predict({\"att_flags\":sample_text_att,\n",
    "                                    \"words\":sample_text_ids,\n",
    "                                    \"token_ids\":np.zeros_like(sample_text_att)},\n",
    "                                  batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    res = pd.DataFrame({\"predicted_sentiment\":pred.argmax(axis=1)})\n",
    "    \n",
    "    return res.predicted_sentiment.apply(lambda x:[k for k,v in sentiment_lookup.items() if v==x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data[\"predicted_sentiment\"] = infer_sentiment(x=data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.90      0.90      7047\n",
      "     neutral       0.87      0.88      0.87     10041\n",
      "    positive       0.92      0.90      0.91      7724\n",
      "\n",
      "    accuracy                           0.89     24812\n",
      "   macro avg       0.90      0.89      0.90     24812\n",
      "weighted avg       0.89      0.89      0.89     24812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=data.sentiment[data.index.isin(tr_index)],\n",
    "                            y_pred=data.predicted_sentiment[data.index.isin(tr_index)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.78      0.78      1735\n",
      "     neutral       0.73      0.74      0.74      2507\n",
      "    positive       0.83      0.81      0.82      1961\n",
      "\n",
      "    accuracy                           0.77      6203\n",
      "   macro avg       0.78      0.78      0.78      6203\n",
      "weighted avg       0.77      0.77      0.77      6203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=data.sentiment[data.index.isin(va_index)],\n",
    "                            y_pred=data.predicted_sentiment[data.index.isin(va_index)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8557,  1023,   105],\n",
       "       [  848, 10698,  1002],\n",
       "       [   85,  1003,  7694]], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=data.sentiment,\n",
    "                 y_pred=data.predicted_sentiment,\n",
    "                 labels=['positive', 'neutral', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27589876, 0.03298404, 0.00338546],\n",
       "       [0.02734161, 0.34492987, 0.03230695],\n",
       "       [0.00274061, 0.03233919, 0.24807351]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=data.sentiment,\n",
    "                 y_pred=data.predicted_sentiment,\n",
    "                 labels=['positive', 'neutral', 'negative'],\n",
    "                 normalize=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set2\n",
       "train    0.892794\n",
       "valid    0.773335\n",
       "dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"set2\"] = np.where(data.index.isin(tr_index), \"train\", \"valid\")\n",
    "data.groupby(\"set2\").apply(lambda x : accuracy_score(y_true=x.sentiment, y_pred=x.predicted_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set\n",
       "test     0.874363\n",
       "train    0.868200\n",
       "dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(\"set\").apply(lambda x : accuracy_score(y_true=x.sentiment, y_pred=x.predicted_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set    set2 \n",
       "test   train     2811\n",
       "       valid      723\n",
       "train  train    22001\n",
       "       valid     5480\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"set\", \"set2\"])[\"sentiment\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set    set2 \n",
       "test   train    0.899324\n",
       "       valid    0.777317\n",
       "train  train    0.891959\n",
       "       valid    0.772810\n",
       "dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"set\", \"set2\"]).apply(lambda x : accuracy_score(y_true=x.sentiment, y_pred=x.predicted_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 2, 'neutral': 1, 'negative': 0}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>set</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "      <th>set2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i miss jack n box  and whataburger  and oooo taco cabanaaaaaa  lmao</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Byebye</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Doggone it! I only got an hour nap in  I really want some Ponderosa chicken wings!</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>My radiator boiled out</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>_precious06 sooo mad</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30723</th>\n",
       "      <td>LOL that is so Charlie......I miss Charlie</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30887</th>\n",
       "      <td>broke my gorilla pod  and I think one Lego Knight lost his helmet</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30979</th>\n",
       "      <td>mama the boys gon be sweating her hard in Pre K  lol muah</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30989</th>\n",
       "      <td>Sitting at my desk wishing things were different</td>\n",
       "      <td>train</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30991</th>\n",
       "      <td>I didn`t get a callback for the play I cried...  oh well I guess it`s back to being a techie..</td>\n",
       "      <td>test</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1002 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 text  \\\n",
       "0                                 i miss jack n box  and whataburger  and oooo taco cabanaaaaaa  lmao   \n",
       "64                                                                                             Byebye   \n",
       "81                 Doggone it! I only got an hour nap in  I really want some Ponderosa chicken wings!   \n",
       "112                                                                            My radiator boiled out   \n",
       "173                                                                              _precious06 sooo mad   \n",
       "...                                                                                               ...   \n",
       "30723                                                      LOL that is so Charlie......I miss Charlie   \n",
       "30887                               broke my gorilla pod  and I think one Lego Knight lost his helmet   \n",
       "30979                                       mama the boys gon be sweating her hard in Pre K  lol muah   \n",
       "30989                                                Sitting at my desk wishing things were different   \n",
       "30991  I didn`t get a callback for the play I cried...  oh well I guess it`s back to being a techie..   \n",
       "\n",
       "         set sentiment predicted_sentiment   set2  \n",
       "0      train   neutral            negative  train  \n",
       "64     train   neutral            negative  train  \n",
       "81     train   neutral            negative  train  \n",
       "112    train   neutral            negative  train  \n",
       "173    train   neutral            negative  train  \n",
       "...      ...       ...                 ...    ...  \n",
       "30723  train   neutral            negative  train  \n",
       "30887  train   neutral            negative  train  \n",
       "30979  train   neutral            negative  train  \n",
       "30989  train   neutral            negative  valid  \n",
       "30991   test   neutral            negative  train  \n",
       "\n",
       "[1002 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[(data.sentiment == \"neutral\") & (data.predicted_sentiment == \"negative\")] # most incorrect in this cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span Detection Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_FOLDS):\n",
    "    if os.path.exists(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\"):\n",
    "        os.remove(RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(i)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(pred_dict):\n",
    "    print(\"[INFO] \",\"=\"*15,\"Validation for FOLD#\", num, \"=\"*15)\n",
    "    funcs = [accuracy_score, f1_score, precision_score, recall_score, confusion_matrix]\n",
    "    for f in funcs:\n",
    "        for data_set in [\"train\",\"valid\"]:\n",
    "            for var in [\"starts\", \"stops\"]:\n",
    "                if f in [accuracy_score]:\n",
    "                    res = f(**pred_dict[data_set][var])\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "                elif f in [confusion_matrix]:\n",
    "                    res = f(**pred_dict[data_set][var], labels=np.arange(MAX_SEQ_LEN))\n",
    "                    np.savetxt(X=res, fmt='%i', delimiter=\",\",\n",
    "                               fname=RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                    print(\"[INFO] \\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__, \"\\t||\", \n",
    "                          RESULTS_DIR+\"ConfusionMatrix_\"+MODEL_PREFIX+\"_\"+data_set+\"_\"+var+\".csv\")\n",
    "                else:\n",
    "                    res = f(**pred_dict[data_set][var], average=\"macro\")\n",
    "                    print(\"[INFO] {:.2f}\".format(100 * res), \"\\t||\", data_set, \"\\t||\", var, \"\\t||\", f.__name__)\n",
    "        print(\"=======================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(string):\n",
    "    string = re.sub(pattern=\" (negative|positive|neutral)[ ]+$\", repl=\"\", string=string)\n",
    "    string = re.sub(pattern=\" +\", repl=\" \", string=string.strip())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><h2 id=\"finetuning\">Fine-tuning</h2></a>\n",
    "<p>Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.</p>\n",
    " <p>This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.</p>\n",
    " <p>It is critical to only do this step <em>after</em> the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.</p>\n",
    " <p>It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>\n",
    "\n",
    "<a href=\"https://keras.io/guides/transfer_learning/#finetuning\" target=\"_blank\"><p><strong>Important note about <code>compile()</code> and <code>trainable</code></strong></p></a>\n",
    "<p>Calling <code>compile()</code> on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the <code>trainable</code>\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until <code>compile</code> is called again. Hence, if you change any <code>trainable</code> value, make sure\n",
    " to call <code>compile()</code> again on your\n",
    "model for your changes to be taken into account.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_detection_model_bkup = span_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ==================== FOLD# 0 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "11212/11212 [==============================] - 151s 13ms/sample - loss: 9.0147 - starts_0_loss: 1.7647 - stops_0_loss: 1.9347 - starts_1_loss: 2.4682 - stops_1_loss: 2.8467 - starts_0_accuracy: 0.4418 - stops_0_accuracy: 0.4243 - starts_1_accuracy: 0.3738 - stops_1_accuracy: 0.3357 - val_loss: 7.0804 - val_starts_0_loss: 1.3891 - val_stops_0_loss: 1.6999 - val_starts_1_loss: 1.8655 - val_stops_1_loss: 2.1227 - val_starts_0_accuracy: 0.5173 - val_stops_0_accuracy: 0.4720 - val_starts_1_accuracy: 0.4502 - val_stops_1_accuracy: 0.4866\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 7.5766 - starts_0_loss: 1.7399 - stops_0_loss: 1.8820 - starts_1_loss: 1.8452 - stops_1_loss: 2.1094 - starts_0_accuracy: 0.4663 - stops_0_accuracy: 0.4486 - starts_1_accuracy: 0.4552 - stops_1_accuracy: 0.4687 - val_loss: 6.5914 - val_starts_0_loss: 1.3839 - val_stops_0_loss: 1.6093 - val_starts_1_loss: 1.6223 - val_stops_1_loss: 1.9659 - val_starts_0_accuracy: 0.5301 - val_stops_0_accuracy: 0.4574 - val_starts_1_accuracy: 0.5162 - val_stops_1_accuracy: 0.4884\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 7.2255 - starts_0_loss: 1.7117 - stops_0_loss: 1.8148 - starts_1_loss: 1.7298 - stops_1_loss: 1.9688 - starts_0_accuracy: 0.4576 - stops_0_accuracy: 0.4515 - starts_1_accuracy: 0.4798 - stops_1_accuracy: 0.4802 - val_loss: 6.2146 - val_starts_0_loss: 1.3892 - val_stops_0_loss: 1.4936 - val_starts_1_loss: 1.5806 - val_stops_1_loss: 1.7469 - val_starts_0_accuracy: 0.5173 - val_stops_0_accuracy: 0.4863 - val_starts_1_accuracy: 0.5080 - val_stops_1_accuracy: 0.5259\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 7.1791 - starts_0_loss: 1.7599 - stops_0_loss: 1.8043 - starts_1_loss: 1.7035 - stops_1_loss: 1.9112 - starts_0_accuracy: 0.4645 - stops_0_accuracy: 0.4529 - starts_1_accuracy: 0.4857 - stops_1_accuracy: 0.4894 - val_loss: 6.2516 - val_starts_0_loss: 1.4068 - val_stops_0_loss: 1.4935 - val_starts_1_loss: 1.5402 - val_stops_1_loss: 1.8063 - val_starts_0_accuracy: 0.5266 - val_stops_0_accuracy: 0.4920 - val_starts_1_accuracy: 0.5202 - val_stops_1_accuracy: 0.5287\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 7.0137 - starts_0_loss: 1.7008 - stops_0_loss: 1.7814 - starts_1_loss: 1.6585 - stops_1_loss: 1.8728 - starts_0_accuracy: 0.4735 - stops_0_accuracy: 0.4627 - starts_1_accuracy: 0.4946 - stops_1_accuracy: 0.4991 - val_loss: 6.4569 - val_starts_0_loss: 1.4774 - val_stops_0_loss: 1.6338 - val_starts_1_loss: 1.6067 - val_stops_1_loss: 1.7362 - val_starts_0_accuracy: 0.5127 - val_stops_0_accuracy: 0.4748 - val_starts_1_accuracy: 0.5120 - val_stops_1_accuracy: 0.5212\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "11212/11212 [==============================] - 149s 13ms/sample - loss: 6.3792 - starts_0_loss: 1.5236 - stops_0_loss: 1.5860 - starts_1_loss: 1.5520 - stops_1_loss: 1.7179 - starts_0_accuracy: 0.4986 - stops_0_accuracy: 0.4834 - starts_1_accuracy: 0.5085 - stops_1_accuracy: 0.5316 - val_loss: 5.7588 - val_starts_0_loss: 1.2574 - val_stops_0_loss: 1.3351 - val_starts_1_loss: 1.4959 - val_stops_1_loss: 1.6643 - val_starts_0_accuracy: 0.5419 - val_stops_0_accuracy: 0.5173 - val_starts_1_accuracy: 0.5291 - val_stops_1_accuracy: 0.5548\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 6.1022 - starts_0_loss: 1.4191 - stops_0_loss: 1.4906 - starts_1_loss: 1.5138 - stops_1_loss: 1.6785 - starts_0_accuracy: 0.5122 - stops_0_accuracy: 0.4988 - starts_1_accuracy: 0.5175 - stops_1_accuracy: 0.5456 - val_loss: 5.7373 - val_starts_0_loss: 1.2462 - val_stops_0_loss: 1.3359 - val_starts_1_loss: 1.4956 - val_stops_1_loss: 1.6541 - val_starts_0_accuracy: 0.5441 - val_stops_0_accuracy: 0.5162 - val_starts_1_accuracy: 0.5298 - val_stops_1_accuracy: 0.5555\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 6.0565 - starts_0_loss: 1.4150 - stops_0_loss: 1.4730 - starts_1_loss: 1.5064 - stops_1_loss: 1.6621 - starts_0_accuracy: 0.5120 - stops_0_accuracy: 0.5045 - starts_1_accuracy: 0.5221 - stops_1_accuracy: 0.5470 - val_loss: 5.7087 - val_starts_0_loss: 1.2468 - val_stops_0_loss: 1.3236 - val_starts_1_loss: 1.4833 - val_stops_1_loss: 1.6495 - val_starts_0_accuracy: 0.5423 - val_stops_0_accuracy: 0.5155 - val_starts_1_accuracy: 0.5316 - val_stops_1_accuracy: 0.5508\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 6.0357 - starts_0_loss: 1.4069 - stops_0_loss: 1.4672 - starts_1_loss: 1.5021 - stops_1_loss: 1.6594 - starts_0_accuracy: 0.5053 - stops_0_accuracy: 0.4996 - starts_1_accuracy: 0.5221 - stops_1_accuracy: 0.5532 - val_loss: 5.7020 - val_starts_0_loss: 1.2436 - val_stops_0_loss: 1.3230 - val_starts_1_loss: 1.4834 - val_stops_1_loss: 1.6469 - val_starts_0_accuracy: 0.5408 - val_stops_0_accuracy: 0.5180 - val_starts_1_accuracy: 0.5355 - val_stops_1_accuracy: 0.5523\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 6.0019 - starts_0_loss: 1.3978 - stops_0_loss: 1.4596 - starts_1_loss: 1.4875 - stops_1_loss: 1.6572 - starts_0_accuracy: 0.5152 - stops_0_accuracy: 0.4956 - starts_1_accuracy: 0.5286 - stops_1_accuracy: 0.5479 - val_loss: 5.6834 - val_starts_0_loss: 1.2356 - val_stops_0_loss: 1.3260 - val_starts_1_loss: 1.4767 - val_stops_1_loss: 1.6400 - val_starts_0_accuracy: 0.5444 - val_stops_0_accuracy: 0.5180 - val_starts_1_accuracy: 0.5326 - val_stops_1_accuracy: 0.5530\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "11212/11212 [==============================] - 317s 28ms/sample - loss: 5.7838 - starts_0_loss: 1.3480 - stops_0_loss: 1.3987 - starts_1_loss: 1.4464 - stops_1_loss: 1.5900 - starts_0_accuracy: 0.5302 - stops_0_accuracy: 0.5178 - starts_1_accuracy: 0.5393 - stops_1_accuracy: 0.5669 - val_loss: 5.5241 - val_starts_0_loss: 1.2122 - val_stops_0_loss: 1.2627 - val_starts_1_loss: 1.4400 - val_stops_1_loss: 1.6045 - val_starts_0_accuracy: 0.5548 - val_stops_0_accuracy: 0.5455 - val_starts_1_accuracy: 0.5412 - val_stops_1_accuracy: 0.5694\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 299s 27ms/sample - loss: 5.5579 - starts_0_loss: 1.2896 - stops_0_loss: 1.3160 - starts_1_loss: 1.4017 - stops_1_loss: 1.5503 - starts_0_accuracy: 0.5484 - stops_0_accuracy: 0.5326 - starts_1_accuracy: 0.5541 - stops_1_accuracy: 0.5803 - val_loss: 5.4184 - val_starts_0_loss: 1.1930 - val_stops_0_loss: 1.2267 - val_starts_1_loss: 1.4223 - val_stops_1_loss: 1.5729 - val_starts_0_accuracy: 0.5590 - val_stops_0_accuracy: 0.5473 - val_starts_1_accuracy: 0.5458 - val_stops_1_accuracy: 0.5733\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 297s 26ms/sample - loss: 5.4017 - starts_0_loss: 1.2339 - stops_0_loss: 1.2861 - starts_1_loss: 1.3662 - stops_1_loss: 1.5159 - starts_0_accuracy: 0.5592 - stops_0_accuracy: 0.5478 - starts_1_accuracy: 0.5700 - stops_1_accuracy: 0.5844 - val_loss: 5.3227 - val_starts_0_loss: 1.1683 - val_stops_0_loss: 1.1934 - val_starts_1_loss: 1.4031 - val_stops_1_loss: 1.5537 - val_starts_0_accuracy: 0.5655 - val_stops_0_accuracy: 0.5548 - val_starts_1_accuracy: 0.5483 - val_stops_1_accuracy: 0.5812\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 297s 26ms/sample - loss: 5.2419 - starts_0_loss: 1.2036 - stops_0_loss: 1.2318 - starts_1_loss: 1.3330 - stops_1_loss: 1.4737 - starts_0_accuracy: 0.5667 - stops_0_accuracy: 0.5689 - starts_1_accuracy: 0.5720 - stops_1_accuracy: 0.5950 - val_loss: 5.2588 - val_starts_0_loss: 1.1494 - val_stops_0_loss: 1.1745 - val_starts_1_loss: 1.3902 - val_stops_1_loss: 1.5407 - val_starts_0_accuracy: 0.5669 - val_stops_0_accuracy: 0.5594 - val_starts_1_accuracy: 0.5501 - val_stops_1_accuracy: 0.5819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 297s 27ms/sample - loss: 5.1270 - starts_0_loss: 1.1649 - stops_0_loss: 1.2033 - starts_1_loss: 1.3089 - stops_1_loss: 1.4502 - starts_0_accuracy: 0.5715 - stops_0_accuracy: 0.5710 - starts_1_accuracy: 0.5823 - stops_1_accuracy: 0.6013 - val_loss: 5.2125 - val_starts_0_loss: 1.1399 - val_stops_0_loss: 1.1555 - val_starts_1_loss: 1.3825 - val_stops_1_loss: 1.5304 - val_starts_0_accuracy: 0.5722 - val_stops_0_accuracy: 0.5669 - val_starts_1_accuracy: 0.5569 - val_stops_1_accuracy: 0.5837\n",
      "[INFO]  =============== Validation for FOLD# 0 ===============\n",
      "[INFO] 61.02 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 60.16 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 57.22 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 56.69 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 52.29 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 51.12 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 47.64 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 47.29 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 49.45 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 49.54 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 46.56 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 45.42 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 57.16 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 55.63 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 50.11 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 50.92 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n",
      "[INFO] \t||"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (11212, 100) (11212, 100)\n",
      "[INFO] Prediction shape for validation data:  (2803, 100) (2803, 100)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  7973 out of 11212\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  1965 out of 2803\n",
      "[INFO] Training Jaccard Score:  0.5430792269610896\n",
      "[INFO] Validation Jaccard Score:  0.526457102760759\n",
      "[INFO] Training for fold: 0 finished at Mon Jun  1 03:30:22 2020\n",
      "[INFO] ==================== FOLD# 1 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "11212/11212 [==============================] - 149s 13ms/sample - loss: 5.9941 - starts_0_loss: 1.3996 - stops_0_loss: 1.4397 - starts_1_loss: 1.4767 - stops_1_loss: 1.6789 - starts_0_accuracy: 0.5318 - stops_0_accuracy: 0.5313 - starts_1_accuracy: 0.5381 - stops_1_accuracy: 0.5551 - val_loss: 5.0802 - val_starts_0_loss: 1.1486 - val_stops_0_loss: 1.1386 - val_starts_1_loss: 1.2922 - val_stops_1_loss: 1.4953 - val_starts_0_accuracy: 0.5772 - val_stops_0_accuracy: 0.6036 - val_starts_1_accuracy: 0.5840 - val_stops_1_accuracy: 0.6122\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 5.9021 - starts_0_loss: 1.3914 - stops_0_loss: 1.4165 - starts_1_loss: 1.4491 - stops_1_loss: 1.6445 - starts_0_accuracy: 0.5346 - stops_0_accuracy: 0.5324 - starts_1_accuracy: 0.5451 - stops_1_accuracy: 0.5567 - val_loss: 5.1029 - val_starts_0_loss: 1.1271 - val_stops_0_loss: 1.2322 - val_starts_1_loss: 1.3063 - val_stops_1_loss: 1.4307 - val_starts_0_accuracy: 0.5933 - val_stops_0_accuracy: 0.5605 - val_starts_1_accuracy: 0.5940 - val_stops_1_accuracy: 0.5997\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 5.9293 - starts_0_loss: 1.4101 - stops_0_loss: 1.4294 - starts_1_loss: 1.4469 - stops_1_loss: 1.6438 - starts_0_accuracy: 0.5334 - stops_0_accuracy: 0.5317 - starts_1_accuracy: 0.5467 - stops_1_accuracy: 0.5601 - val_loss: 5.1189 - val_starts_0_loss: 1.1907 - val_stops_0_loss: 1.1618 - val_starts_1_loss: 1.2848 - val_stops_1_loss: 1.4766 - val_starts_0_accuracy: 0.5840 - val_stops_0_accuracy: 0.5969 - val_starts_1_accuracy: 0.5972 - val_stops_1_accuracy: 0.6090\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 5.9287 - starts_0_loss: 1.4115 - stops_0_loss: 1.4434 - starts_1_loss: 1.4389 - stops_1_loss: 1.6345 - starts_0_accuracy: 0.5344 - stops_0_accuracy: 0.5324 - starts_1_accuracy: 0.5467 - stops_1_accuracy: 0.5628 - val_loss: 5.3006 - val_starts_0_loss: 1.1830 - val_stops_0_loss: 1.2029 - val_starts_1_loss: 1.3049 - val_stops_1_loss: 1.6040 - val_starts_0_accuracy: 0.5862 - val_stops_0_accuracy: 0.5829 - val_starts_1_accuracy: 0.6015 - val_stops_1_accuracy: 0.5901\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 135s 12ms/sample - loss: 5.9467 - starts_0_loss: 1.4236 - stops_0_loss: 1.4562 - starts_1_loss: 1.4404 - stops_1_loss: 1.6277 - starts_0_accuracy: 0.5305 - stops_0_accuracy: 0.5352 - starts_1_accuracy: 0.5459 - stops_1_accuracy: 0.5656 - val_loss: 5.2463 - val_starts_0_loss: 1.1200 - val_stops_0_loss: 1.2531 - val_starts_1_loss: 1.3258 - val_stops_1_loss: 1.5390 - val_starts_0_accuracy: 0.5794 - val_stops_0_accuracy: 0.5655 - val_starts_1_accuracy: 0.5744 - val_stops_1_accuracy: 0.5833\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "11212/11212 [==============================] - 150s 13ms/sample - loss: 5.2950 - starts_0_loss: 1.2231 - stops_0_loss: 1.2781 - starts_1_loss: 1.3082 - stops_1_loss: 1.4858 - starts_0_accuracy: 0.5648 - stops_0_accuracy: 0.5558 - starts_1_accuracy: 0.5771 - stops_1_accuracy: 0.5882 - val_loss: 4.7901 - val_starts_0_loss: 1.0471 - val_stops_0_loss: 1.0858 - val_starts_1_loss: 1.2458 - val_stops_1_loss: 1.4072 - val_starts_0_accuracy: 0.5936 - val_stops_0_accuracy: 0.5887 - val_starts_1_accuracy: 0.5929 - val_stops_1_accuracy: 0.6197\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 134s 12ms/sample - loss: 5.0373 - starts_0_loss: 1.1662 - stops_0_loss: 1.1890 - starts_1_loss: 1.2723 - stops_1_loss: 1.4094 - starts_0_accuracy: 0.5714 - stops_0_accuracy: 0.5768 - starts_1_accuracy: 0.5837 - stops_1_accuracy: 0.6121 - val_loss: 4.7535 - val_starts_0_loss: 1.0449 - val_stops_0_loss: 1.0728 - val_starts_1_loss: 1.2352 - val_stops_1_loss: 1.3971 - val_starts_0_accuracy: 0.5883 - val_stops_0_accuracy: 0.6051 - val_starts_1_accuracy: 0.5958 - val_stops_1_accuracy: 0.6258\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 134s 12ms/sample - loss: 5.0527 - starts_0_loss: 1.1727 - stops_0_loss: 1.1880 - starts_1_loss: 1.2801 - stops_1_loss: 1.4122 - starts_0_accuracy: 0.5712 - stops_0_accuracy: 0.5705 - starts_1_accuracy: 0.5753 - stops_1_accuracy: 0.6044 - val_loss: 4.7416 - val_starts_0_loss: 1.0446 - val_stops_0_loss: 1.0730 - val_starts_1_loss: 1.2364 - val_stops_1_loss: 1.3838 - val_starts_0_accuracy: 0.5933 - val_stops_0_accuracy: 0.5951 - val_starts_1_accuracy: 0.5983 - val_stops_1_accuracy: 0.6254\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 134s 12ms/sample - loss: 4.9830 - starts_0_loss: 1.1526 - stops_0_loss: 1.1710 - starts_1_loss: 1.2625 - stops_1_loss: 1.3975 - starts_0_accuracy: 0.5769 - stops_0_accuracy: 0.5747 - starts_1_accuracy: 0.5841 - stops_1_accuracy: 0.6126 - val_loss: 4.7298 - val_starts_0_loss: 1.0376 - val_stops_0_loss: 1.0702 - val_starts_1_loss: 1.2369 - val_stops_1_loss: 1.3818 - val_starts_0_accuracy: 0.5961 - val_stops_0_accuracy: 0.6015 - val_starts_1_accuracy: 0.5972 - val_stops_1_accuracy: 0.6286\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 134s 12ms/sample - loss: 4.9628 - starts_0_loss: 1.1458 - stops_0_loss: 1.1727 - starts_1_loss: 1.2522 - stops_1_loss: 1.3921 - starts_0_accuracy: 0.5839 - stops_0_accuracy: 0.5797 - starts_1_accuracy: 0.5867 - stops_1_accuracy: 0.6134 - val_loss: 4.7292 - val_starts_0_loss: 1.0404 - val_stops_0_loss: 1.0711 - val_starts_1_loss: 1.2330 - val_stops_1_loss: 1.3812 - val_starts_0_accuracy: 0.5883 - val_stops_0_accuracy: 0.5961 - val_starts_1_accuracy: 0.6022 - val_stops_1_accuracy: 0.6283\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "11212/11212 [==============================] - 318s 28ms/sample - loss: 4.9181 - starts_0_loss: 1.1419 - stops_0_loss: 1.1537 - starts_1_loss: 1.2441 - stops_1_loss: 1.3776 - starts_0_accuracy: 0.5815 - stops_0_accuracy: 0.5819 - starts_1_accuracy: 0.5913 - stops_1_accuracy: 0.6135 - val_loss: 4.7091 - val_starts_0_loss: 1.0356 - val_stops_0_loss: 1.0555 - val_starts_1_loss: 1.2322 - val_stops_1_loss: 1.3830 - val_starts_0_accuracy: 0.5872 - val_stops_0_accuracy: 0.6033 - val_starts_1_accuracy: 0.6036 - val_stops_1_accuracy: 0.6304\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 298s 27ms/sample - loss: 4.8520 - starts_0_loss: 1.1166 - stops_0_loss: 1.1412 - starts_1_loss: 1.2302 - stops_1_loss: 1.3645 - starts_0_accuracy: 0.5876 - stops_0_accuracy: 0.5857 - starts_1_accuracy: 0.5996 - stops_1_accuracy: 0.6184 - val_loss: 4.6932 - val_starts_0_loss: 1.0361 - val_stops_0_loss: 1.0520 - val_starts_1_loss: 1.2269 - val_stops_1_loss: 1.3754 - val_starts_0_accuracy: 0.5837 - val_stops_0_accuracy: 0.6051 - val_starts_1_accuracy: 0.6047 - val_stops_1_accuracy: 0.6222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 298s 27ms/sample - loss: 4.7934 - starts_0_loss: 1.1005 - stops_0_loss: 1.1220 - starts_1_loss: 1.2166 - stops_1_loss: 1.3541 - starts_0_accuracy: 0.5912 - stops_0_accuracy: 0.5867 - starts_1_accuracy: 0.6029 - stops_1_accuracy: 0.6208 - val_loss: 4.6823 - val_starts_0_loss: 1.0295 - val_stops_0_loss: 1.0462 - val_starts_1_loss: 1.2218 - val_stops_1_loss: 1.3827 - val_starts_0_accuracy: 0.5865 - val_stops_0_accuracy: 0.6068 - val_starts_1_accuracy: 0.6047 - val_stops_1_accuracy: 0.6204\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 300s 27ms/sample - loss: 4.7131 - starts_0_loss: 1.0926 - stops_0_loss: 1.0917 - starts_1_loss: 1.2043 - stops_1_loss: 1.3243 - starts_0_accuracy: 0.5933 - stops_0_accuracy: 0.5927 - starts_1_accuracy: 0.6044 - stops_1_accuracy: 0.6268 - val_loss: 4.6771 - val_starts_0_loss: 1.0242 - val_stops_0_loss: 1.0505 - val_starts_1_loss: 1.2292 - val_stops_1_loss: 1.3712 - val_starts_0_accuracy: 0.5879 - val_stops_0_accuracy: 0.6008 - val_starts_1_accuracy: 0.6011 - val_stops_1_accuracy: 0.6204\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 298s 27ms/sample - loss: 4.6401 - starts_0_loss: 1.0704 - stops_0_loss: 1.0770 - starts_1_loss: 1.1863 - stops_1_loss: 1.3065 - starts_0_accuracy: 0.5993 - stops_0_accuracy: 0.6016 - starts_1_accuracy: 0.6146 - stops_1_accuracy: 0.6321 - val_loss: 4.6624 - val_starts_0_loss: 1.0244 - val_stops_0_loss: 1.0437 - val_starts_1_loss: 1.2223 - val_stops_1_loss: 1.3697 - val_starts_0_accuracy: 0.5883 - val_stops_0_accuracy: 0.6040 - val_starts_1_accuracy: 0.6072 - val_stops_1_accuracy: 0.6265\n",
      "[INFO]  =============== Validation for FOLD# 1 ===============\n",
      "[INFO] 63.80 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 63.45 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 58.83 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 60.40 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 53.50 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 55.67 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 52.51 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 49.34 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 51.66 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 53.44 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 51.45 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 49.58 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 56.53 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 59.33 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 55.00 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 50.11 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (11212, 100) (11212, 100)\n",
      "[INFO] Prediction shape for validation data:  (2803, 100) (2803, 100)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  7779 out of 11212\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  1989 out of 2803\n",
      "[INFO] Training Jaccard Score:  0.5549721583500442\n",
      "[INFO] Validation Jaccard Score:  0.554618041050345\n",
      "[INFO] Training for fold: 1 finished at Mon Jun  1 04:20:22 2020\n",
      "[INFO] ==================== FOLD# 2 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "11212/11212 [==============================] - 149s 13ms/sample - loss: 5.5170 - starts_0_loss: 1.2896 - stops_0_loss: 1.3163 - starts_1_loss: 1.3529 - stops_1_loss: 1.5587 - starts_0_accuracy: 0.5592 - stops_0_accuracy: 0.5597 - starts_1_accuracy: 0.5716 - stops_1_accuracy: 0.5829 - val_loss: 4.7081 - val_starts_0_loss: 1.0904 - val_stops_0_loss: 1.0795 - val_starts_1_loss: 1.2226 - val_stops_1_loss: 1.3071 - val_starts_0_accuracy: 0.5986 - val_stops_0_accuracy: 0.6161 - val_starts_1_accuracy: 0.6076 - val_stops_1_accuracy: 0.6397\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 134s 12ms/sample - loss: 5.4812 - starts_0_loss: 1.2782 - stops_0_loss: 1.3142 - starts_1_loss: 1.3502 - stops_1_loss: 1.5386 - starts_0_accuracy: 0.5537 - stops_0_accuracy: 0.5600 - starts_1_accuracy: 0.5691 - stops_1_accuracy: 0.5873 - val_loss: 4.9372 - val_starts_0_loss: 1.2655 - val_stops_0_loss: 1.0308 - val_starts_1_loss: 1.2986 - val_stops_1_loss: 1.3471 - val_starts_0_accuracy: 0.5708 - val_stops_0_accuracy: 0.6268 - val_starts_1_accuracy: 0.5840 - val_stops_1_accuracy: 0.6372\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 134s 12ms/sample - loss: 5.4737 - starts_0_loss: 1.3001 - stops_0_loss: 1.2978 - starts_1_loss: 1.3476 - stops_1_loss: 1.5284 - starts_0_accuracy: 0.5537 - stops_0_accuracy: 0.5585 - starts_1_accuracy: 0.5697 - stops_1_accuracy: 0.5879 - val_loss: 5.0451 - val_starts_0_loss: 1.1256 - val_stops_0_loss: 1.0901 - val_starts_1_loss: 1.3372 - val_stops_1_loss: 1.4886 - val_starts_0_accuracy: 0.5683 - val_stops_0_accuracy: 0.6172 - val_starts_1_accuracy: 0.5715 - val_stops_1_accuracy: 0.6183\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 134s 12ms/sample - loss: 5.5015 - starts_0_loss: 1.3096 - stops_0_loss: 1.3158 - starts_1_loss: 1.3501 - stops_1_loss: 1.5259 - starts_0_accuracy: 0.5565 - stops_0_accuracy: 0.5627 - starts_1_accuracy: 0.5678 - stops_1_accuracy: 0.5921 - val_loss: 4.8261 - val_starts_0_loss: 1.1051 - val_stops_0_loss: 1.0479 - val_starts_1_loss: 1.2526 - val_stops_1_loss: 1.4151 - val_starts_0_accuracy: 0.6022 - val_stops_0_accuracy: 0.6268 - val_starts_1_accuracy: 0.5990 - val_stops_1_accuracy: 0.6372\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 134s 12ms/sample - loss: 5.4695 - starts_0_loss: 1.3002 - stops_0_loss: 1.3119 - starts_1_loss: 1.3357 - stops_1_loss: 1.5216 - starts_0_accuracy: 0.5606 - stops_0_accuracy: 0.5623 - starts_1_accuracy: 0.5725 - stops_1_accuracy: 0.5907 - val_loss: 4.8681 - val_starts_0_loss: 1.1264 - val_stops_0_loss: 1.0396 - val_starts_1_loss: 1.2926 - val_stops_1_loss: 1.4067 - val_starts_0_accuracy: 0.5690 - val_stops_0_accuracy: 0.6208 - val_starts_1_accuracy: 0.5751 - val_stops_1_accuracy: 0.6250\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "11212/11212 [==============================] - 149s 13ms/sample - loss: 4.8848 - starts_0_loss: 1.1467 - stops_0_loss: 1.1333 - starts_1_loss: 1.2344 - stops_1_loss: 1.3701 - starts_0_accuracy: 0.5810 - stops_0_accuracy: 0.5876 - starts_1_accuracy: 0.5919 - stops_1_accuracy: 0.6190 - val_loss: 4.4879 - val_starts_0_loss: 1.0053 - val_stops_0_loss: 1.0040 - val_starts_1_loss: 1.1905 - val_stops_1_loss: 1.2872 - val_starts_0_accuracy: 0.6108 - val_stops_0_accuracy: 0.6283 - val_starts_1_accuracy: 0.6058 - val_stops_1_accuracy: 0.6375\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 4.6722 - starts_0_loss: 1.0747 - stops_0_loss: 1.0915 - starts_1_loss: 1.1894 - stops_1_loss: 1.3165 - starts_0_accuracy: 0.5961 - stops_0_accuracy: 0.6013 - starts_1_accuracy: 0.6035 - stops_1_accuracy: 0.6349 - val_loss: 4.4490 - val_starts_0_loss: 0.9928 - val_stops_0_loss: 0.9920 - val_starts_1_loss: 1.1851 - val_stops_1_loss: 1.2780 - val_starts_0_accuracy: 0.6122 - val_stops_0_accuracy: 0.6347 - val_starts_1_accuracy: 0.6086 - val_stops_1_accuracy: 0.6393\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 4.5966 - starts_0_loss: 1.0525 - stops_0_loss: 1.0782 - starts_1_loss: 1.1667 - stops_1_loss: 1.2988 - starts_0_accuracy: 0.5994 - stops_0_accuracy: 0.6060 - starts_1_accuracy: 0.6121 - stops_1_accuracy: 0.6367 - val_loss: 4.4342 - val_starts_0_loss: 0.9929 - val_stops_0_loss: 0.9903 - val_starts_1_loss: 1.1793 - val_stops_1_loss: 1.2702 - val_starts_0_accuracy: 0.6126 - val_stops_0_accuracy: 0.6322 - val_starts_1_accuracy: 0.6111 - val_stops_1_accuracy: 0.6425\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 4.5776 - starts_0_loss: 1.0592 - stops_0_loss: 1.0692 - starts_1_loss: 1.1651 - stops_1_loss: 1.2846 - starts_0_accuracy: 0.5986 - stops_0_accuracy: 0.6125 - starts_1_accuracy: 0.6103 - stops_1_accuracy: 0.6433 - val_loss: 4.4280 - val_starts_0_loss: 0.9915 - val_stops_0_loss: 0.9888 - val_starts_1_loss: 1.1768 - val_stops_1_loss: 1.2698 - val_starts_0_accuracy: 0.6108 - val_stops_0_accuracy: 0.6375 - val_starts_1_accuracy: 0.6115 - val_stops_1_accuracy: 0.6429\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 135s 12ms/sample - loss: 4.5805 - starts_0_loss: 1.0568 - stops_0_loss: 1.0673 - starts_1_loss: 1.1671 - stops_1_loss: 1.2893 - starts_0_accuracy: 0.6045 - stops_0_accuracy: 0.6071 - starts_1_accuracy: 0.6097 - stops_1_accuracy: 0.6410 - val_loss: 4.4105 - val_starts_0_loss: 0.9905 - val_stops_0_loss: 0.9810 - val_starts_1_loss: 1.1743 - val_stops_1_loss: 1.2640 - val_starts_0_accuracy: 0.6140 - val_stops_0_accuracy: 0.6407 - val_starts_1_accuracy: 0.6083 - val_stops_1_accuracy: 0.6450\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "11212/11212 [==============================] - 318s 28ms/sample - loss: 4.5455 - starts_0_loss: 1.0456 - stops_0_loss: 1.0633 - starts_1_loss: 1.1578 - stops_1_loss: 1.2784 - starts_0_accuracy: 0.5976 - stops_0_accuracy: 0.6113 - starts_1_accuracy: 0.6169 - stops_1_accuracy: 0.6418 - val_loss: 4.4114 - val_starts_0_loss: 0.9920 - val_stops_0_loss: 0.9826 - val_starts_1_loss: 1.1741 - val_stops_1_loss: 1.2617 - val_starts_0_accuracy: 0.6108 - val_stops_0_accuracy: 0.6407 - val_starts_1_accuracy: 0.6126 - val_stops_1_accuracy: 0.6468\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 298s 27ms/sample - loss: 4.4863 - starts_0_loss: 1.0336 - stops_0_loss: 1.0508 - starts_1_loss: 1.1404 - stops_1_loss: 1.2612 - starts_0_accuracy: 0.6074 - stops_0_accuracy: 0.6135 - starts_1_accuracy: 0.6209 - stops_1_accuracy: 0.6466 - val_loss: 4.4207 - val_starts_0_loss: 0.9928 - val_stops_0_loss: 0.9791 - val_starts_1_loss: 1.1756 - val_stops_1_loss: 1.2725 - val_starts_0_accuracy: 0.6093 - val_stops_0_accuracy: 0.6422 - val_starts_1_accuracy: 0.6118 - val_stops_1_accuracy: 0.6475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 298s 27ms/sample - loss: 4.4097 - starts_0_loss: 1.0172 - stops_0_loss: 1.0239 - starts_1_loss: 1.1274 - stops_1_loss: 1.2409 - starts_0_accuracy: 0.6123 - stops_0_accuracy: 0.6191 - starts_1_accuracy: 0.6246 - stops_1_accuracy: 0.6493 - val_loss: 4.4254 - val_starts_0_loss: 0.9926 - val_stops_0_loss: 0.9865 - val_starts_1_loss: 1.1773 - val_stops_1_loss: 1.2674 - val_starts_0_accuracy: 0.6090 - val_stops_0_accuracy: 0.6350 - val_starts_1_accuracy: 0.6093 - val_stops_1_accuracy: 0.6457\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 298s 27ms/sample - loss: 4.3554 - starts_0_loss: 1.0025 - stops_0_loss: 1.0174 - starts_1_loss: 1.1079 - stops_1_loss: 1.2271 - starts_0_accuracy: 0.6182 - stops_0_accuracy: 0.6269 - starts_1_accuracy: 0.6322 - stops_1_accuracy: 0.6546 - val_loss: 4.4207 - val_starts_0_loss: 0.9927 - val_stops_0_loss: 0.9769 - val_starts_1_loss: 1.1779 - val_stops_1_loss: 1.2709 - val_starts_0_accuracy: 0.6097 - val_stops_0_accuracy: 0.6354 - val_starts_1_accuracy: 0.6122 - val_stops_1_accuracy: 0.6447\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 298s 27ms/sample - loss: 4.3448 - starts_0_loss: 0.9996 - stops_0_loss: 1.0096 - starts_1_loss: 1.1104 - stops_1_loss: 1.2251 - starts_0_accuracy: 0.6202 - stops_0_accuracy: 0.6214 - starts_1_accuracy: 0.6335 - stops_1_accuracy: 0.6498 - val_loss: 4.4178 - val_starts_0_loss: 0.9932 - val_stops_0_loss: 0.9788 - val_starts_1_loss: 1.1783 - val_stops_1_loss: 1.2658 - val_starts_0_accuracy: 0.6040 - val_stops_0_accuracy: 0.6390 - val_starts_1_accuracy: 0.6097 - val_stops_1_accuracy: 0.6497\n",
      "[INFO]  =============== Validation for FOLD# 2 ===============\n",
      "[INFO] 66.11 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 66.07 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 60.40 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 63.90 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 59.12 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 56.57 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 49.83 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 56.18 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 57.80 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 54.99 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 49.90 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 56.79 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 62.06 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 59.22 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 51.62 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 59.14 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (11212, 100) (11212, 100)\n",
      "[INFO] Prediction shape for validation data:  (2803, 100) (2803, 100)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  7774 out of 11212\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  1994 out of 2803\n",
      "[INFO] Training Jaccard Score:  0.5646201303955997\n",
      "[INFO] Validation Jaccard Score:  0.5502704665184222\n",
      "[INFO] Training for fold: 2 finished at Mon Jun  1 05:10:19 2020\n",
      "[INFO] ==================== FOLD# 3 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "11212/11212 [==============================] - 151s 13ms/sample - loss: 5.3390 - starts_0_loss: 1.2582 - stops_0_loss: 1.2560 - starts_1_loss: 1.3163 - stops_1_loss: 1.5085 - starts_0_accuracy: 0.5704 - stops_0_accuracy: 0.5681 - starts_1_accuracy: 0.5846 - stops_1_accuracy: 0.5945 - val_loss: 4.2309 - val_starts_0_loss: 0.9675 - val_stops_0_loss: 1.0414 - val_starts_1_loss: 1.0430 - val_stops_1_loss: 1.1890 - val_starts_0_accuracy: 0.6425 - val_stops_0_accuracy: 0.5997 - val_starts_1_accuracy: 0.6593 - val_stops_1_accuracy: 0.6468\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 5.3119 - starts_0_loss: 1.2607 - stops_0_loss: 1.2518 - starts_1_loss: 1.3121 - stops_1_loss: 1.4877 - starts_0_accuracy: 0.5681 - stops_0_accuracy: 0.5739 - starts_1_accuracy: 0.5834 - stops_1_accuracy: 0.5980 - val_loss: 4.2001 - val_starts_0_loss: 0.9304 - val_stops_0_loss: 0.9693 - val_starts_1_loss: 1.0594 - val_stops_1_loss: 1.2507 - val_starts_0_accuracy: 0.6539 - val_stops_0_accuracy: 0.6343 - val_starts_1_accuracy: 0.6550 - val_stops_1_accuracy: 0.6550\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 5.2617 - starts_0_loss: 1.2490 - stops_0_loss: 1.2487 - starts_1_loss: 1.2960 - stops_1_loss: 1.4681 - starts_0_accuracy: 0.5758 - stops_0_accuracy: 0.5810 - starts_1_accuracy: 0.5896 - stops_1_accuracy: 0.6024 - val_loss: 4.4087 - val_starts_0_loss: 0.9739 - val_stops_0_loss: 1.0399 - val_starts_1_loss: 1.1325 - val_stops_1_loss: 1.2596 - val_starts_0_accuracy: 0.6450 - val_stops_0_accuracy: 0.5972 - val_starts_1_accuracy: 0.6472 - val_stops_1_accuracy: 0.6397\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 5.2996 - starts_0_loss: 1.2715 - stops_0_loss: 1.2504 - starts_1_loss: 1.3077 - stops_1_loss: 1.4695 - starts_0_accuracy: 0.5688 - stops_0_accuracy: 0.5821 - starts_1_accuracy: 0.5859 - stops_1_accuracy: 0.6074 - val_loss: 4.2638 - val_starts_0_loss: 0.9655 - val_stops_0_loss: 0.9261 - val_starts_1_loss: 1.1341 - val_stops_1_loss: 1.2413 - val_starts_0_accuracy: 0.6329 - val_stops_0_accuracy: 0.6543 - val_starts_1_accuracy: 0.6361 - val_stops_1_accuracy: 0.6636\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 5.3068 - starts_0_loss: 1.2633 - stops_0_loss: 1.2612 - starts_1_loss: 1.3083 - stops_1_loss: 1.4740 - starts_0_accuracy: 0.5677 - stops_0_accuracy: 0.5739 - starts_1_accuracy: 0.5845 - stops_1_accuracy: 0.6002 - val_loss: 4.3238 - val_starts_0_loss: 1.0049 - val_stops_0_loss: 0.9804 - val_starts_1_loss: 1.0904 - val_stops_1_loss: 1.2582 - val_starts_0_accuracy: 0.6393 - val_stops_0_accuracy: 0.6343 - val_starts_1_accuracy: 0.6504 - val_stops_1_accuracy: 0.6614\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "11212/11212 [==============================] - 149s 13ms/sample - loss: 4.7195 - starts_0_loss: 1.1267 - stops_0_loss: 1.0878 - starts_1_loss: 1.1911 - stops_1_loss: 1.3136 - starts_0_accuracy: 0.5960 - stops_0_accuracy: 0.6044 - starts_1_accuracy: 0.6110 - stops_1_accuracy: 0.6357 - val_loss: 3.9736 - val_starts_0_loss: 0.8624 - val_stops_0_loss: 0.9063 - val_starts_1_loss: 1.0432 - val_stops_1_loss: 1.1692 - val_starts_0_accuracy: 0.6711 - val_stops_0_accuracy: 0.6625 - val_starts_1_accuracy: 0.6639 - val_stops_1_accuracy: 0.6818\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 4.4373 - starts_0_loss: 1.0339 - stops_0_loss: 1.0289 - starts_1_loss: 1.1339 - stops_1_loss: 1.2410 - starts_0_accuracy: 0.6089 - stops_0_accuracy: 0.6237 - starts_1_accuracy: 0.6225 - stops_1_accuracy: 0.6515 - val_loss: 3.9364 - val_starts_0_loss: 0.8555 - val_stops_0_loss: 0.8968 - val_starts_1_loss: 1.0329 - val_stops_1_loss: 1.1583 - val_starts_0_accuracy: 0.6789 - val_stops_0_accuracy: 0.6671 - val_starts_1_accuracy: 0.6664 - val_stops_1_accuracy: 0.6807\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 4.4215 - starts_0_loss: 1.0294 - stops_0_loss: 1.0183 - starts_1_loss: 1.1417 - stops_1_loss: 1.2321 - starts_0_accuracy: 0.6118 - stops_0_accuracy: 0.6266 - starts_1_accuracy: 0.6162 - stops_1_accuracy: 0.6533 - val_loss: 3.9344 - val_starts_0_loss: 0.8559 - val_stops_0_loss: 0.9003 - val_starts_1_loss: 1.0333 - val_stops_1_loss: 1.1510 - val_starts_0_accuracy: 0.6750 - val_stops_0_accuracy: 0.6632 - val_starts_1_accuracy: 0.6657 - val_stops_1_accuracy: 0.6811\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 4.4020 - starts_0_loss: 1.0196 - stops_0_loss: 1.0201 - starts_1_loss: 1.1291 - stops_1_loss: 1.2332 - starts_0_accuracy: 0.6100 - stops_0_accuracy: 0.6206 - starts_1_accuracy: 0.6200 - stops_1_accuracy: 0.6510 - val_loss: 3.9198 - val_starts_0_loss: 0.8535 - val_stops_0_loss: 0.8954 - val_starts_1_loss: 1.0245 - val_stops_1_loss: 1.1516 - val_starts_0_accuracy: 0.6736 - val_stops_0_accuracy: 0.6654 - val_starts_1_accuracy: 0.6654 - val_stops_1_accuracy: 0.6796\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 133s 12ms/sample - loss: 4.3453 - starts_0_loss: 1.0088 - stops_0_loss: 1.0034 - starts_1_loss: 1.1176 - stops_1_loss: 1.2156 - starts_0_accuracy: 0.6167 - stops_0_accuracy: 0.6295 - starts_1_accuracy: 0.6304 - stops_1_accuracy: 0.6540 - val_loss: 3.9266 - val_starts_0_loss: 0.8559 - val_stops_0_loss: 0.9022 - val_starts_1_loss: 1.0269 - val_stops_1_loss: 1.1464 - val_starts_0_accuracy: 0.6743 - val_stops_0_accuracy: 0.6604 - val_starts_1_accuracy: 0.6643 - val_stops_1_accuracy: 0.6775\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "11212/11212 [==============================] - 318s 28ms/sample - loss: 4.3691 - starts_0_loss: 1.0144 - stops_0_loss: 1.0145 - starts_1_loss: 1.1242 - stops_1_loss: 1.2159 - starts_0_accuracy: 0.6091 - stops_0_accuracy: 0.6227 - starts_1_accuracy: 0.6215 - stops_1_accuracy: 0.6537 - val_loss: 3.9340 - val_starts_0_loss: 0.8561 - val_stops_0_loss: 0.9020 - val_starts_1_loss: 1.0299 - val_stops_1_loss: 1.1483 - val_starts_0_accuracy: 0.6750 - val_stops_0_accuracy: 0.6561 - val_starts_1_accuracy: 0.6654 - val_stops_1_accuracy: 0.6796\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 300s 27ms/sample - loss: 4.2895 - starts_0_loss: 0.9954 - stops_0_loss: 0.9944 - starts_1_loss: 1.1027 - stops_1_loss: 1.1968 - starts_0_accuracy: 0.6176 - stops_0_accuracy: 0.6307 - starts_1_accuracy: 0.6267 - stops_1_accuracy: 0.6593 - val_loss: 3.9462 - val_starts_0_loss: 0.8614 - val_stops_0_loss: 0.9091 - val_starts_1_loss: 1.0322 - val_stops_1_loss: 1.1518 - val_starts_0_accuracy: 0.6721 - val_stops_0_accuracy: 0.6547 - val_starts_1_accuracy: 0.6639 - val_stops_1_accuracy: 0.6753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 298s 27ms/sample - loss: 4.2613 - starts_0_loss: 0.9967 - stops_0_loss: 0.9831 - starts_1_loss: 1.0997 - stops_1_loss: 1.1817 - starts_0_accuracy: 0.6159 - stops_0_accuracy: 0.6353 - starts_1_accuracy: 0.6307 - stops_1_accuracy: 0.6609 - val_loss: 3.9430 - val_starts_0_loss: 0.8594 - val_stops_0_loss: 0.9006 - val_starts_1_loss: 1.0318 - val_stops_1_loss: 1.1562 - val_starts_0_accuracy: 0.6736 - val_stops_0_accuracy: 0.6593 - val_starts_1_accuracy: 0.6671 - val_stops_1_accuracy: 0.6771\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 299s 27ms/sample - loss: 4.1999 - starts_0_loss: 0.9753 - stops_0_loss: 0.9775 - starts_1_loss: 1.0743 - stops_1_loss: 1.1735 - starts_0_accuracy: 0.6283 - stops_0_accuracy: 0.6420 - starts_1_accuracy: 0.6417 - stops_1_accuracy: 0.6704 - val_loss: 3.9543 - val_starts_0_loss: 0.8607 - val_stops_0_loss: 0.9103 - val_starts_1_loss: 1.0333 - val_stops_1_loss: 1.1566 - val_starts_0_accuracy: 0.6739 - val_stops_0_accuracy: 0.6554 - val_starts_1_accuracy: 0.6686 - val_stops_1_accuracy: 0.6761\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 299s 27ms/sample - loss: 4.1774 - starts_0_loss: 0.9678 - stops_0_loss: 0.9725 - starts_1_loss: 1.0700 - stops_1_loss: 1.1671 - starts_0_accuracy: 0.6290 - stops_0_accuracy: 0.6364 - starts_1_accuracy: 0.6413 - stops_1_accuracy: 0.6675 - val_loss: 3.9739 - val_starts_0_loss: 0.8631 - val_stops_0_loss: 0.9163 - val_starts_1_loss: 1.0435 - val_stops_1_loss: 1.1597 - val_starts_0_accuracy: 0.6693 - val_stops_0_accuracy: 0.6472 - val_starts_1_accuracy: 0.6636 - val_stops_1_accuracy: 0.6746\n",
      "[INFO]  =============== Validation for FOLD# 3 ===============\n",
      "[INFO] 67.73 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 67.76 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 66.93 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 64.72 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 61.56 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 59.34 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 58.43 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 60.21 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 59.96 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 57.84 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 57.61 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 59.19 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 63.89 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 62.27 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 61.27 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 62.43 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (11212, 100) (11212, 100)\n",
      "[INFO] Prediction shape for validation data:  (2803, 100) (2803, 100)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  7891 out of 11212\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  1970 out of 2803\n",
      "[INFO] Training Jaccard Score:  0.5749323053356906\n",
      "[INFO] Validation Jaccard Score:  0.564886924031743\n",
      "[INFO] Training for fold: 3 finished at Mon Jun  1 06:00:18 2020\n",
      "[INFO] ==================== FOLD# 4 ====================\n",
      "[INFO] Training only the final layers at higher learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "11212/11212 [==============================] - 151s 13ms/sample - loss: 5.1277 - starts_0_loss: 1.2104 - stops_0_loss: 1.2082 - starts_1_loss: 1.2587 - stops_1_loss: 1.4513 - starts_0_accuracy: 0.5895 - stops_0_accuracy: 0.5865 - starts_1_accuracy: 0.6036 - stops_1_accuracy: 0.6158 - val_loss: 3.9955 - val_starts_0_loss: 0.9504 - val_stops_0_loss: 0.9333 - val_starts_1_loss: 1.0326 - val_stops_1_loss: 1.0831 - val_starts_0_accuracy: 0.6532 - val_stops_0_accuracy: 0.6632 - val_starts_1_accuracy: 0.6721 - val_stops_1_accuracy: 0.6993\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 134s 12ms/sample - loss: 5.0898 - starts_0_loss: 1.2241 - stops_0_loss: 1.1972 - starts_1_loss: 1.2499 - stops_1_loss: 1.4194 - starts_0_accuracy: 0.5888 - stops_0_accuracy: 0.5923 - starts_1_accuracy: 0.6046 - stops_1_accuracy: 0.6176 - val_loss: 4.0514 - val_starts_0_loss: 1.0043 - val_stops_0_loss: 0.8905 - val_starts_1_loss: 1.0453 - val_stops_1_loss: 1.1130 - val_starts_0_accuracy: 0.6447 - val_stops_0_accuracy: 0.6714 - val_starts_1_accuracy: 0.6650 - val_stops_1_accuracy: 0.6889\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 135s 12ms/sample - loss: 5.0563 - starts_0_loss: 1.2126 - stops_0_loss: 1.1886 - starts_1_loss: 1.2494 - stops_1_loss: 1.4048 - starts_0_accuracy: 0.5840 - stops_0_accuracy: 0.5963 - starts_1_accuracy: 0.5987 - stops_1_accuracy: 0.6208 - val_loss: 4.9061 - val_starts_0_loss: 0.9656 - val_stops_0_loss: 1.3256 - val_starts_1_loss: 1.2050 - val_stops_1_loss: 1.4136 - val_starts_0_accuracy: 0.6333 - val_stops_0_accuracy: 0.5540 - val_starts_1_accuracy: 0.6243 - val_stops_1_accuracy: 0.6054\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 135s 12ms/sample - loss: 5.1206 - starts_0_loss: 1.2359 - stops_0_loss: 1.2100 - starts_1_loss: 1.2538 - stops_1_loss: 1.4217 - starts_0_accuracy: 0.5855 - stops_0_accuracy: 0.5936 - starts_1_accuracy: 0.5995 - stops_1_accuracy: 0.6154 - val_loss: 4.1736 - val_starts_0_loss: 1.0037 - val_stops_0_loss: 0.9052 - val_starts_1_loss: 1.0938 - val_stops_1_loss: 1.1756 - val_starts_0_accuracy: 0.6297 - val_stops_0_accuracy: 0.6629 - val_starts_1_accuracy: 0.6564 - val_stops_1_accuracy: 0.6729\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 136s 12ms/sample - loss: 5.0728 - starts_0_loss: 1.2067 - stops_0_loss: 1.2158 - starts_1_loss: 1.2417 - stops_1_loss: 1.4086 - starts_0_accuracy: 0.5866 - stops_0_accuracy: 0.5986 - starts_1_accuracy: 0.6025 - stops_1_accuracy: 0.6225 - val_loss: 4.2132 - val_starts_0_loss: 0.9918 - val_stops_0_loss: 0.9273 - val_starts_1_loss: 1.1011 - val_stops_1_loss: 1.1970 - val_starts_0_accuracy: 0.6425 - val_stops_0_accuracy: 0.6714 - val_starts_1_accuracy: 0.6572 - val_stops_1_accuracy: 0.6839\n",
      "[INFO] Training only the final layers at lower learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "11212/11212 [==============================] - 151s 13ms/sample - loss: 4.5342 - starts_0_loss: 1.0855 - stops_0_loss: 1.0381 - starts_1_loss: 1.1517 - stops_1_loss: 1.2592 - starts_0_accuracy: 0.6089 - stops_0_accuracy: 0.6267 - starts_1_accuracy: 0.6256 - stops_1_accuracy: 0.6531 - val_loss: 3.8505 - val_starts_0_loss: 0.8572 - val_stops_0_loss: 0.8712 - val_starts_1_loss: 1.0239 - val_stops_1_loss: 1.1009 - val_starts_0_accuracy: 0.6707 - val_stops_0_accuracy: 0.6761 - val_starts_1_accuracy: 0.6786 - val_stops_1_accuracy: 0.6921\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 135s 12ms/sample - loss: 4.2042 - starts_0_loss: 0.9840 - stops_0_loss: 0.9657 - starts_1_loss: 1.0793 - stops_1_loss: 1.1753 - starts_0_accuracy: 0.6249 - stops_0_accuracy: 0.6421 - starts_1_accuracy: 0.6432 - stops_1_accuracy: 0.6705 - val_loss: 3.8254 - val_starts_0_loss: 0.8515 - val_stops_0_loss: 0.8711 - val_starts_1_loss: 1.0197 - val_stops_1_loss: 1.0864 - val_starts_0_accuracy: 0.6679 - val_stops_0_accuracy: 0.6725 - val_starts_1_accuracy: 0.6825 - val_stops_1_accuracy: 0.6932\n",
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 134s 12ms/sample - loss: 4.1586 - starts_0_loss: 0.9619 - stops_0_loss: 0.9676 - starts_1_loss: 1.0626 - stops_1_loss: 1.1665 - starts_0_accuracy: 0.6292 - stops_0_accuracy: 0.6435 - starts_1_accuracy: 0.6442 - stops_1_accuracy: 0.6737 - val_loss: 3.8153 - val_starts_0_loss: 0.8499 - val_stops_0_loss: 0.8692 - val_starts_1_loss: 1.0177 - val_stops_1_loss: 1.0822 - val_starts_0_accuracy: 0.6689 - val_stops_0_accuracy: 0.6746 - val_starts_1_accuracy: 0.6757 - val_stops_1_accuracy: 0.6935\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 134s 12ms/sample - loss: 4.1275 - starts_0_loss: 0.9545 - stops_0_loss: 0.9531 - starts_1_loss: 1.0587 - stops_1_loss: 1.1609 - starts_0_accuracy: 0.6339 - stops_0_accuracy: 0.6422 - starts_1_accuracy: 0.6466 - stops_1_accuracy: 0.6718 - val_loss: 3.8019 - val_starts_0_loss: 0.8511 - val_stops_0_loss: 0.8627 - val_starts_1_loss: 1.0138 - val_stops_1_loss: 1.0779 - val_starts_0_accuracy: 0.6679 - val_stops_0_accuracy: 0.6796 - val_starts_1_accuracy: 0.6771 - val_stops_1_accuracy: 0.6953\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 135s 12ms/sample - loss: 4.1152 - starts_0_loss: 0.9525 - stops_0_loss: 0.9523 - starts_1_loss: 1.0540 - stops_1_loss: 1.1561 - starts_0_accuracy: 0.6316 - stops_0_accuracy: 0.6446 - starts_1_accuracy: 0.6488 - stops_1_accuracy: 0.6700 - val_loss: 3.7897 - val_starts_0_loss: 0.8481 - val_stops_0_loss: 0.8581 - val_starts_1_loss: 1.0114 - val_stops_1_loss: 1.0766 - val_starts_0_accuracy: 0.6729 - val_stops_0_accuracy: 0.6839 - val_starts_1_accuracy: 0.6771 - val_stops_1_accuracy: 0.6971\n",
      "[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\n",
      "Train on 11212 samples, validate on 2803 samples\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "11212/11212 [==============================] - 320s 29ms/sample - loss: 4.1160 - starts_0_loss: 0.9545 - stops_0_loss: 0.9541 - starts_1_loss: 1.0532 - stops_1_loss: 1.1537 - starts_0_accuracy: 0.6348 - stops_0_accuracy: 0.6484 - starts_1_accuracy: 0.6501 - stops_1_accuracy: 0.6805 - val_loss: 3.7944 - val_starts_0_loss: 0.8505 - val_stops_0_loss: 0.8520 - val_starts_1_loss: 1.0135 - val_stops_1_loss: 1.0824 - val_starts_0_accuracy: 0.6718 - val_stops_0_accuracy: 0.6910 - val_starts_1_accuracy: 0.6782 - val_stops_1_accuracy: 0.7017\n",
      "Epoch 2/5\n",
      "11212/11212 [==============================] - 299s 27ms/sample - loss: 4.0509 - starts_0_loss: 0.9441 - stops_0_loss: 0.9359 - starts_1_loss: 1.0388 - stops_1_loss: 1.1323 - starts_0_accuracy: 0.6453 - stops_0_accuracy: 0.6533 - starts_1_accuracy: 0.6539 - stops_1_accuracy: 0.6827 - val_loss: 3.8158 - val_starts_0_loss: 0.8548 - val_stops_0_loss: 0.8663 - val_starts_1_loss: 1.0194 - val_stops_1_loss: 1.0796 - val_starts_0_accuracy: 0.6725 - val_stops_0_accuracy: 0.6782 - val_starts_1_accuracy: 0.6721 - val_stops_1_accuracy: 0.6975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "11212/11212 [==============================] - 299s 27ms/sample - loss: 4.0146 - starts_0_loss: 0.9337 - stops_0_loss: 0.9285 - starts_1_loss: 1.0274 - stops_1_loss: 1.1251 - starts_0_accuracy: 0.6457 - stops_0_accuracy: 0.6559 - starts_1_accuracy: 0.6564 - stops_1_accuracy: 0.6841 - val_loss: 3.8139 - val_starts_0_loss: 0.8532 - val_stops_0_loss: 0.8656 - val_starts_1_loss: 1.0189 - val_stops_1_loss: 1.0805 - val_starts_0_accuracy: 0.6711 - val_stops_0_accuracy: 0.6789 - val_starts_1_accuracy: 0.6743 - val_stops_1_accuracy: 0.6982\n",
      "Epoch 4/5\n",
      "11212/11212 [==============================] - 720s 64ms/sample - loss: 3.9713 - starts_0_loss: 0.9260 - stops_0_loss: 0.9129 - starts_1_loss: 1.0224 - stops_1_loss: 1.1103 - starts_0_accuracy: 0.6448 - stops_0_accuracy: 0.6596 - starts_1_accuracy: 0.6636 - stops_1_accuracy: 0.6884 - val_loss: 3.8263 - val_starts_0_loss: 0.8536 - val_stops_0_loss: 0.8745 - val_starts_1_loss: 1.0213 - val_stops_1_loss: 1.0814 - val_starts_0_accuracy: 0.6757 - val_stops_0_accuracy: 0.6739 - val_starts_1_accuracy: 0.6707 - val_stops_1_accuracy: 0.6978\n",
      "Epoch 5/5\n",
      "11212/11212 [==============================] - 378s 34ms/sample - loss: 3.9211 - starts_0_loss: 0.9139 - stops_0_loss: 0.9083 - starts_1_loss: 1.0046 - stops_1_loss: 1.0943 - starts_0_accuracy: 0.6506 - stops_0_accuracy: 0.6629 - starts_1_accuracy: 0.6702 - stops_1_accuracy: 0.6900 - val_loss: 3.8261 - val_starts_0_loss: 0.8574 - val_stops_0_loss: 0.8613 - val_starts_1_loss: 1.0234 - val_stops_1_loss: 1.0882 - val_starts_0_accuracy: 0.6693 - val_stops_0_accuracy: 0.6803 - val_starts_1_accuracy: 0.6718 - val_stops_1_accuracy: 0.6960\n",
      "[INFO]  =============== Validation for FOLD# 4 ===============\n",
      "[INFO] 70.11 \t|| train \t|| starts \t|| accuracy_score\n",
      "[INFO] 71.87 \t|| train \t|| stops \t|| accuracy_score\n",
      "[INFO] 66.93 \t|| valid \t|| starts \t|| accuracy_score\n",
      "[INFO] 68.03 \t|| valid \t|| stops \t|| accuracy_score\n",
      "=======================================================================\n",
      "[INFO] 65.45 \t|| train \t|| starts \t|| f1_score\n",
      "[INFO] 65.11 \t|| train \t|| stops \t|| f1_score\n",
      "[INFO] 61.40 \t|| valid \t|| starts \t|| f1_score\n",
      "[INFO] 59.18 \t|| valid \t|| stops \t|| f1_score\n",
      "=======================================================================\n",
      "[INFO] 63.31 \t|| train \t|| starts \t|| precision_score\n",
      "[INFO] 64.71 \t|| train \t|| stops \t|| precision_score\n",
      "[INFO] 59.42 \t|| valid \t|| starts \t|| precision_score\n",
      "[INFO] 58.29 \t|| valid \t|| stops \t|| precision_score\n",
      "=======================================================================\n",
      "[INFO] 68.63 \t|| train \t|| starts \t|| recall_score\n",
      "[INFO] 65.98 \t|| train \t|| stops \t|| recall_score\n",
      "[INFO] 64.83 \t|| valid \t|| starts \t|| recall_score\n",
      "[INFO] 61.43 \t|| valid \t|| stops \t|| recall_score\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \t|| train \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_train_starts.csv\n",
      "[INFO] \t|| train \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_train_stops.csv\n",
      "[INFO] \t|| valid \t|| starts \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_valid_starts.csv\n",
      "[INFO] \t|| valid \t|| stops \t|| confusion_matrix \t|| ../results/ConfusionMatrix_V24_valid_stops.csv\n",
      "=======================================================================\n",
      "[INFO] Prediction shape for training data:  (11212, 100) (11212, 100)\n",
      "[INFO] Prediction shape for validation data:  (2803, 100) (2803, 100)\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for training data:  7322 out of 11212\n",
      "[INFO] Normal predictions (StartIndex less than EndIndex) for validation data:  1860 out of 2803\n",
      "[INFO] Training Jaccard Score:  0.5772569418058389\n",
      "[INFO] Validation Jaccard Score:  0.5654481260498994\n",
      "[INFO] Training for fold: 4 finished at Mon Jun  1 06:58:52 2020\n",
      "Mon Jun  1 06:58:52 2020\n"
     ]
    }
   ],
   "source": [
    "for num, (t_index, v_index) in enumerate(sss.split(X_span, Y_span_stops)): # NOTICE the swap of val and train indices, on purpose for faster training\n",
    "    print(\"[INFO] ==================== FOLD#\", num, \"====================\")\n",
    "    \n",
    "    if num > 0:\n",
    "        del span_history\n",
    "        del span_detection_model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    \n",
    "    span_detection_model = span_detection_model_bkup\n",
    "    \n",
    "    span_mcp = ModelCheckpoint(filepath=RESULTS_DIR+MODEL_PREFIX+\"BestCheckpoint_\"+str(num)+\".h5\", monitor='val_loss',\n",
    "                               verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "    \n",
    "    span_csvl = CSVLogger(filename=RESULTS_DIR+MODEL_PREFIX+\"_LossLogs_\"+str(num)+\".csv\",\n",
    "                          separator=\",\",\n",
    "                          append=True)\n",
    "    \n",
    "    print(\"[INFO] Training only the final layers at higher learning rates.\")\n",
    "    span_detection_model.layers[3].trainable = False\n",
    "    adam = Adam(learning_rate=MAX_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[0],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl])\n",
    "    \n",
    "    print(\"[INFO] Training only the final layers at lower learning rates.\")\n",
    "    adam = Adam(learning_rate=MID_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[1],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl])\n",
    "    \n",
    "    print(\"[INFO] Unfreezing RoBerta layer and training at lowest learning rates.\")\n",
    "    span_detection_model.layers[3].trainable = True\n",
    "    adam = Adam(learning_rate=MIN_LR)\n",
    "    span_detection_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                 optimizer=adam,\n",
    "                                 metrics=['accuracy'],\n",
    "                                 loss_weights={\"starts_0\":1.0,\"stops_0\":1.0,\"starts_1\":1.0,\"stops_1\":1.0})\n",
    "    span_history = span_detection_model.fit(x={\"att_flags\":X_span_att[t_index],\n",
    "                                               \"words\":X_span[t_index],\n",
    "                                               \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                            y={\"starts_0\":Y_span_starts[t_index],\n",
    "                                               \"stops_0\":Y_span_stops[t_index], \n",
    "                                               \"starts_1\":Y_span_starts[t_index],\n",
    "                                               \"stops_1\":Y_span_stops[t_index]},\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            epochs=NUM_EPOCHS[2],\n",
    "                                            validation_data=({\"att_flags\":X_span_att[v_index],\n",
    "                                                              \"words\":X_span[v_index],\n",
    "                                                              \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                                             {\"starts_0\":Y_span_starts[v_index],\n",
    "                                                              \"stops_0\":Y_span_stops[v_index], \n",
    "                                                              \"starts_1\":Y_span_starts[v_index],\n",
    "                                                              \"stops_1\":Y_span_stops[v_index]}),\n",
    "                                            verbose=1,\n",
    "                                            callbacks=[span_mcp, span_csvl])\n",
    "    \n",
    "    pred_train = span_detection_model.predict(x = {\"att_flags\":X_span_att[t_index],\n",
    "                                                   \"words\":X_span[t_index],\n",
    "                                                   \"token_ids\":np.zeros_like(X_span_att[t_index])},\n",
    "                                              batch_size=PREDICT_BATCH_SIZE)\n",
    "\n",
    "    pred_val = span_detection_model.predict(x = {\"att_flags\":X_span_att[v_index],\n",
    "                                                 \"words\":X_span[v_index],\n",
    "                                                 \"token_ids\":np.zeros_like(X_span_att[v_index])},\n",
    "                                            batch_size=PREDICT_BATCH_SIZE)\n",
    "    \n",
    "    pred_starts_train, pred_stops_train = (pred_train[0]/2.0 + pred_train[2]/2.0), (pred_train[1]/2.0 + pred_train[3]/2.0)\n",
    "    pred_starts_val, pred_stops_val = (pred_val[0]/2.0 + pred_val[2]/2.0), (pred_val[1]/2.0 + pred_val[3]/2.0)\n",
    "    \n",
    "    # Accumulate test results after training every fold\n",
    "    pred_test_fold = span_detection_model.predict(x = {\"att_flags\":X_span_att_test,\n",
    "                                                       \"words\":X_span_test,\n",
    "                                                       \"token_ids\":np.zeros_like(X_span_att_test)},\n",
    "                                                  batch_size=PREDICT_BATCH_SIZE)\n",
    "    if num==0:\n",
    "        pred_test = []\n",
    "        pred_test.append(pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test.append(pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    else:\n",
    "        pred_test[0] += (pred_test_fold[0]/2.0 + pred_test_fold[2]/2.0)\n",
    "        pred_test[1] += (pred_test_fold[1]/2.0 + pred_test_fold[3]/2.0)\n",
    "    \n",
    "    # Tabulate\n",
    "    preds = {\n",
    "        \"train\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_span_starts[t_index],\n",
    "                \"y_pred\":pred_train[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_span_stops[t_index],\n",
    "                \"y_pred\":pred_train[1].argmax(axis=1)\n",
    "            }\n",
    "        },\n",
    "        \"valid\":{\n",
    "            \"starts\":{\n",
    "                \"y_true\":Y_span_starts[v_index],\n",
    "                \"y_pred\":pred_val[0].argmax(axis=1)\n",
    "            },\n",
    "            \"stops\":{\n",
    "                \"y_true\":Y_span_stops[v_index],\n",
    "                \"y_pred\":pred_val[1].argmax(axis=1)\n",
    "            }        \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print_metrics(pred_dict=preds)\n",
    "\n",
    "    print(\"[INFO] Prediction shape for training data: \", pred_starts_train.shape, pred_stops_train.shape)\n",
    "    print(\"[INFO] Prediction shape for validation data: \", pred_starts_val.shape, pred_stops_val.shape)\n",
    "\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for training data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_train.argmax(axis=1),\n",
    "                                  pred_stops_train.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_train.shape[0])\n",
    "    print(\"[INFO] Normal predictions (StartIndex less than EndIndex) for validation data: \",\n",
    "          sum([s<e for s,e in zip(pred_starts_val.argmax(axis=1),\n",
    "                                  pred_stops_val.argmax(axis=1))]),\n",
    "          \"out of\", pred_starts_val.shape[0],)\n",
    "\n",
    "    pred_words_train = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[t_index],\n",
    "                                                                                                                                   pred_starts_train.argmax(axis=1),\n",
    "                                                                                                                                   pred_stops_train.argmax(axis=1))]\n",
    "    \n",
    "    pred_words_val = [post_process(tokenizer.decode(t[s:e])) if s<e else post_process(tokenizer.decode(t[e:])) for t, s,e in zip(X_span[v_index],\n",
    "                                                                                                                                 pred_starts_val.argmax(axis=1),\n",
    "                                                                                                                                 pred_stops_val.argmax(axis=1))]\n",
    "\n",
    "    print(\"[INFO] Training Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in t_index],\n",
    "                                                          pred_words_train)]))\n",
    "    print(\"[INFO] Validation Jaccard Score: \",\n",
    "          np.mean([jaccard(str1=i, str2=j) for i,j in zip([t for n,t in enumerate(Y_span_words) if n in v_index],\n",
    "                                                          pred_words_val)]))\n",
    "    print(\"[INFO] Training for fold:\", num, \"finished at\", ctime(time()))\n",
    "print(ctime(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Prediction shape for testing data:  (3534, 100) (3534, 100)\n"
     ]
    }
   ],
   "source": [
    "pred_starts_test, pred_stops_test = pred_test[0]/NUM_FOLDS, pred_test[1]/NUM_FOLDS\n",
    "print(\"[INFO] Prediction shape for testing data: \", pred_starts_test.shape, pred_stops_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal predictions (StartIndex less than EndIndex) for testing data: 2764 out of 3534\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal predictions (StartIndex less than EndIndex) for testing data:\",\n",
    "      sum([s<e for s,e in zip(pred_starts_test.argmax(axis=1),\n",
    "                              pred_stops_test.argmax(axis=1))]), \n",
    "      \"out of\",\n",
    "      pred_starts_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words_test = [\n",
    "    post_process(tokenizer.decode(t[s:e+1])) if s<e else post_process(tokenizer.decode(t[e:])) for t,s,e in zip(X_span_test,\n",
    "                                                                                                              pred_starts_test.argmax(axis=1),\n",
    "                                                                                                              pred_stops_test.argmax(axis=1))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " yes! im down to 50% full on my dvr  i was at 98% like 3 days ago... lol i swear if i didnt have a dvr i would never watch tv  neutral \n",
      "25\n",
      "38\n",
      "i swear if i didnt have a dvr i would never watch tv\n"
     ]
    }
   ],
   "source": [
    "check_idx = 158\n",
    "#print([[t,i,j,k] for t,i,j,k in zip(tokenizer.decode(),X_test[check_idx],pred_starts_test[check_idx],pred_stops_test[check_idx])])\n",
    "print(tokenizer.decode(X_span_test[check_idx]))\n",
    "print(pred_starts_test.argmax(axis=1)[check_idx])\n",
    "print(pred_stops_test.argmax(axis=1)[check_idx])\n",
    "print(post_process(tokenizer.decode(X_span_test[check_idx][pred_starts_test.argmax(axis=1)[check_idx]:1+pred_stops_test.argmax(axis=1)[check_idx]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span['selected_text'] = pred_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[\"selected_text\"] = np.where(test_df_span[\"sentiment\"] == \"neutral\",\n",
    "                                         test_df_span[\"text\"],\n",
    "                                         test_df_span[\"selected_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_span[[\"textID\", \"selected_text\"]].to_csv(RESULTS_DIR+\"submission_\"+MODEL_PREFIX+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>This morning I rode behind a guy with a bird cage containing a plastic Tyrannosaurus rex attached to his bicycle. I couldn`t get a photo.</td>\n",
       "      <td>negative</td>\n",
       "      <td>i couldn`t get a photo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>good luck in the pressure test! don`t worry everything will be great</td>\n",
       "      <td>positive</td>\n",
       "      <td>good luck in the pressure test! don`t worry everything will be great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>Perfect night. Best month of my life so far!! You my boo,</td>\n",
       "      <td>positive</td>\n",
       "      <td>perfect night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>bye LA... I already miss you</td>\n",
       "      <td>negative</td>\n",
       "      <td>miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3442</th>\n",
       "      <td>so did you watch?  I`d forgottenmost of it, enjoyed re-seeing it, but paying for it today, my face doesnt bounce back anymore</td>\n",
       "      <td>positive</td>\n",
       "      <td>enjoyed re-seeing it, but paying for it today, my face doesnt bounce back anymore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>congrats! Photo of dre?</td>\n",
       "      <td>positive</td>\n",
       "      <td>congrats!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>yes yes yes lotsa fun  i cant wait. dont like make up with shaun too much in front of me tho cos ill get awkard haha</td>\n",
       "      <td>positive</td>\n",
       "      <td>fun i cant wait. dont like make up with shaun too much in front of me tho cos ill get awkard haha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>Happy Star Wars Day</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy star wars day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>crying real tears</td>\n",
       "      <td>negative</td>\n",
       "      <td>crying real tears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>I hate my life</td>\n",
       "      <td>negative</td>\n",
       "      <td>i hate my life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>yeah.. I left cause my pc can`t run the client anymore. I had thought I would have a new one by now..</td>\n",
       "      <td>negative</td>\n",
       "      <td>can`t run the client anymore.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>Please send me those youtube links, Erin watched most of them but I was cooking and didn`t get to see the cowboys</td>\n",
       "      <td>negative</td>\n",
       "      <td>didn`t get to see the cowboys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>you are lame  go make me breakfast!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>lame go make me breakfast!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3455</th>\n",
       "      <td>Ugh our two week push to produce a six pack has produced nothing but pain!</td>\n",
       "      <td>negative</td>\n",
       "      <td>ugh our two week push to produce a six pack has produced nothing but pain!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2140</th>\n",
       "      <td>http://twitpic.com/4wh5x - LUCKY! I`m so jealous even though I don`t like her that much</td>\n",
       "      <td>negative</td>\n",
       "      <td>i`m so jealous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>just finished mine, some parts were remarkably difficult  cramming kanji now!</td>\n",
       "      <td>negative</td>\n",
       "      <td>difficult cramming kanji now!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>(RE:'your joint w/  is crazy!!') Thx    Hey Trax heard a 49er rear ended you??</td>\n",
       "      <td>positive</td>\n",
       "      <td>thx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>oh ok  well im sendin lots of love ****</td>\n",
       "      <td>positive</td>\n",
       "      <td>love ****</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>: I`ll keep y`all in my prayers but welcome back to TX  Hope you have a good,safe flight</td>\n",
       "      <td>positive</td>\n",
       "      <td>hope you have a good,safe flight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2978</th>\n",
       "      <td>ive been spending time w/my momma! we`re celebrating mother`s day for her early  ive been good! how was woodburn?</td>\n",
       "      <td>positive</td>\n",
       "      <td>good!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>yep  but I`m going better now</td>\n",
       "      <td>positive</td>\n",
       "      <td>better now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>How lovely</td>\n",
       "      <td>positive</td>\n",
       "      <td>how lovely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>umm get ready, help my mum, give GJ his presents when he gets up (could be a while), wait for people to arrive, then party</td>\n",
       "      <td>positive</td>\n",
       "      <td>party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>Good morning   Been here since 4am, just quiet.  How are you?</td>\n",
       "      <td>positive</td>\n",
       "      <td>good morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2802</th>\n",
       "      <td>Not well again   it`s definitely not hayfever</td>\n",
       "      <td>negative</td>\n",
       "      <td>not well again</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "724   This morning I rode behind a guy with a bird cage containing a plastic Tyrannosaurus rex attached to his bicycle. I couldn`t get a photo.   \n",
       "1306                                                                       good luck in the pressure test! don`t worry everything will be great   \n",
       "528                                                                                  Perfect night. Best month of my life so far!! You my boo,    \n",
       "2893                                                                                                               bye LA... I already miss you   \n",
       "3442              so did you watch?  I`d forgottenmost of it, enjoyed re-seeing it, but paying for it today, my face doesnt bounce back anymore   \n",
       "288                                                                                                                     congrats! Photo of dre?   \n",
       "1453                       yes yes yes lotsa fun  i cant wait. dont like make up with shaun too much in front of me tho cos ill get awkard haha   \n",
       "2898                                                                                                                        Happy Star Wars Day   \n",
       "207                                                                                                                           crying real tears   \n",
       "203                                                                                                                              I hate my life   \n",
       "750                                       yeah.. I left cause my pc can`t run the client anymore. I had thought I would have a new one by now..   \n",
       "1655                          Please send me those youtube links, Erin watched most of them but I was cooking and didn`t get to see the cowboys   \n",
       "24                                                                                                         you are lame  go make me breakfast!!   \n",
       "3455                                                                 Ugh our two week push to produce a six pack has produced nothing but pain!   \n",
       "2140                                                    http://twitpic.com/4wh5x - LUCKY! I`m so jealous even though I don`t like her that much   \n",
       "1050                                                              just finished mine, some parts were remarkably difficult  cramming kanji now!   \n",
       "1445                                                             (RE:'your joint w/  is crazy!!') Thx    Hey Trax heard a 49er rear ended you??   \n",
       "3054                                                                                                    oh ok  well im sendin lots of love ****   \n",
       "544                                                    : I`ll keep y`all in my prayers but welcome back to TX  Hope you have a good,safe flight   \n",
       "2978                          ive been spending time w/my momma! we`re celebrating mother`s day for her early  ive been good! how was woodburn?   \n",
       "461                                                                                                               yep  but I`m going better now   \n",
       "2864                                                                                                                                 How lovely   \n",
       "1199                 umm get ready, help my mum, give GJ his presents when he gets up (could be a while), wait for people to arrive, then party   \n",
       "2223                                                                              Good morning   Been here since 4am, just quiet.  How are you?   \n",
       "2802                                                                                              Not well again   it`s definitely not hayfever   \n",
       "\n",
       "     sentiment  \\\n",
       "724   negative   \n",
       "1306  positive   \n",
       "528   positive   \n",
       "2893  negative   \n",
       "3442  positive   \n",
       "288   positive   \n",
       "1453  positive   \n",
       "2898  positive   \n",
       "207   negative   \n",
       "203   negative   \n",
       "750   negative   \n",
       "1655  negative   \n",
       "24    negative   \n",
       "3455  negative   \n",
       "2140  negative   \n",
       "1050  negative   \n",
       "1445  positive   \n",
       "3054  positive   \n",
       "544   positive   \n",
       "2978  positive   \n",
       "461   positive   \n",
       "2864  positive   \n",
       "1199  positive   \n",
       "2223  positive   \n",
       "2802  negative   \n",
       "\n",
       "                                                                                          selected_text  \n",
       "724                                                                             i couldn`t get a photo.  \n",
       "1306                               good luck in the pressure test! don`t worry everything will be great  \n",
       "528                                                                                      perfect night.  \n",
       "2893                                                                                           miss you  \n",
       "3442                  enjoyed re-seeing it, but paying for it today, my face doesnt bounce back anymore  \n",
       "288                                                                                           congrats!  \n",
       "1453  fun i cant wait. dont like make up with shaun too much in front of me tho cos ill get awkard haha  \n",
       "2898                                                                                happy star wars day  \n",
       "207                                                                                   crying real tears  \n",
       "203                                                                                      i hate my life  \n",
       "750                                                                       can`t run the client anymore.  \n",
       "1655                                                                      didn`t get to see the cowboys  \n",
       "24                                                                          lame go make me breakfast!!  \n",
       "3455                         ugh our two week push to produce a six pack has produced nothing but pain!  \n",
       "2140                                                                                     i`m so jealous  \n",
       "1050                                                                      difficult cramming kanji now!  \n",
       "1445                                                                                                thx  \n",
       "3054                                                                                          love ****  \n",
       "544                                                                    hope you have a good,safe flight  \n",
       "2978                                                                                              good!  \n",
       "461                                                                                          better now  \n",
       "2864                                                                                         how lovely  \n",
       "1199                                                                                              party  \n",
       "2223                                                                                       good morning  \n",
       "2802                                                                                     not well again  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_span.loc[test_df_span.sentiment!=\"neutral\"][[\"text\", \"sentiment\",\"selected_text\"]].sample(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
